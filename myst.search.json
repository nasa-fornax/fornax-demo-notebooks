{"version":"1","records":[{"hierarchy":{"lvl1":"Cross Matching"},"type":"lvl1","url":"/crossmatch","position":0},{"hierarchy":{"lvl1":"Cross Matching"},"content":"In this set of Use Case Scenarios, we cross-match catalogs to obtain multi-mission data for objects of interest.","type":"content","url":"/crossmatch","position":1},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB"},"type":"lvl1","url":"/ztf-ps1-crossmatch","position":0},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB"},"content":"","type":"content","url":"/ztf-ps1-crossmatch","position":1},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"Learning Goals"},"type":"lvl2","url":"/ztf-ps1-crossmatch#learning-goals","position":2},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will:\n\nunderstand how to cross-match cloud-based catalogs using lsdb.\n\nunderstand how to parallelize lsdb cross-matches using dask.\n\nhave a feeling for when dask parallelization can be helpful.\n\nhave a rough idea of the maximum number of objects that can be cross matched on each Fornax Science Console server type.\n\n","type":"content","url":"/ztf-ps1-crossmatch#learning-goals","position":3},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"Introduction"},"type":"lvl2","url":"/ztf-ps1-crossmatch#introduction","position":4},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"Introduction"},"content":"In the era of increasingly large astronomical survey missions like TESS, Gaia, ZTF, Pan-STARRS, Roman, and Rubin, catalog operations are becoming less and less practical to complete on a personal computer. Operations such as source cross-matching can require many GB of memory and take many hours to complete using a single CPU. Recognizing these looming obstacles, many catalogs are becoming accessible to cloud computing platforms like the Fornax Science Console, and increasingly high-performance tools are being developed that leverage cloud computing resources to simplify and speed up catalog operations.\n\nLSDB is a useful package for performing large cross-matches between HATS catalogs. It can leverage the \n\nDask library to work with larger-than-memory data sets and distribute computation tasks across multiple cores. Users perform large cross-matches without ever having to download a file.\n\nIn this tutorial, we will use lsdb with dask to perform a cross-match between ZTF and Pan-STARRS HATS catalogs to benchmark the performance. These HATS catalogs are stored on AWS S3 cloud storage. An application might be to collect time-series photometry for 10,000 or more stars in the Kepler field from ZTF and Pan-STARRS. With this in mind, we will begin by cross-matching 10,000 sources from ZTF with the Pan-STARRS mean-object catalog. The user can choose to scale up to a larger number of rows to test the performance. The CSV file provided with this notebook contains the runtime results of tests run on various Fornax Science Console server types for comparison.\n\nAs of August 2025, the Fornax Science Console server types were:\n\n“Small” - 4 GB RAM, 2 CPU\n\n“Medium” - 16 GB RAM, 4 CPU\n\n“Large” -  64 GB RAM, 16 CPU\n\n“XLarge” -  512 GB RAM, 128 CPU\n\nFor generality, we will refer to the server type by the number of CPUs. For each server and each number of cross-match rows, we want to know the performance with (1) default dask configuration, (2) minimal dask - 1 worker, (3) bigger dask - as many workers as we can use, and (4) auto-scaling dask.\n\nNote that in this notebook, only one configuration (combination of CPU count, number of cross-match rows, and dask configuration) is run at a time. It must be reconfigured and rerun to benchmark each configuration.\n\n","type":"content","url":"/ztf-ps1-crossmatch#introduction","position":5},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl3":"Runtime","lvl2":"Introduction"},"type":"lvl3","url":"/ztf-ps1-crossmatch#runtime","position":6},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl3":"Runtime","lvl2":"Introduction"},"content":"As of August 2025, as written (10,000 rows with the “default” dask settings), this notebook takes about 45 seconds to run on the “small” Fornax Science Console server type (4 GB RAM and 2 CPUs). Users can modify the configuration for larger cross-matches, which will take more time. E.g., cross-matching 10 million rows on the “large” server type (64 GB RAM and 16 CPUs) can take ~5 minutes.\n\n","type":"content","url":"/ztf-ps1-crossmatch#runtime","position":7},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"Imports"},"type":"lvl2","url":"/ztf-ps1-crossmatch#imports","position":8},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"Imports"},"content":"We require the following packages:\n\nos solely for the cpu_count function,\n\ndatetime for measuring the crossmatch time,\n\npandas to write and read the CSV of benchmarks\n\nmatplotlib to plot the benchmarks,\n\nastropy for coordinates and units,\n\nlsdb to read the catalogs and perform the cross-match, and\n\ndask for parallelization.\n\n# Uncomment the next line to install dependencies if needed.\n# %pip install -r requirements_ztf_ps1_crossmatch.txt\n\n\n\n\n\nfrom os import cpu_count\nfrom datetime import datetime\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom astropy.coordinates import SkyCoord\nfrom astropy import units as u\n\nimport lsdb\nfrom dask.distributed import Client, LocalCluster\n\n\n\n","type":"content","url":"/ztf-ps1-crossmatch#imports","position":9},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"1. Preconfiguring the Run"},"type":"lvl2","url":"/ztf-ps1-crossmatch#id-1-preconfiguring-the-run","position":10},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"1. Preconfiguring the Run"},"content":"First choose the number of rows we want to cross-match and our dask environment. For tips on using dask with lsdb, see \n\nlsdb’s Dask Cluster Tips.\n\n# The left table will have about this many rows. The cross-matched product will have slightly fewer.\n# Set Nrows = -1 to cross-match the entire catalog. The full catalog cross-match is\n# recommended ONLY on XLarge instance with at least 32 workers.\nNrows = 10_000\n\n# dask_workers can be:\n# - \"default\" (uses the default scheduler which runs all tasks in the main process)\n# - an integer, which uses a fixed number of workers\n# - \"scale\" to use an adaptive, auto-scaling cluster.\ndask_workers = \"default\"\n\n\n\n\n\n# The code in this cell is automatic setup based on the configuration in the previous cell, so it can be left alone.\n\n# We must pass a radius to lsdb. Here is the mapping between radius and desired number of rows for ZTF.\n# The values in this dictionary were determined experimentally, by\n# incrementing/decrementing the radius until the desired number of\n# catalog rows was returned.\nradius = { # Nrows: radius_arcseconds\n           10_000:     331,\n          100_000:    1047,\n        1_000_000:    3318,\n       10_000_000:  11_180,\n      100_000_000:  33_743,\n    1_000_000_000: 102_000,\n}\n\n# Set up dask cluster\nif dask_workers == \"scale\":\n    cluster = LocalCluster()\n    cluster.adapt(minimum_cores=1, maximum_cores=cpu_count())\n    client = Client(cluster)\nelif isinstance(dask_workers, int):\n    client = Client(\n        # Number of Dask workers - Python processes to run\n        n_workers=dask_workers,\n        # Limits number of Python threads per worker\n        threads_per_worker=1,\n        # Memory limit per worker\n        memory_limit=None,\n    )\nelif dask_workers == \"default\":\n    # Either using default configuration or configuring Dask\n    # using the built-in DaskHub\n    pass\nelse:\n    raise ValueError(\"`dask_workers` must be one of 'default', 'scale' or an int.\")\n\n# The performance may depend on the number of CPUs available, so we'll track that as well.\nnum_cpus = cpu_count()\n\n\n\n","type":"content","url":"/ztf-ps1-crossmatch#id-1-preconfiguring-the-run","position":11},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"2. Read in catalogs and downselect ZTF to Nrows"},"type":"lvl2","url":"/ztf-ps1-crossmatch#id-2-read-in-catalogs-and-downselect-ztf-to-nrows","position":12},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"2. Read in catalogs and downselect ZTF to Nrows"},"content":"\n\n# Define sky area. Here we're using the Kepler field.\nc = SkyCoord('19:22:40  +44:30:00', unit=(u.hourangle, u.deg))\ncone_ra, cone_dec = c.ra.value, c.dec.value\n\nif Nrows > 0:\n    radius_arcsec = radius[Nrows]\n    search_filter = lsdb.ConeSearch(cone_ra, cone_dec, radius_arcsec)\nelse:\n    # Full cross-match\n    # ONLY ON XLARGE ENVIRONMENT USING AT LEAST 32 CPUS\n    search_filter = None\n\n# Read ZTF DR23\nztf_path = \"s3://ipac-irsa-ztf/contributed/dr23/objects/hats\"\nztf_piece = lsdb.open_catalog(\n    ztf_path,\n    columns=[\"oid\", \"ra\", \"dec\"],\n    search_filter=search_filter\n)\n\n# Read Pan-STARRS DR2\nps1_path = \"s3://stpubdata/panstarrs/ps1/public/hats/otmo\"\nps1_margin = \"s3://stpubdata/panstarrs/ps1/public/hats/otmo_10arcs\"\nps1 = lsdb.open_catalog(\n    ps1_path,\n    margin_cache=ps1_margin,\n    columns=[\"objName\",\"objID\",\"raMean\",\"decMean\"],\n)\n\n\n\n","type":"content","url":"/ztf-ps1-crossmatch#id-2-read-in-catalogs-and-downselect-ztf-to-nrows","position":13},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"3. Initialize the crossmatch and compute, measuring the time elapsed."},"type":"lvl2","url":"/ztf-ps1-crossmatch#id-3-initialize-the-crossmatch-and-compute-measuring-the-time-elapsed","position":14},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"3. Initialize the crossmatch and compute, measuring the time elapsed."},"content":"\n\n# Setting up the cross-match actually takes very little time\nztf_x_ps1 = ztf_piece.crossmatch(ps1, radius_arcsec=1, n_neighbors=1, suffixes=(\"_ztf\", \"_ps1\"))\nztf_x_ps1\n\n\n\n\n\n# Executing the cross-match does take time\nt0 = datetime.now()\nxmatch = ztf_x_ps1.compute()\nt1 = datetime.now() - t0\n\nprint(\"Time Elapsed:\", t1)\n\n\n\n\n\n# Check the length of the resulting table\nrows_out = len(xmatch)\nprint(f\"Number of rows out: {rows_out:,d}\")\n\n\n\n\n\n# Close the Dask cluster/client when finished\nif dask_workers != \"default\":\n    if dask_workers == \"scale\":\n        cluster.close()\n    client.close()\n\n\n\n","type":"content","url":"/ztf-ps1-crossmatch#id-3-initialize-the-crossmatch-and-compute-measuring-the-time-elapsed","position":15},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"4. Record and plot benchmarks"},"type":"lvl2","url":"/ztf-ps1-crossmatch#id-4-record-and-plot-benchmarks","position":16},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"4. Record and plot benchmarks"},"content":"Write the recorded benchmark to an output file, then make plots to analyze the benchmarks.\n\ntry:\n    # read in file if it exists\n    benchmarks = pd.read_csv(\"output/xmatch_benchmarks.csv\", index_col=[\"Ncpus\", \"Nrows\", \"Nworkers\"])\nexcept FileNotFoundError:\n    # otherwise create an empty DataFrame\n    multi_index = pd.MultiIndex(levels=[[], [], []], codes=[[], [], []], names=[\"Ncpus\", \"Nrows\", \"Nworkers\"])\n    benchmarks = pd.DataFrame(index=multi_index, columns=[\"time\", \"Nrows_out\", \"updated\"])\n\n# assign values\nbenchmarks.loc[\n    (num_cpus, Nrows, dask_workers),\n    [\"time\", \"Nrows_out\", \"updated\"]\n    ] = t1.total_seconds(), int(rows_out), datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\nbenchmarks = benchmarks.sort_index()\n# benchmarks.to_csv(\"output/xmatch_benchmarks.csv\") # Uncomment this to write the new benchmarks to file\nbenchmarks\n\n\n\n\n\nbenchmarks = pd.read_csv(\"output/xmatch_benchmarks.csv\", index_col=[\"Ncpus\", \"Nrows\", \"Nworkers\"])\n\nnworkers = [1, 2, 4, 8, 16, 32, 64, 128, 256, \"default\", \"scale\"]\n\ndef plot_by_nworkers(num_cpus, ax):\n    # Plot execution time vs. number of dask workers for each scale job\n    b = benchmarks.loc[num_cpus]\n\n    for n in b.index.levels[0]:\n        try:\n            # get the relevant benchmarks\n            b_N = b.xs(n, level=\"Nrows\").reset_index()\n        except KeyError:\n            pass\n\n        # convert Nworkers to categorical with desired order\n        b_N[\"Nworkers\"] = pd.Categorical(b_N[\"Nworkers\"],\n                                            categories=[str(x) for x in nworkers],\n                                            ordered=True)\n        b_N = b_N.sort_values(\"Nworkers\")\n\n        # only plot if there is more than 1 data point\n        if len(b_N) > 1:\n            ax.plot(\"Nworkers\", \"time\", marker=\"s\", linestyle=\"-\", data=b_N, label=f\"{n} rows\")\n\n    ax.set(yscale=\"log\", xlabel=\"Nworkers\", ylabel=\"Execution Time (s)\",\n           title=f\"{num_cpus} CPUs\")\n    ax.legend()\n\n\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\ncpu_counts = benchmarks.index.get_level_values(\"Ncpus\").drop_duplicates().sort_values()\nfor i, ncpu in enumerate(cpu_counts):\n    plot_by_nworkers(ncpu, axs[i])\nfig.tight_layout()\n\n\n\nThese plots show how the cross-match execution time depends on the number of workers used. Broadly, increasing the number of workers up to the number of available CPUs improves the performance. The exception is for smaller cross-matches (\\le 1 million rows) spread across many (\\ge 16) CPUs, when the overhead of managing many workers is significant compared to the cross-match time. Increasing the number of workers past the number of CPUs results in somewhat worse performance. Interestingly, the agnostic approach---not manually specifying any dask parameters, but instead letting lsdb use the default dask behavior---results in the best performance for smaller cross-matches. This indicates that the lsdb cross-match functions are well-optimized and well-configured for use with dask without much user oversight, at least on cross-matches with \\le 1 million rows.\n\nFor the largest jobs (Nrows \\ge 10 million), the default dask configuration performs worse than fixing the number of dask_workers to be the number of available CPUs. Interestingly, on the XLarge server with 128 CPUs, the 64-worker run performed the best on the large cross-matches. For larger cross-matches, we recommend manually setting the number of workers to be \\frac{1}{2} N_\\mathrm{CPU} \\leq N_\\mathrm{workers} \\leq N_\\mathrm{CPU}.\n\nGiven the amount of time it takes to perform the largest cross-matches (Nrows \\ge 1 billion), these were run only on the XLarge server using the default dask configuration. The full cross-match of over 7 billion rows took just over 4 hours.\n\nThese graphs also illustrate the approximate maximum number of rows that can be cross matched with a given amount of RAM and CPU. Trying to cross-match more rows or use worker configurations that aren’t shown in the plots can lead to one of the following behaviors:\n\nSmall (4GB RAM, 2 CPU)\n\nWhen attempting Nrows \\ge 10 million, the cross-match exhibits long compute times, seeming never to finish.\n\nMedium (16GB RAM, 4 CPU)\n\nWhen attempting Nrows \\ge 100 million, the cross-match exhibits long compute times, seeming never to finish.\n\nLarge (64GB RAM, 16 CPU)\n\nWhen attempting Nrows = 100 million with anything but 16 or ‘default’ values for dask_workers, the cross-match exhibits long compute times, seeming never to finish. When attempting Nrows \\ge 1 billion, the cross-match exhibits long compute times, seeming never to finish.\n\nXLarge (512GB RAM, 128 CPU)\n\nThe XLarge environment was able to complete all attempted cross-matches, except that setting dask_workers='scale' resulted in the dead worker behavior documented below.\n\nAutoscaling on any environment\n\nWhen using dask_workers='scale' and scaling up the number of rows to cross-match, eventually you will see logging output from the dask cluster indicating that workers have died and are being respawn. This behavior repeats, and the cross-match never finishes (after being allowed to run for, e.g., an hour when it is expected to finish in 5 minutes).\n\n","type":"content","url":"/ztf-ps1-crossmatch#id-4-record-and-plot-benchmarks","position":17},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"5. Summary"},"type":"lvl2","url":"/ztf-ps1-crossmatch#id-5-summary","position":18},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"5. Summary"},"content":"The Fornax Science Console is capable of hosting cross-matches between large catalogs using the lsdb package. lsdb leverages dask to efficiently plan cross-match jobs using multiple workers, which results in large performance gains, especially as jobs scale up to millions or tens of millions of catalog rows.\n\nRecommendations\n\nFor small cross-matches (1 million rows or less) it is acceptable and often optimal to use lsdb’s default dask settings to parallelize jobs. In other words, for these jobs you don’t need to configure or even import dask at all to leverage its parallelization power.\n\nFor large cross-matches (millions of rows or more), we recommend running in an environment with at least tens of CPUs, and setting the number of dask workers to at least half the number of available CPUs, and up to the number of CPUs. E.g., 128 CPUs with 64 dask workers.\n\nCross-matches of 10 million rows or less can, at the time of writing, be completed on the Small Fornax Console using the default dask configuration. However, given the performance, we recommend the following use scaling:\n\nNrows \\le 10^6: Small\n\n10^6 < Nrows < 10^7: Medium\n\n10^7 < Nrows < 10^8: Large\n\nNrows > 10^8: XLarge\n\nIn this tutorial we have cross-matched sections of catalogs in a particular region of sky, where most of the stars likely fall in the same or neighboring sky cells. It would be useful in the future to try this using generic samples of stars from across the entire sky. How different is performance when the catalog rows come from many sky cells instead of just a few? We have also generated the cross-match using compute, which loads the full result into memory, but lsdb supports larger-than-memory cross-matches by writing them directly to disk using to_hats. It would also be illustrative to run these benchmarks, and benchmarking a full cross-match between ZTF and Pan-STARRS, by writing the result to disk.\n\nOther Science Cases\n\nThe example science case used here is an investigation to collect time-series photometry from sources in and around the Kepler field. The time series associated with the ZTF and Pan-STARRS sources might be used to supplement the Kepler light curves, extend their time baseline, examine how stellar light curves change in different photometric filters, and more. But lsdb enables other science cases with other catalogs as well. Some examples might be to\n\ncombine photometric spectral energy distributions (SEDs) of distant galaxies with their spectra to build a training set for machine learning to predict photometric redshifts,\n\ncombine Gaia’s exquisite astrometry with your favorite star survey to obtain 6D kinematic solutions for your sample stars,\n\ncombine stellar spectroscopic catalogs with light curve rotation period measurements to estimate stellar ages using gyrochronology.\n\n","type":"content","url":"/ztf-ps1-crossmatch#id-5-summary","position":19},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"About this Notebook"},"type":"lvl2","url":"/ztf-ps1-crossmatch#about-this-notebook","position":20},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl2":"About this Notebook"},"content":"Authors: Zach Claytor (Astronomical Data Scientist at Space Telescope Science Institute) and the Fornax team\n\nContact: For help with this notebook, please open a topic in the \n\nFornax Community Forum “Support” category.\n\n","type":"content","url":"/ztf-ps1-crossmatch#about-this-notebook","position":21},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl3":"References","lvl2":"About this Notebook"},"type":"lvl3","url":"/ztf-ps1-crossmatch#references","position":22},{"hierarchy":{"lvl1":"Cross-Match ZTF and Pan-STARRS using LSDB","lvl3":"References","lvl2":"About this Notebook"},"content":"This work uses \n\nastropy.\n\nThis work uses \n\nlsdb.","type":"content","url":"/ztf-ps1-crossmatch#references","position":23},{"hierarchy":{"lvl1":"Multi-band forced photometry"},"type":"lvl1","url":"/forced-photometry","position":0},{"hierarchy":{"lvl1":"Multi-band forced photometry"},"content":"In this User Case we obtain multiwavelenght imaging data and run\nautomated forced photometry.","type":"content","url":"/forced-photometry","position":1},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets"},"type":"lvl1","url":"/multiband-photometry","position":0},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets"},"content":"","type":"content","url":"/multiband-photometry","position":1},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"Learning Goals"},"type":"lvl2","url":"/multiband-photometry#learning-goals","position":2},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will be able to:\n\nget catalogs and images from NASA archives in the cloud where possible\n\nmeasure fluxes at any location by running forced photometry using “The Tractor”\n\nemploy parallel processing to make this as fast as possible\n\ncross match large catalogs\n\nplot results","type":"content","url":"/multiband-photometry#learning-goals","position":3},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"Introduction"},"type":"lvl2","url":"/multiband-photometry#introduction","position":4},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"Introduction"},"content":"This code performs photometry in an automated fashion at all locations in an input catalog on 4 bands of IRAC data from IRSA and 2 bands of GALEX data from MAST.  The resulting catalog is then cross-matched with a Chandra catalog from HEASARC to generate a multiband catalog to facilitate galaxy evolution studies.\n\nIf you run this code as is, it will only look at a small region of the COSMOS survey, and as a result, the plots near the end will not be so very informative.  We do this for faster runtimes and to show proof of concept as to how to do this work.  Please change the radius in section 1. to be something larger if you want to work with more data, but expect longer runtimes associated with larger areas.\n\nThe code makes full use of multiple processors to optimize run time on large datasets.","type":"content","url":"/multiband-photometry#introduction","position":5},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"Input","lvl2":"Introduction"},"type":"lvl3","url":"/multiband-photometry#input","position":6},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"Input","lvl2":"Introduction"},"content":"RA and DEC within COSMOS catalog\n\ndesired catalog radius in arcminutes","type":"content","url":"/multiband-photometry#input","position":7},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"Output","lvl2":"Introduction"},"type":"lvl3","url":"/multiband-photometry#output","position":8},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"Output","lvl2":"Introduction"},"content":"merged, multiband, science ready pandas dataframe\n\nIRAC color color plot for identifying interesting populations","type":"content","url":"/multiband-photometry#output","position":9},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"Runtime","lvl2":"Introduction"},"type":"lvl3","url":"/multiband-photometry#runtime","position":10},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"Runtime","lvl2":"Introduction"},"content":"As of 2025 September, this notebook takes about 13 minutes to run to completion on Fornax using a server with 8GB RAM/2 CPU’ and Environment: ‘Default Astrophysics’ (image).","type":"content","url":"/multiband-photometry#runtime","position":11},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"Imports"},"type":"lvl2","url":"/multiband-photometry#imports","position":12},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"Imports"},"content":"Non-standard Dependencies:\n\ntractor code which does the forced photometry from Lang et al., 2016\n\nastroquery to interface with archives APIs\n\nastropy to work with coordinates/units and data structures\n\nskimage to work with the images\n\nThis cell will install the Python ones if needed:\n\n# Uncomment the next line to install dependencies if needed.\n# %pip install -r requirements_multiband_photometry.txt\n\n\n\n# standard lib imports\nimport time\nimport sys\nimport os\nimport shutil\n\n# Third party imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\nfrom astroquery.heasarc import Heasarc\nimport pyvo\n\n# Local code imports\nsys.path.append('code_src/')\n\nimport cutout\nfrom exceptions import TractorError\nimport photometry\nfrom nway_write_header import nway_write_header\nfrom photometry import Band\nfrom photometry import lookup_img_pair\nfrom galex_functions import galex_get_images\n\n# This code is to parse cloud access information; currently in `code_src`, eventually will be part of pyvo\nimport fornax\n\n# temporarily let the notebook start without tractor as a dependency\ntry:\n    from find_nconfsources import find_nconfsources\nexcept ImportError:\n    print(\"tractor is missing\")\n    pass\n\n\n%matplotlib inline\n\n\n\n","type":"content","url":"/multiband-photometry#imports","position":13},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"1. Retrieve Initial Catalog from IRSA"},"type":"lvl2","url":"/multiband-photometry#id-1-retrieve-initial-catalog-from-irsa","position":14},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"1. Retrieve Initial Catalog from IRSA"},"content":"In this section we query the COSMOS2015 catalog (Laigle et al. 2016) from the IRSA archive using the Table Access Protocol (TAP). We specify the sky position and search radius, select a curated subset of columns needed for forced photometry, validation, and later multiwavelength analysis, and download only those rows that fall within the requested region of the COSMOS field. The result is a compact catalog containing positions, photometric redshifts, optical and infrared fluxes, UV measurements, and existing X-ray associations, which forms the starting point for the remainder of the workflow.\n\nWe access the COSMOS2015 catalog using the Table Access Protocol (TAP), an IVOA standard that allows SQL-like queries across astronomical databases. For more about TAP, see the IVOA \n\ndocumentation or \n\nsuggestions on its usage from IRSA.\n\n# Pull a COSMOS catalog from IRSA using pyVO\n\n# Central RA/Dec of the COSMOS field (from SIMBAD).\n# Used as the center of the TAP cone search.\ncoords = SkyCoord('150.01d 2.2d', frame='icrs')\n\n# Search radius (in arcminutes).\n# Full COSMOS radius is ~48', but querying that area is extremely slow\n# (∼24 hours on 128 cores). We start with a small radius for speed.\nradius = 0.5 * u.arcmin\n\n\n# The COSMOS2015 catalog has several thousand columns. Requesting all of\n# them is unnecessary and makes the TAP query more memory intensive\n# and slower than required.\n# We select only the columns needed for:\n#   • forced-photometry inputs (Ks and SPLASH fluxes)\n#   • validation of Tractor photometry (SPLASH fluxes → Section 3.6)\n#   • UV/X-ray diagnostics (GALEX + Chandra fluxes)\n#   • source classification (PHOTOZ, type)\n#   • optical context (r-band mags)\n\n# These columns allow us to *extend* COSMOS2015 with new IRAC+GALEX\n# forced photometry and improved X-ray associations (via nway).\n\ncols = [\n    # Astrometry + unique ID\n    'ra', 'dec', 'id',\n\n    # Ks-band photometry (used by Tractor for contamination modeling)\n    'Ks_FLUX_APER2', 'Ks_FLUXERR_APER2',\n\n    # Photometric redshift\n    'PHOTOZ',\n\n    # SPLASH (IRAC) magnitudes and fluxes — used to validate Tractor outputs\n    'SPLASH_1_MAG', 'SPLASH_1_MAGERR',\n    'SPLASH_1_FLUX', 'SPLASH_1_FLUX_ERR',\n    'SPLASH_2_FLUX', 'SPLASH_2_FLUX_ERR',\n    'SPLASH_3_FLUX', 'SPLASH_3_FLUX_ERR',\n    'SPLASH_4_FLUX', 'SPLASH_4_FLUX_ERR',\n\n    # GALEX UV fluxes\n    'FLUX_GALEX_NUV', 'FLUX_GALEX_FUV',\n\n    # Chandra X-ray fluxes + identifier\n    'FLUX_CHANDRA_05_2', 'FLUX_CHANDRA_2_10', 'FLUX_CHANDRA_05_10', 'ID_CHANDRA09 ',\n\n    # Morphology / source type (0=galaxy, 1=star, etc.)\n    'type',\n\n    # Optical r-band photometry\n    'r_MAG_AUTO', 'r_MAGERR_AUTO',\n\n    # 24 μm photometry (used for additional diagnostics)\n    'FLUX_24', 'FLUXERR_24',\n\n    # GALEX UV magnitudes (for color–color diagrams in Section 5)\n    'MAG_GALEX_NUV', 'MAGERR_GALEX_NUV',\n    'MAG_GALEX_FUV', 'MAGERR_GALEX_FUV'\n]\n\n# Create TAP service object and submit the cone-search query.\n# The WHERE clause selects only sources within the requested radius.\ntap = pyvo.dal.TAPService('https://irsa.ipac.caltech.edu/TAP')\nresult = tap.run_sync(\"\"\"\n           SELECT {}\n           FROM cosmos2015\n           WHERE CONTAINS(POINT('ICRS',ra, dec), CIRCLE('ICRS',{}, {}, {}))=1\n    \"\"\".format(','.join(cols), coords.ra.value, coords.dec.value, radius.to(u.deg).value))\n\n# Convert the TAP result into an astropy Table for further processing.\ncosmos_table = result.to_table()\n\n\nprint(\"Number of objects: \", len(cosmos_table))\n\n\n\n","type":"content","url":"/multiband-photometry#id-1-retrieve-initial-catalog-from-irsa","position":15},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"1.1 Filter Catalog","lvl2":"1. Retrieve Initial Catalog from IRSA"},"type":"lvl3","url":"/multiband-photometry#id-1-1-filter-catalog","position":16},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"1.1 Filter Catalog","lvl2":"1. Retrieve Initial Catalog from IRSA"},"content":"If desired, you can filter the initial catalog to include only sources that meet specific criteria.\n\n# Here is an example of how to filter the catalog to\n# select those rows with either chandra fluxes or GALEX NUV fluxes\n\n# example_table = cosmos_table[(cosmos_table['flux_chandra_05_10']> 0) | (cosmos_table['flux_galex_fuv'] > 0)]\n\n\n\n","type":"content","url":"/multiband-photometry#id-1-1-filter-catalog","position":17},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"2. Retrieve Image Datasets from the Cloud"},"type":"lvl2","url":"/multiband-photometry#id-2-retrieve-image-datasets-from-the-cloud","position":18},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"2. Retrieve Image Datasets from the Cloud"},"content":"\n\n","type":"content","url":"/multiband-photometry#id-2-retrieve-image-datasets-from-the-cloud","position":19},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"2.1 Use the Fornax cloud access API to obtain the IRAC data from the IRSA S3 bucket","lvl2":"2. Retrieve Image Datasets from the Cloud"},"type":"lvl3","url":"/multiband-photometry#id-2-1-use-the-fornax-cloud-access-api-to-obtain-the-irac-data-from-the-irsa-s3-bucket","position":20},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"2.1 Use the Fornax cloud access API to obtain the IRAC data from the IRSA S3 bucket","lvl2":"2. Retrieve Image Datasets from the Cloud"},"content":"Details in this section may change over time.\n\n# Retrieve the COSMOS service entry from the VO registry using the standard PyVO workflow.\n# This avoids hard-coding the service URL and ensures the query follows VO best practices.\nimage_services = pyvo.regsearch(servicetype='sia')\nirsa_cosmos = [s for s in image_services if 'irsa' in s.ivoid and 'cosmos' in s.ivoid][0]\n\n# The search returns 11191 entries, but unfortunately we cannot really filter efficiently in the query\n# itself (https://irsa.ipac.caltech.edu/applications/Atlas/AtlasProgramInterface.html#inputparam)\n# to get only the Spitzer IRAC results from COSMOS as a mission. We will do the filtering in a next step before download.\ncosmos_results = irsa_cosmos.search(coords).to_table()\n\nspitzer = cosmos_results[cosmos_results['dataset'] == 'IRAC']\n\n\n\n\n\n# This dataset has limited access, thus 'region' should be used instead of 'open'.\n# S3 access should be available from the Fornax Science Console.\n\nfname = spitzer['fname']\nspitzer['cloud_access'] = [(f'{{\"aws\": {{ \"bucket_name\": \"irsa-fornax-testdata\",'\n                            f'              \"region\": \"us-east-1\",'\n                            f'              \"access\": \"restricted\",'\n                            f'              \"key\": \"COSMOS/{fn}\" }} }}') for fn in fname]\n\n\n\n\n\n# Requires https://github.com/nasa-fornax/fornax-cloud-access-API/pull/4\n\ndef fornax_download(data_table, data_subdirectory, access_url_column='access_url',\n                    fname_filter=None, verbose=False):\n    \"\"\"\n    Downloads data files if they do not already exist in the specified directory.\n\n    Parameters\n    ----------\n    data_table : iterable\n        An iterable containing metadata for files to be downloaded. Each element\n        should be a dictionary-like object with at least a 'fname' key.\n    data_subdirectory : str\n        Name of the subdirectory where the downloaded files will be stored.\n    access_url_column : str, optional\n        Column name containing the access URLs for downloading the files.\n        Default is 'access_url'.\n    fname_filter : str, optional\n        If provided, only files whose names contain this substring will be downloaded.\n    verbose : bool, optional\n        If True, print status messages. Default is False.\n\n    Raises\n    ------\n    ValueError\n        If neither 'fname' nor 'name' columns are found in `data_table`.\n\n    Notes\n    -----\n    - The function checks if a file already exists before downloading it.\n    - It creates the target directory if it does not exist.\n    - Uses `fornax.get_data_product()` to retrieve the data handler.\n\n    Examples\n    --------\n    >>> data_table = [{'fname': 'file1.fits', 'access_url': 'https://example.com/file1.fits'},\n    ...               {'fname': 'file2.fits', 'access_url': 'https://example.com/file2.fits'}]\n    >>> fornax_download(data_table, 'my_data', verbose=True)\n    Skipping file1.fits: already exists.\n    Downloaded and saved file2.fits to data/my_data\n    Download process complete.\n    \"\"\"\n    # Define the absolute path of the target directory\n    data_directory = os.path.join(\"data\", data_subdirectory)\n    os.makedirs(data_directory, exist_ok=True)  # Ensure the directory exists\n\n    # Check which filename column exists\n    filename_column = None\n    for col in ['fname', 'name']:\n        if col in data_table.colnames:\n            filename_column = col\n            break  # Use the first found column\n\n    if not filename_column:\n        raise ValueError(\"Error: Neither 'fname' nor 'name' columns found in the data table.\")\n\n    for row in data_table:\n        filename = os.path.basename(row[filename_column])  # Extract filename\n        file_path = os.path.join(data_directory, filename)  # Full file path\n\n        # Skip download if file already exists\n        if os.path.exists(file_path):\n            if verbose:\n                print(f\"Skipping {filename}: already exists.\")\n            continue\n\n        # Apply filename filter, if provided\n        if fname_filter is not None and fname_filter not in filename:\n            continue\n\n        # Download the file\n        handler = fornax.get_data_product(row, 'aws', access_url_column=access_url_column, verbose=verbose)\n        temp_file = handler.download()\n\n        # Move the downloaded file if a local file path is returned\n        if temp_file:\n            shutil.move(temp_file, file_path)\n            if verbose:\n                print(f\"Downloaded and saved {filename} to {data_directory}\")\n\n    if verbose:\n        print(\"Download process complete.\")\n\n\n\n\n\nfornax_download(spitzer, access_url_column='sia_url', fname_filter='go2_sci',\n                data_subdirectory='IRAC', verbose=False)\n\n\n\n","type":"content","url":"/multiband-photometry#id-2-1-use-the-fornax-cloud-access-api-to-obtain-the-irac-data-from-the-irsa-s3-bucket","position":21},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"2.2 Obtain GALEX from the MAST archive","lvl2":"2. Retrieve Image Datasets from the Cloud"},"type":"lvl3","url":"/multiband-photometry#id-2-2-obtain-galex-from-the-mast-archive","position":22},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"2.2 Obtain GALEX from the MAST archive","lvl2":"2. Retrieve Image Datasets from the Cloud"},"content":"\n\n# The GALEX COSMOS mosaic is split into four separate tiles, each with its own\n# central pointing. To determine which tile each catalog source belongs to,\n# we compute the angular distance from every source to the center of each tile.\n\n# Coordinates for the four GALEX tile centers (in degrees)\nra_center=[150.369,150.369,149.869,149.869]\ndec_center=[2.45583,1.95583,2.45583,1.95583]\n\n# Build SkyCoord objects for the tile centers and for all catalog sources\ngalex = SkyCoord(ra = ra_center*u.degree, dec = dec_center*u.degree)\ncatalog = SkyCoord(ra = cosmos_table['ra'], dec = cosmos_table['dec'])\n\n# For each of the four tiles, compute the angular separation (in degrees)\n# between the tile center and every source in the catalog. These columns will\n# allow us to identify which tile each source falls into.\ncosmos_table['COSMOS_01'] = galex[0].separation(catalog)\ncosmos_table['COSMOS_02'] = galex[1].separation(catalog)\ncosmos_table['COSMOS_03'] = galex[2].separation(catalog)\ncosmos_table['COSMOS_04'] = galex[3].separation(catalog)\n\n# Convert to pandas for easier column operations and downstream merging\ndf = cosmos_table.to_pandas()\n\n# For each source, identify the GALEX tile with the *minimum* separation.\n# The result is a label ('COSMOS_01' ... 'COSMOS_04') indicating which of the\n# four GALEX mosaics should be used when extracting UV cutouts.\ndf['galex_image'] = df[['COSMOS_01','COSMOS_02','COSMOS_03','COSMOS_04']].idxmin(axis = 1)\n\n\n\n\n\n# We expect 76k rows with 15arcmin diameter IRAC images\ndf.describe()\n\n\n\n\n\n# Download both the images and the skybg files using astroquery.mast\ngalex_images = galex_get_images(coords, verbose=True)\n\n\n\n\n\n# Check the catalog for missing or invalid values to ensure the data are usable.\ndf.isna().sum()\n\n# It is acceptable for some flux columns to contain missing values;\n# the remaining columns are complete.\n\n\n\n\n\n# As an exploratory check, examine how many sources of each\n# classification type appear in the catalog.\n#Type: 0 = galaxy, 1 = star, 2 = X-ray source, -9 is failure to fit\ndf.type.value_counts()\n\n\n\n","type":"content","url":"/multiband-photometry#id-2-2-obtain-galex-from-the-mast-archive","position":23},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"3. Run Forced Photometry"},"type":"lvl2","url":"/multiband-photometry#id-3-run-forced-photometry","position":24},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"3. Run Forced Photometry"},"content":"This section performs forced photometry at the positions of all COSMOS2015 sources using The Tractor. We prepare the inputs required for Tractor by first collecting the IRAC and GALEX images and then supplying the corresponding point response functions (PRFs), which describe how a point source is spread across the detector. The PRFs are included with the notebook rather than downloaded at runtime because they are stable, calibrated instrument files that do not change across observations; if they were not provided, users could obtain them from the Spitzer IRAC Instrument Handbook or the GALEX calibration database. Next, we identify nearby contaminating sources for each target, estimate the local sky background, and define the parameters needed to extract a cutout around each object. The following two code cells create Band objects — small data structures that bundle together the PRF, pixel scale, cutout size, flux-conversion factor, and band index — so that each IRAC and GALEX image can be handled consistently during the photometry. Once these inputs are assembled, Tractor is run to measure instrumental fluxes and uncertainties in each band. These measurements form the foundation of the multiband catalog constructed later in the workflow.\n\n","type":"content","url":"/multiband-photometry#id-3-run-forced-photometry","position":25},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"3.1 Setup","lvl2":"3. Run Forced Photometry"},"type":"lvl3","url":"/multiband-photometry#id-3-1-setup","position":26},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"3.1 Setup","lvl2":"3. Run Forced Photometry"},"content":"In this step we initialize the DataFrame columns that will hold the Tractor-derived fluxes and uncertainties. We then assemble the configuration for each IRAC and GALEX band, including the PRFs, pixel scales, and cutout sizes, and load the corresponding science and background images needed for the photometry. This setup ensures that all band-specific parameters are ready for the main photometry loop.\n\n# initialize columns in data frame for photometry results\ncols = [\"ch1flux\", \"ch1flux_unc\", \"ch2flux\", \"ch2flux_unc\", \"ch3flux\", \"ch3flux_unc\",\n        \"ch4flux\", \"ch4flux_unc\", \"ch5flux\", \"ch5flux_unc\", \"ch6flux\", \"ch6flux_unc\"]\ndf[cols] = 0.0\n\n# list to collect all the bands\nall_bands = []\n\n\n\n\n\n# IRAC channels\nirac_band_indexes = [\n    0,  # ch1\n    1,  # ch2\n    2,  # ch3\n    3,  # ch4\n]\n\nirac_fluxconversion = (1E12) / (4.254517E10) * (0.6) *(0.6)\n\nirac_mosaic_pix_scale = 0.6\n\nirac_cutout_width = 10 # in arcseconds, taken from Nyland et al. 2017\n\nirac_prfs = [\n    fits.open('data/IRAC/PRF_IRAC_ch1.fits')[0].data,\n    fits.open('data/IRAC/PRF_IRAC_ch2.fits')[0].data,\n    fits.open('data/IRAC/PRF_IRAC_ch3.fits')[0].data,\n    fits.open('data/IRAC/PRF_IRAC_ch4.fits')[0].data,\n]\n\n# zip parameters for each band into a container and append to the master list\nirac_bands = [\n    Band(\n        idx, prf, irac_cutout_width, irac_fluxconversion, irac_mosaic_pix_scale\n    )\n    for idx, prf in zip(irac_band_indexes, irac_prfs)\n]\nall_bands += irac_bands\n\n\n\n\n\n# GALEX bands\ngalex_band_indexes = [\n    4,  # nuv\n    5,  # fuv\n]\n\ngalex_cutout_width = 40\n\ngalex_fluxconversions = [\n    3.373E1,  # uJy. fudging this to make the numbers bigger for plotting later\n    1.076E2,  # uJy. fudging this to make the numbers bigger for plotting later\n]\n\ngalex_mosaic_pix_scale = 1.5\n\nprf_nuv = fits.open(\"data/Galex/PSFnuv_faint.fits\")[0].data\nprf_fuv = fits.open(\"data/Galex/PSFfuv.fits\")[0].data\nprf_nuv = prf_nuv[0:119, 0:119]\nprf_fuv = prf_fuv[0:119, 0:119]\n\n# These are much larger than the cutouts we are using, so only keep the central\n# region which is the size of our cutouts\nngalex_pix = galex_cutout_width / galex_mosaic_pix_scale\nprf_cen = int(60)\nprf_nuv = prf_nuv[(prf_cen - int(ngalex_pix / 2) - 1) : (prf_cen + int(ngalex_pix / 2)),\n                  (prf_cen - int(ngalex_pix / 2) - 1) : (prf_cen + int(ngalex_pix / 2))]\nprf_fuv = prf_fuv[(prf_cen - int(ngalex_pix / 2) - 1) : (prf_cen + int(ngalex_pix / 2)),\n                  (prf_cen - int(ngalex_pix / 2) - 1) : (prf_cen + int(ngalex_pix / 2))]\ngalex_prfs = [prf_nuv, prf_fuv]\n\n# Zip parameters for each band into a container and append to the master list\ngalex_bands = [\n    Band(\n        idx, prf, galex_cutout_width, flux_conv, galex_mosaic_pix_scale\n    )\n    for idx, prf, flux_conv in zip(galex_band_indexes, galex_prfs, galex_fluxconversions)\n]\nall_bands += galex_bands\n\n\n\n\n\n#Collect input images\n# collect the files in pairs: (science image, sky-background image)\n# if the same file should be used for both, just send it once\nsci_bkg_pairs = [\n    # IRAC. use the science image to calculate the background\n    ('data/IRAC/irac_ch1_go2_sci_10.fits', ),\n    ('data/IRAC/irac_ch2_go2_sci_10.fits', ),\n    ('data/IRAC/irac_ch3_go2_sci_10.fits', ),\n    ('data/IRAC/irac_ch4_go2_sci_10.fits', ),\n    # GALEX. calculate the background from a dedicated file\n    ('data/Galex/COSMOS_01-nd-int.fits.gz', 'data/Galex/COSMOS_01-nd-skybg.fits.gz'),\n    ('data/Galex/COSMOS_01-fd-int.fits.gz', 'data/Galex/COSMOS_01-fd-skybg.fits.gz'),\n    ('data/Galex/COSMOS_02-nd-int.fits.gz', 'data/Galex/COSMOS_02-nd-skybg.fits.gz'),\n    ('data/Galex/COSMOS_02-fd-int.fits.gz', 'data/Galex/COSMOS_02-fd-skybg.fits.gz'),\n    ('data/Galex/COSMOS_03-nd-int.fits.gz', 'data/Galex/COSMOS_03-nd-skybg.fits.gz'),\n    ('data/Galex/COSMOS_03-fd-int.fits.gz', 'data/Galex/COSMOS_03-fd-skybg.fits.gz'),\n    ('data/Galex/COSMOS_04-nd-int.fits.gz', 'data/Galex/COSMOS_04-nd-skybg.fits.gz'),\n    ('data/Galex/COSMOS_04-fd-int.fits.gz', 'data/Galex/COSMOS_04-fd-skybg.fits.gz'),\n]\n\n\n\n","type":"content","url":"/multiband-photometry#id-3-1-setup","position":27},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"3.2 Main Function to do the Forced Photometry","lvl2":"3. Run Forced Photometry"},"type":"lvl3","url":"/multiband-photometry#id-3-2-main-function-to-do-the-forced-photometry","position":28},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"3.2 Main Function to do the Forced Photometry","lvl2":"3. Run Forced Photometry"},"content":"\n\ndef calc_instrflux(band, ra, dec, stype, ks_flux_aper2, img_pair, df):\n    \"\"\"\n    Calculate single-band instrumental fluxes and uncertainties at the given RA, Dec\n    using tractor.\n\n    Parameters:\n    -----------\n    band : `Band`\n        Collection of parameters for a single band.\n        A `Band` is a named tuple with the following attributes:\n            idx : int\n                Identifier for the band/channel.\n                (integer in [0, 1, 2, 3, 4, 5] for the four IRAC bands and two GALEX bands)\n            prf : np.ndarray\n                Point spread function for the band/channel.\n            cutout_width : int\n                width of desired cutout in arcseconds\n            flux_conv : float\n                factor used to convert tractor result to microjanskies\n            mosaic_pix_scale : float\n                Pixel scale of the image\n    ra, dec : float\n        celestial coordinates for measuring photometry\n    stype : int\n        0, 1, 2, -9 for star, galaxy, x-ray source\n    ks_flux_aper_2 : float\n        flux in aperture 2\n    img_pair : tuple\n        Pair of images for science and background respectively.\n        If the tuple only contains one element it will serve double duty.\n        A tuple element can be a `fits.ImageHDU` or the path to a FITS file as a `str`.\n    df : pd.DataFrame\n        Source catalog.\n        Previous arguments (ra, dec, stype, ks_flux_aper_2) come from a single row of this df.\n        However, we must also pass the entire dataframe in order to find nearby sources which are possible contaminates.\n\n    Returns:\n    --------\n    outband : int\n        reflects the input band index for identification purposes\n    flux : float\n        Measured flux in microJansky.\n        NaN if the forced photometery failed.\n    flux_unc : float\n        Flux uncertainty in microJansky, calculated from the tractor results.\n        NaN if the forced photometery failed or if tractor didn't report a flux variance.\n    \"\"\"\n\n    # Extract a cutout centered on the target object.\n    # This returns:\n    #   - subimage:    the science cutout\n    #   - bkgsubimage: small background region used for sky estimation\n    #   - x1, y1:      pixel coordinates of the target within the cutout\n    #   - subimage_wcs: WCS for converting RA/Dec of neighbors into pixel coords\n    subimage, bkgsubimage, x1, y1, subimage_wcs = cutout.extract_pair(\n        ra, dec, img_pair=img_pair, cutout_width=band.cutout_width, mosaic_pix_scale=band.mosaic_pix_scale\n    )\n\n    # Identify all nearby sources that fall inside the cutout.\n    # These “confusing sources” are modeled alongside the target so that Tractor\n    # can correctly account for blending and crowding. `objsrc` is a list of\n    # Tractor-ready source objects (target first, then neighbors).\n    objsrc, nconfsrcs = find_nconfsources(\n        ra, dec, stype, ks_flux_aper2, x1, y1, band.cutout_width, subimage_wcs, df\n    )\n\n    # Estimate the local sky background and noise from the background cutout.\n    # Tractor needs this information for likelihood evaluation.\n    skymean, skynoise = photometry.calc_background(bkgsubimage=bkgsubimage)\n\n    # Run Tractor to fit the model PRF + sky + neighbors to the science cutout.\n    # If Tractor cannot converge (common when sources are faint or blended),\n    # return NaNs to signal failure for this band.\n    try:\n        flux_var = photometry.run_tractor(\n            subimage=subimage, prf=band.prf, objsrc=objsrc, skymean=skymean, skynoise=skynoise\n        )\n    except TractorError:\n        return (band.idx, np.nan, np.nan)\n\n    # Convert Tractor’s instrumental flux and variance into physical units\n    # (microJanskys) using the band-specific flux conversion factor.\n    # The uncertainty is derived from the reported flux variance.\n    microJy_flux, microJy_unc = photometry.interpret_tractor_results(\n        flux_var=flux_var, flux_conv=band.flux_conv, objsrc=objsrc, nconfsrcs=nconfsrcs\n    )\n\n    return (band.idx, microJy_flux, microJy_unc)\n\n\n\n","type":"content","url":"/multiband-photometry#id-3-2-main-function-to-do-the-forced-photometry","position":29},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"3.3 Calculate Forced Photometry with Straightforward but Slow Method","lvl2":"3. Run Forced Photometry"},"type":"lvl3","url":"/multiband-photometry#id-3-3-calculate-forced-photometry-with-straightforward-but-slow-method","position":30},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"3.3 Calculate Forced Photometry with Straightforward but Slow Method","lvl2":"3. Run Forced Photometry"},"content":"Here we demonstrate a baseline implementation of forced photometry that processes each source sequentially and computes fluxes for all bands. Although this approach is simple and instructive, it is computationally inefficient for large catalogs, and we therefore do not use it.\n\n## Do the calculation without multiprocessing for benchmarking\n\n## Make a copy for parallel computation\n#pl_df = df.copy(deep=True)\n\n#t0 = time.time()\n## For each object\n#for row in df.itertuples():\n#    # For each band\n#    for band in range(6):\n#        # Measure the flux with tractor\n#        outband, flux, unc = calc_instrflux(band, row.ra, row.dec, row.type, row.ks_flux_aper2)\n\n#        # Put the results back into the dataframe\n#        df.loc[row.Index, 'ch{:d}flux'.format(outband+1)] = flux\n#        df.loc[row.Index, 'ch{:d}flux_unc'.format(outband+1)] = unc\n\n#t1 = time.time()\n\n##10,000 sources took 1.5 hours with this code on a medium sized machine\n\n\n\n","type":"content","url":"/multiband-photometry#id-3-3-calculate-forced-photometry-with-straightforward-but-slow-method","position":31},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"3.4 Calculate Forced Photometry - Parallelization","lvl2":"3. Run Forced Photometry"},"type":"lvl3","url":"/multiband-photometry#id-3-4-calculate-forced-photometry-parallelization","position":32},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"3.4 Calculate Forced Photometry - Parallelization","lvl2":"3. Run Forced Photometry"},"content":"To improve performance, we parallelize the forced photometry computation across multiple CPU cores. We construct a parameter list containing the source coordinates and band configurations, distribute the workload to worker processes, and combine the results into the main DataFrame. This parallel implementation drastically reduces runtime and enables practical processing of large areas of the COSMOS field.\n\n# Setup:\n\n# list of parameter sets to pass to the parallel photometry function\nparamlist = []\n\nfor row in df.itertuples():\n    for band in all_bands:\n         # Select the appropriate science/background image pair for this band and GALEX tile\n        img_pair = lookup_img_pair(sci_bkg_pairs, band.idx, row.galex_image)\n\n        # Append the full set of parameters needed by calc_instrflux()\n        paramlist.append(\n            [row.Index, band, row.ra, row.dec, row.type, row.ks_flux_aper2, img_pair, df]\n        )\nprint('paramlist: ', len(paramlist)) # total number of photometry jobs to run\n\n\n\n\n\n# Here we show how the calc_instrflux function works on a single source\ncalc_instrflux(*paramlist[0][1:])\n\n# Another way to run calc_instrflux with different syntax than the line above:\n# calc_instrflux(paramlist[0][1], paramlist[0][2], paramlist[0][3], paramlist[0][4], paramlist[0][5], paramlist[0][6])\n\n\n\n\n\n#wrapper to measure the photometry on a single object, single band\ndef calculate_flux(args):\n    \"\"\"\n    Wrapper function for parallel photometry that computes fluxes for a\n    single (source, band) combination.\n\n    This function unpacks the argument list expected by ``calc_instrflux()``,\n    calls that function to perform the forced photometry, and returns both\n    the job index and the photometry result. It also performs lightweight\n    logging by writing progress updates to a log file every 100 iterations.\n\n    Parameters\n    ----------\n    args : list or tuple\n        A structured argument list of the form\n        ``[index, band, ra, dec, stype, ks_flux_aper2, img_pair, df]``\n        where:\n            index : int\n                Row index of the source in the input catalog.\n            band : Band\n                Band configuration object used by Tractor.\n            ra, dec : float\n                Celestial coordinates of the source (degrees).\n            stype : int\n                Source type code for Tractor (0 = star, 1 = galaxy, etc.).\n            ks_flux_aper2 : float\n                Ks-band flux used as an initial flux estimate.\n            img_pair : tuple\n                Science/background image pair for the band.\n            df : pandas.DataFrame\n                Full catalog used to identify neighboring contaminants.\n\n    Returns\n    -------\n    index : int\n        The original job index corresponding to this source–band combination.\n    val : tuple\n        Output from ``calc_instrflux()``, containing\n        ``(band_index, flux_microJy, flux_unc_microJy)``.\n\n    Notes\n    -----\n    This function is designed to be executed inside a multiprocessing pool,\n    which requires functions to accept a single argument. It serves as a\n    thin wrapper around ``calc_instrflux()`` and adds optional logging\n    of long-running jobs for monitoring purposes.\n    \"\"\"\n\n    val = calc_instrflux(*args[1:])\n    # add simple logging\n    if (args[0] % 100) == 0 and val[0] == 0:\n        with open('output/output.log', 'a') as fp: fp.write(f'{args[0]}\\n')\n    return(args[0], val)\n\n\n\n\n\n# if results were previously saved to this location, load them\n# else start a pool of workers to calculate results in parallel, and save them here\n\n\n# File where parallel photometry results are stored (one file per search radius)\nfname = f'output/results_{radius.value}.npz'\n\n# If cached results exist, load them to avoid re-running the computation\nif os.path.exists(fname):\n    results = np.load(fname, allow_pickle=True)['results']\n\n# Otherwise, compute all fluxes in parallel and save the output\nelse:\n    from  multiprocessing import Pool\n    t0 = time.time()\n    # Reset the lightweight progress log\n    with open('output/output.log', 'w') as fp: fp.write('')\n\n    # Launch a multiprocessing pool to compute fluxes for all parameter sets\n    with Pool() as pool:\n        results = pool.map(calculate_flux, paramlist)\n\n    # Save execution time and final results\n    dtime = time.time() - t0\n    np.savez(fname, results=np.array(results, dtype=object))\n    print(f'Parallel calculation took {dtime} seconds')\n\n\n\n\n\n# Put the results into the main daraframe\nfor res in results:\n    idx,(ich, val, err) = res\n    df.loc[idx, f'ch{ich+1}flux'] = val\n    df.loc[idx, f'ch{ich+1}flux_unc'] = err\n\n\n\n\n\n# Count the number of non-zero ch1 fluxes\nprint('Parallel calculation: number of ch1 fluxes filled in =',\n      np.sum(df.ch1flux > 0))\n\n\n\n","type":"content","url":"/multiband-photometry#id-3-4-calculate-forced-photometry-parallelization","position":33},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"3.5 Cleanup","lvl2":"3. Run Forced Photometry"},"type":"lvl3","url":"/multiband-photometry#id-3-5-cleanup","position":34},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"3.5 Cleanup","lvl2":"3. Run Forced Photometry"},"content":"\n\n# Had to call the GALEX flux columns ch5 and ch6\n# Fix that by renaming them now\ncols = {'ch5flux':'nuvflux', 'ch5flux_unc':'nuvflux_unc','ch6flux':'fuvflux', 'ch6flux_unc':'fuvflux_unc'}\ndf.rename(columns=cols, inplace = True)\n\n\n\n\n\n# When doing a large run of a large area, save the dataframe with the forced photometry\n# so we don't have to do the forced photometry every time\n\ndf.to_pickle(f'output/COSMOS_{radius.value}arcmin.pkl')\n\n\n\n\n\n#If you are not runnig the forced photometry, then read in the catalog from a previous run\n\n#df = pd.read_pickle('output/COSMOS_48.0arcmin.pkl')\n\n\n\n","type":"content","url":"/multiband-photometry#id-3-5-cleanup","position":35},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"3.6 Plot to Confirm our Photometry Results","lvl2":"3. Run Forced Photometry"},"type":"lvl3","url":"/multiband-photometry#id-3-6-plot-to-confirm-our-photometry-results","position":36},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"3.6 Plot to Confirm our Photometry Results","lvl2":"3. Run Forced Photometry"},"content":"In this step we evaluate the quality of the Tractor-derived fluxes by comparing them to the corresponding SPLASH IRAC fluxes from the COSMOS2015 catalog. Scatter plots with reference lines allow us to check for consistency and ensure that the forced photometry behaves as expected across all four IRAC channels. These diagnostic figures serve as an important validation of the method.\n\n# Plot tractor fluxes vs. catalog splash fluxes\n# Should see a straightline with a slope of 1\n\n# Setup to plot\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\nfluxmax = 200\nymax = 80\nxmax = 80\n\n# Ch1\n# First shrink the dataframe to only those rows where we have tractor photometry\ndf_tractor = df[(df.splash_1_flux> 0) & (df.splash_1_flux < fluxmax)] #200\n#sns.regplot(data = df_tractor, x = \"splash_1_flux\", y = \"ch1flux\", ax = ax1, robust = True)\nsns.scatterplot(data = df_tractor, x = \"splash_1_flux\", y = \"ch1flux\", ax = ax1)\n\n# Add a diagonal line with y = x\nlims = [\n    np.min([ax1.get_xlim(), ax1.get_ylim()]),  # min of both axes\n    np.max([ax1.get_xlim(), ax1.get_ylim()]),  # max of both axes\n]\n\n# Now plot both limits against each other\nax1.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\nax1.set(xlabel = r'COSMOS 2015 flux ($\\mu$Jy)', ylabel = r'tractor flux ($\\mu$Jy)', title = 'IRAC 3.6')\nax1.set_ylim([0, ymax])\nax1.set_xlim([0, xmax])\n\n# Ch2\n# First shrink the dataframe to only those rows where we have tractor photometry\ndf_tractor = df[(df.splash_2_flux> 0) & (df.splash_2_flux < fluxmax)]\nsns.scatterplot(data = df_tractor, x = \"splash_2_flux\", y = \"ch2flux\", ax = ax2)\n\n# Add a diagonal line with y = x\nlims = [\n    np.min([ax2.get_xlim(), ax2.get_ylim()]),  # min of both axes\n    np.max([ax2.get_xlim(), ax2.get_ylim()]),  # max of both axes\n]\n\n# Now plot both limits against each other\nax2.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\nax2.set(xlabel = r'COSMOS 2015 flux ($\\mu$Jy)', ylabel = r'tractor flux ($\\mu$Jy)', title = 'IRAC 4.5')\nax2.set_ylim([0, ymax])\nax2.set_xlim([0, xmax])\n\n\n# Ch3\n# First shrink the dataframe to only those rows where we have tractor photometry\ndf_tractor = df[(df.splash_3_flux> 0) & (df.splash_3_flux < fluxmax)]\n\nsns.scatterplot(data = df_tractor, x = \"splash_3_flux\", y = \"ch3flux\", ax = ax3)\n\n# Add a diagonal line with y = x\nlims = [\n    np.min([ax3.get_xlim(), ax3.get_ylim()]),  # min of both axes\n    np.max([ax3.get_xlim(), ax3.get_ylim()]),  # max of both axes\n]\n\n# Now plot both limits against each other\nax3.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\nax3.set(xlabel = r'COSMOS 2015 flux ($\\mu$Jy)', ylabel = r'tractor flux ($\\mu$Jy)', title = 'IRAC 5.8')\nax3.set_ylim([0, ymax])\nax3.set_xlim([0, xmax])\n\n\n# Ch4\n# First shrink the dataframe to only those rows where we have tractor photometry\ndf_tractor = df[(df.splash_4_flux> 0) & (df.splash_4_flux < fluxmax)]\n\nsns.scatterplot(data = df_tractor, x = \"splash_4_flux\", y = \"ch4flux\", ax = ax4)\n\n# Add a diagonal line with y = x\nlims = [\n    np.min([ax4.get_xlim(), ax4.get_ylim()]),  # min of both axes\n    np.max([ax4.get_xlim(), ax4.get_ylim()]),  # max of both axes\n]\n\n# Now plot both limits against each other\nax4.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\nax4.set(xlabel = r'COSMOS 2015 flux ($\\mu$Jy)', ylabel = r'tractor flux ($\\mu$Jy)', title = 'IRAC 8.0')\nax4.set_ylim([0, ymax])\nax4.set_xlim([0, xmax])\n\n\nplt.tight_layout()\n\nfig.subplots_adjust( hspace=0.5)\nfig.set_size_inches(8, 12)\n\n#plt.savefig('output/flux_comparison.png')\n\n\n\nTractor performs well for the IRAC bands. The panels above compare the Tractor-derived fluxes with the corresponding COSMOS2015 SPLASH fluxes for all four IRAC channels. Each blue point represents an individual object from the selected subset of the catalog. The black line shows the one-to-one relation (y = x), providing a visual reference for agreement between the two measurements.\n\n","type":"content","url":"/multiband-photometry#id-3-6-plot-to-confirm-our-photometry-results","position":37},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"4. Cross Match our New Photometry Catalog with an X-ray archival Catalog"},"type":"lvl2","url":"/multiband-photometry#id-4-cross-match-our-new-photometry-catalog-with-an-x-ray-archival-catalog","position":38},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"4. Cross Match our New Photometry Catalog with an X-ray archival Catalog"},"content":"To identify X-ray counterparts to our IRAC sources, we use the nway algorithm (Salvato et al. 2017), a Bayesian cross-matching tool designed for catalogs with different positional uncertainties, different source densities, and multiple possible counterparts. Unlike a simple cone-based positional match, nway evaluates the full probability that each IRAC source corresponds to each Chandra detection by combining:\n\nthe positional uncertainties from both catalogs,\n\nthe local surface density of background sources, and\n\noptional priors such as flux distributions or colors.\n\nThis probabilistic approach is essential for Chandra data in particular, where positional uncertainties vary with off-axis angle and where multiple IRAC sources may lie within the Chandra error ellipse. Because of this, a TAP crossmatch alone would return many candidate matches, but nway rejects those that are statistically unlikely and keeps only the high-probability associations.\n\nThe files\n\ndata/Chandra/COSMOS_chandra.fits\n\ndata/multiband_phot.fits\n\nare pre-generated catalogs included with this tutorial to keep the workflow focused on the cross-matching step rather than on catalog construction.\n\nCOSMOS_chandra.fits\ncontains Chandra source positions and fluxes extracted from the COSMOS Legacy Survey. This file is a cleaned, pre-formatted version of the public catalog (Civano et al.), reduced to the columns needed for nway (RA, Dec, fluxes, positional uncertainties). It is provided so the notebook does not need to repeat the data-reduction steps required to reproduce this catalog.\n\nmultiband_phot.fits\ncontains the IRAC catalog with Tractor-measured fluxes and basic metadata. This file is generated earlier in the tutorial workflow and serves as the input photometric catalog for the crossmatch. It is pre-saved to avoid recomputing the photometry every time the notebook is run.\n\nProviding these files keeps the tutorial lightweight and enables readers to run the cross-match step without requiring dedicated compute time to reproduce the upstream photometry or Chandra catalog reduction.\n\n","type":"content","url":"/multiband-photometry#id-4-cross-match-our-new-photometry-catalog-with-an-x-ray-archival-catalog","position":39},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"4.1 Retrieve the HEASARC Catalog","lvl2":"4. Cross Match our New Photometry Catalog with an X-ray archival Catalog"},"type":"lvl3","url":"/multiband-photometry#id-4-1-retrieve-the-heasarc-catalog","position":40},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"4.1 Retrieve the HEASARC Catalog","lvl2":"4. Cross Match our New Photometry Catalog with an X-ray archival Catalog"},"content":"\n\n# Instantiate Heasarc\nheasarc = Heasarc()\n\n# List all available catalogs\ncatalog_list = heasarc.list_catalogs()\n\n# Print names of catalogs that include \"ccosmoscat\"\n# we already know it is there, but just in case we want to be sure, or if you\n# want to search for a different catalog and confirm its presence\nheasarc.list_catalogs(keywords='ccosmoscat')\n\n# Query the ccosmoscat catalog around our position\nccosmoscat = heasarc.query_region(\n    position=coords,\n    catalog='ccosmoscat',\n    radius=1.0 * u.deg,\n    maxrec=5000,\n    columns='*'  # Use '*' for all columns instead of \"ALL\"\n)\n\n\n\n","type":"content","url":"/multiband-photometry#id-4-1-retrieve-the-heasarc-catalog","position":41},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"4.2 Run nway to do the Cross-Match","lvl2":"4. Cross Match our New Photometry Catalog with an X-ray archival Catalog"},"type":"lvl3","url":"/multiband-photometry#id-4-2-run-nway-to-do-the-cross-match","position":42},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"4.2 Run nway to do the Cross-Match","lvl2":"4. Cross Match our New Photometry Catalog with an X-ray archival Catalog"},"content":"\n\n# Setup:\n\n# Astropy doesn't recognize capitalized units\n# so there might be some warnings here on writing out the file, but we can safely ignore those\n\n# Need to make the chandra catalog into a FITS table\n# and needs to include area of the survey.\nccosmoscat_rad = 1 #radius of chandra cosmos catalog\nnway_write_header('data/Chandra/COSMOS_chandra.fits', 'CHANDRA', float(ccosmoscat_rad**2) )\n\n\n# Also need to transform the main pandas dataframe into FITS table for nway\n# Make an index column for tracking later\ndf['ID'] = range(1, len(df) + 1)\n\n# Need this to be a FITS table and needs to include area of the survey.\nrad_in_arcmin = radius.value  #units attached to this are confusing nway down the line\nnway_write_header('data/multiband_phot.fits', 'OPT', float((2*rad_in_arcmin/60)**2) )\n\n\n\n\n\n%%bash\n\n# Call nway\nnway.py 'data/Chandra/COSMOS_chandra.fits' :ERROR_RADIUS 'data/multiband_phot.fits' 0.1 --out=data/Chandra/chandra_multiband.fits --radius 15 --prior-completeness 0.9\n\n\n\n\n\n# Clean up the cross match results and merge them back into main pandas dataframe\n\n# Read in the nway matched catalog\nxmatch = Table.read('data/Chandra/chandra_multiband.fits', hdu = 1)\ndf_xmatch = xmatch.to_pandas()\n\n# The manual suggests that p_i should be greater than 0.1 for a pure catalog.\n# The matched catalog has multiple optical associations for some of the XMM detections.\n# The simplest thing to do is only keep match_flag = 1\nmatched = df_xmatch.loc[(df_xmatch['p_i']>=0.1) & df_xmatch['match_flag']==1]\n\n# Merge this info back into the df_optical dataframe.\nmerged = pd.merge(df, matched, 'outer',left_on='ID', right_on = 'OPT_ID')\n\n# Remove all the rows which start with \"OPT\" because they are duplications of the original catalog\nmerged = merged.loc[:, ~merged.columns.str.startswith('OPT')]\n\n# Somehow the matching is giving negative fluxes in the band where there is no detection\n# if there is a detection in the other band.\n# Clean that up to make those negative fluxes = 0.\n\nmerged.loc[merged['flux_chandra_2_10'] < 0, 'flux_chandra_2_10'] = 0\nmerged.loc[merged['flux_chandra_05_2'] < 0, 'flux_chandra_05_2'] = 0\n\n\n\n\n\n# How many Chandra sources are there?\n# How many GALEX sources are there?\n\n# Make a new column which is a bool of existing chandra measurements\nmerged['cosmos_chandra_detect'] = 0\nmerged.loc[merged.flux_chandra_2_10 > 0,'cosmos_chandra_detect']=1\n\n# Make one for GALEX too\nmerged['galex_detect'] = 0\nmerged.loc[merged.flux_galex_nuv > 0,'galex_detect']=1\n\n# Make chandra hardness ratio column:\n# Hard = 'flux_chandra_2_10', soft = flux_chandra_05_2, HR = (H-S)/(H+S)\nmerged['chandra_HR'] = (merged['flux_chandra_2_10'] - merged['flux_chandra_05_2']) / (merged['flux_chandra_2_10'] + merged['flux_chandra_05_2'])\n\n\nprint('number of Chandra detections =',np.sum(merged.cosmos_chandra_detect > 0))\nprint('number of GALEX detections =',np.sum(merged.galex_detect > 0))\n\n\n\n","type":"content","url":"/multiband-photometry#id-4-2-run-nway-to-do-the-cross-match","position":43},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"5. Plot Final Results"},"type":"lvl2","url":"/multiband-photometry#id-5-plot-final-results","position":44},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"5. Plot Final Results"},"content":"In this section we visualize the final multiwavelength photometry by examining the color–color distributions of the sources. These plots help reveal how different populations separate in color space and allow us to assess whether the Tractor-derived fluxes produce the expected trends across the IRAC and GALEX bands.\n\n# IRAC color color plots akin to Lacy et al. 2004\n# Overplot GALEX sources\n# Overplot xray sources\n\n# First select on 24 micron\nmerged_24 = merged[(merged.flux_24 >= 0)].copy()\n\n# Negative GALEX fluxes are causing problems, so set those to zero\nmerged_24.loc[merged_24.fuvflux < 0, 'fuvflux'] = 0\nmerged_24.loc[merged_24.nuvflux < 0, 'nuvflux'] = 0\n\n# Make color columns\nmerged_24['F5.8divF3.6'] = merged_24.ch3flux / merged_24.ch1flux\nmerged_24['F8.0divF4.5'] = merged_24.ch4flux / merged_24.ch2flux\n\n# Detected in all IRAC bands\nmerged_allirac = merged_24[(merged_24['F8.0divF4.5'] > 0) & (merged_24['F5.8divF3.6'] > 0)]\n\n# Plot all the points\nfig, ax = plt.subplots()\nsns.scatterplot(data = merged_allirac, x = 'F5.8divF3.6', y = 'F8.0divF4.5',\n                 ax = ax, alpha = 0.5, label = 'all')\n\n# Plot only those points with GALEX detections\ngalex_detect = merged_allirac[merged_allirac.galex_detect > 0]\nsns.scatterplot(data = galex_detect, x = 'F5.8divF3.6', y = 'F8.0divF4.5',\n                 ax = ax, alpha = 0.5, label = 'GALEX detect')\n\n# Plot only those points with chandra detections\nchandra_detect = merged_allirac[merged_allirac.cosmos_chandra_detect > 0]\nsns.scatterplot(data = chandra_detect, x = 'F5.8divF3.6', y = 'F8.0divF4.5',\n                 ax = ax, label = 'Chandra detect')\n\n\n\nax.set(xscale=\"log\", yscale=\"log\")\nax.set_ylim([0.1, 10])\nax.set_xlim([0.1, 10])\n\nax.set(xlabel = 'log F5.8/F3.6', ylabel = 'log F8.0/F4.5')\nax.legend(loc='lower right')\nplt.title('IRAC Color Color Plot')\n\n\n\nThis figure shows an IRAC color color plot akin to the seminal work by Lacy et al. 2004.  Points are color coded for those with GALEX UV detections and those with Chandra x-ray detections. Note that the different populations are seperating out in this color color space.\n\nNote\n\nIf you are running this notebook with the default very small search radius, you may not see many (or any) points in this color–color diagram. Increasing the search radius will populate this plot with a more statistically meaningful sample, but doing so will significantly increase the runtime of the forced-photometry step.\n\n# UV IR color color plot akin to Bouquin et al. 2015\nfig, ax = plt.subplots()\nmerged['FUV-NUV'] = merged.mag_galex_fuv - merged.mag_galex_nuv\nmerged['NUV-3.6'] = merged.mag_galex_nuv - merged.splash_1_mag\n\n# Plot only those points with Galex detections\ngalex_detect = merged[merged.galex_detect > 0]\nsns.kdeplot(data = galex_detect, x = 'NUV-3.6', y = 'FUV-NUV',\n            ax = ax, fill = True, levels = 15)#scatterplot , alpha = 0.5\n\n# Plot only those points with chandra detections\n# We color code Chandra sources by hardness ratio a la Moutard et al. 2020\nchandra_detect = merged[merged.cosmos_chandra_detect > 0]\nsns.scatterplot(data = chandra_detect, x = 'NUV-3.6', y = 'FUV-NUV',\n                ax = ax, hue= 'chandra_HR',palette=\"flare\")\n\n# Make the legend for hue into a colorbar outside the plot\nnorm = plt.Normalize(merged['chandra_HR'].min(), merged['chandra_HR'].max())\nsm = plt.cm.ScalarMappable(cmap=\"flare\", norm=norm)\nsm.set_array([])\n\n# Remove the legend and add a colorbar\nlegend = ax.get_legend()\nif legend is not None:\n    legend.remove()\n\nax.figure.colorbar(sm, ax=ax, label='Chandra Hardness Ratio')\n\n#ax.set(xscale=\"log\", yscale=\"log\")\nax.set_ylim([-0.5, 3.5])\nax.set_xlim([-1, 7])\n\nax.set(xlabel = 'NUV - [3.6]', ylabel = 'FUV - NUV')\n#plt.legend([],[], frameon=False)\n\n#fig.savefig(\"output/color_color.png\")\n#mpld3.display(fig)\n\n\n\nNote\n\nThis UV–IR color–color diagram may also appear sparsely populated when using the notebook’s default small-area selection. Only a fraction of sources in this subset have reliable GALEX detections, and even fewer have accompanying Chandra matches. Expanding the search radius will produce a richer distribution, but the larger area will require longer processing times in the earlier photometry steps.\n\nprint(chandra_detect['chandra_HR'].describe())\n\n\n\nWe extend the works of Bouquin et al. 2015 and Moutard et al. 2020 by showing a GALEX - Spitzer color color diagram over plotted with Chandra detections.  Blue galaxies in these colors are generated by O and B stars and so must currently be forming stars. We find a tight blue cloud in this color space identifying those star forming galaxies.  Galaxies off of the blue cloud have had their star formation quenched, quite possibly by the existence of an AGN through removal of the gas reservoir required for star formation.  Chandra detected galaxies host AGN, and while those are more limited in number, can be shown here to be a hosted by all kinds of galaxies, including quiescent galaxies which would be in the upper right of this plot.  This likely implies that AGN are indeed involved in quenching star formation.  Additionally, we show the Chandra hardness ratio (HR) color coded according to the vertical color bar on the right side of the plot.  Those AGN with higher hardness ratios have their soft x-ray bands heavily obscured and appear to reside preferentially toward the quiescent galaxies.","type":"content","url":"/multiband-photometry#id-5-plot-final-results","position":45},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"About this notebook"},"type":"lvl2","url":"/multiband-photometry#about-this-notebook","position":46},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl2":"About this notebook"},"content":"Authors: Jessica Krick, David Shupe, Marziye JafariYazani, Brigitta Sipőcz, Vandana Desai, Steve Groom, Troy Raen, and the Fornax team\n\nContact: For help with this notebook, please open a topic in the \n\nFornax Community Forum “Support” category.","type":"content","url":"/multiband-photometry#about-this-notebook","position":47},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"Acknowledgements","lvl2":"About this notebook"},"type":"lvl3","url":"/multiband-photometry#acknowledgements","position":48},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"Acknowledgements","lvl2":"About this notebook"},"content":"Kristina Nyland for the workflow of the tractor wrapper.\n\nMAST, HEASARC, & IRSA Fornax teams\n\nSome content in this notebook was created with the assistance of ChatGPT by OpenAI.  All content has been reviewed and validated by the authors to ensure accuracy.","type":"content","url":"/multiband-photometry#acknowledgements","position":49},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"References","lvl2":"About this notebook"},"type":"lvl3","url":"/multiband-photometry#references","position":50},{"hierarchy":{"lvl1":"Automated Multiband Forced Photometry on Large Datasets","lvl3":"References","lvl2":"About this notebook"},"content":"This work made use of:\n\nAstroquery; Ginsburg et al., 2019, 2019AJ....157...98G\n\nAstropy; Astropy Collaboration 2022, Astropy Collaboration 2018, Astropy Collaboration 2013, 2022ApJ...935..167A, 2018AJ....156..123A, 2013A&A...558A..33A\n\nThe Tractor; Lang et al. 2016, 2016AJ....151...36L\n\nNyland et al. 2017 , 2017ApJS..230....9N\n\nSalvato et al. 2018, 2018MNRAS.473.4937S\n\nLaigle et al. 2016, 2016ApJS..224...24L","type":"content","url":"/multiband-photometry#references","position":51},{"hierarchy":{"lvl1":"The Fornax Initiative"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"The Fornax Initiative"},"content":"NASA Astrophysics is developing the \n\nFornax Initiative, a cloud-based system that\nbrings together data, software, and computing so that researchers can focus on science.\n\nFornax Science Console | \n\nDocumentation | \n\nFornax Community Forum","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"The Fornax Initiative","lvl2":"Tutorial Notebooks"},"type":"lvl2","url":"/#tutorial-notebooks","position":2},{"hierarchy":{"lvl1":"The Fornax Initiative","lvl2":"Tutorial Notebooks"},"content":"This collection of tutorial notebooks consists of fully worked science use cases for all users.\nCommon goals of the notebooks are the usage of archival data from all NASA archives, cross-archive work, big data, and computationally intensive science.\nCurrently, there are four major topics for which we have notebooks, use the navigation menu on the left hand side to explore them.\n\nThe notebooks and their run-time dependencies are also available pre-installed at the Fornax Science Console, to learn more about the please consult the \n\nNotebooks in the Console section of the documentation.\n\nThese notebooks are written and maintained using the MyST Markdown format, more details and references about it can be found in the \n\nWorking with Markdown Notebooks documentation section.\n\nList of all the notebooks\n\nThe Fornax Initiative\n\nTime Domain\n\nMake Multi-Wavelength Light Curves Using Archival Data\n\nMake Multi-Wavelength Light Curves for Large Samples\n\nLight Curve Classifier\n\nAGN Zoo: Comparison of AGN selected with different metrics\n\nSpectroscopy\n\nExtract Multi-Wavelength Spectroscopy from Archival Data\n\nMulti-band forced photometry\n\nAutomated Multiband Forced Photometry on Large Datasets\n\nCross Matching\n\nCross-Match ZTF and Pan-STARRS using LSDB","type":"content","url":"/#tutorial-notebooks","position":3},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics"},"type":"lvl1","url":"/ml-agnzoo","position":0},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics"},"content":"","type":"content","url":"/ml-agnzoo","position":1},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"Learning Goals"},"type":"lvl2","url":"/ml-agnzoo#learning-goals","position":2},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will:\n\nWork with multi-band lightcurve data\n\nLearn high dimensional manifold of light curves with UMAPs and SOMs\n\nVisualize and compare different samples on reduced dimension projections/grids","type":"content","url":"/ml-agnzoo#learning-goals","position":3},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"Introduction"},"type":"lvl2","url":"/ml-agnzoo#introduction","position":4},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"Introduction"},"content":"Active Galactic Nuclei (AGNs), some of the most powerful sources in the universe, emit a broad range of electromagnetic radiation, from radio waves to gamma rays. Consequently, there is a wide variety of AGN labels depending on the identification/selection scheme and the presence or absence of certain emissions (e.g., Radio loud/quiet, Quasars, Blazars, Seiferts, Changing looks). According to the unified model, this zoo of labels we see depend on a limited number of parameters, namely the viewing angle, the accretion rate, presence or lack of jets, and perhaps the properties of the host/environment (e.g., \n\nPadovani et al. 2017). Here, we collect archival photometry and labels from the literature to compare how some of these different labels/selection schemes compare.\n\nWe use manifold learning and dimensionality reduction to learn the distribution of AGN lightcurves observed with different facilities. We mostly focus on UMAP (\n\nUniform Manifold Approximation and Projection, McInnes 2020) but also show SOM (\n\nSelf organizing Map, Kohonen 1990) examples. The reduced 2D projections from these two unsupervised ML techniques reveal similarities and overlaps of different selection techniques. Coloring the projections with various statistical physical properties (e.g., mean brightness, fractional lightcurve variation) is informative of correlations of the selections technique with physics such as AGN variability. Using different parts of the EM in training (or in building the initial higher dimensional manifold) demonstrates how much information if any is in that part of the data for each labeling scheme, for example whether with ZTF optical light curves alone, we can identify sources with variability in WISE near IR bands. These techniques also have a potential for identifying targets of a specific class or characteristic for future follow up observations.","type":"content","url":"/ml-agnzoo#introduction","position":5},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl3":"Runtime","lvl2":"Introduction"},"type":"lvl3","url":"/ml-agnzoo#runtime","position":6},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl3":"Runtime","lvl2":"Introduction"},"content":"As of 2024 September, this notebook takes ~160s to run to completion (after installs and imports) on Fornax using the ‘Astrophysics Default Image’ environment and the ‘Large’ server with 16GB RAM/ 4CPU.","type":"content","url":"/ml-agnzoo#runtime","position":7},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"Imports"},"type":"lvl2","url":"/ml-agnzoo#imports","position":8},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"Imports"},"content":"Here are the libraries used in this network. They are also mostly mentioned in the requirements in case you don’t have them installed.\n\nsys and os to handle file names, paths, and directories\n\nnumpy  and pandas to handle array functions\n\nmatplotlib pyplot and cm for plotting data\n\nastropy.io fits for accessing FITS files\n\nastropy.table Table for creating tidy tables of the data\n\nAGNzoo_functions for reading in and prepreocessing of lightcurve data\n\numap and minisom for manifold learning, dimensionality reduction, and visualization\n\nThis cell will install them if needed:\n\n# Uncomment the next line to install dependencies if needed.\n# %pip install -r requirements_ML_AGNzoo.txt\n\n\n\nimport sys\n\nimport astropy.units as u\nfrom astropy.table import Table\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport matplotlib.gridspec as gridspec\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nimport numpy as np\nimport pandas as pd\nsys.path.append('code_src/')\nfrom AGNzoo_functions import (unify_lc, unify_lc_gp, stat_bands, autopct_format, combine_bands,\n                      normalize_clipmax_objects, shuffle_datalabel, dtw_distance,\n                      stretch_small_values_arctan, translate_bitwise_sum_to_labels, update_bitsums)\nfrom collections import Counter, defaultdict\n\nimport umap\nfrom minisom import MiniSom\n\nimport logging\n\n# Get the root logger\nlogger = logging.getLogger()\nlogger.setLevel(logging.WARNING)\n\nplt.style.use('bmh')\ncolors = [\n    \"#3F51B5\",  # Ultramarine Blue\n    \"#003153\",  # Prussian Blue\n    \"#0047AB\",  # Cobalt Blue\n    \"#40826D\",  # Viridian Green\n    \"#50C878\",  # Emerald Green\n    \"#FFEA00\",  # Chrome Yellow\n    \"#CC7722\",  # Yellow Ochre\n    \"#E34234\",  # Vermilion\n    \"#E30022\",  # Cadmium Red\n    \"#D68A59\",  # Raw Sienna\n    \"#8A360F\",  # Burnt Sienna\n    \"#826644\",  # Raw Umber\n]\n\ncustom_cmap = LinearSegmentedColormap.from_list(\"custom_theme\", colors[1:])\n\n\n\n","type":"content","url":"/ml-agnzoo#imports","position":9},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"1. Loading data"},"type":"lvl2","url":"/ml-agnzoo#id-1-loading-data","position":10},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"1. Loading data"},"content":"Here we load a parquet file of light curves generated using the light_curve_collector notebook in this same GitHub repo. With that light_curve_collector notebook, you can build your favorite sample from different sources in the literature and grab the data from archives of interest. This sample contains both spatial coordinates and categorical labels for each AGN. The labels are generated by a bitwise addition of a set of binary indicators. Each binary indicator corresponds to the AGN’s membership in various categories, such as being an SDSS_QSO or a WISE_Variable. For example, an AGN that is both an SDSS_QSO, a WISE_Variable, and also shows ‘Turn_on’ characteristics, would have a label calculated by combining these specific binary indicators using bitwise addition.\n\n%%bash\n\n# To download the data file containing the light curves from Googledrive\ngdown 1gb2vWn0V2unstElGTTrHIIWIftHbXJvz -O ./data/df_lc_020724.parquet.gzip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_lc = pd.read_parquet('data/df_lc_020724.parquet.gzip')\n\n# remove 64 for SPIDER only as its too large compared to the rest of the labels\ndf_lc = df_lc[df_lc.index.get_level_values('label') != '64']\n# remove all bitwise sums that had 64 in them\ndf_lc = update_bitsums(df_lc)\n\n\n\ndf_lc\n\n\n\n","type":"content","url":"/ml-agnzoo#id-1-loading-data","position":11},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl3":"1.1 What is in this sample","lvl2":"1. Loading data"},"type":"lvl3","url":"/ml-agnzoo#id-1-1-what-is-in-this-sample","position":12},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl3":"1.1 What is in this sample","lvl2":"1. Loading data"},"content":"To effectively undertake machine learning (ML) in addressing a specific question, it’s imperative to have a clear understanding of the data we’ll be utilizing. This understanding aids in selecting the appropriate ML approach and, critically, allows for informed and necessary data preprocessing. For example whether a normalization is needed, and what band to choose for normalization.\n\nobjid = df_lc.index.get_level_values('objectid')[:].unique()\nseen = Counter()\n\nfor (objectid, label), singleobj in df_lc.groupby(level=[\"objectid\", \"label\"]):\n    bitwise_sum = int(label)\n    active_labels = translate_bitwise_sum_to_labels(bitwise_sum)\n    seen.update(active_labels)\n# changing order of labels in dictionary only for text to be readable on the plot\nkey_order = ('SDSS_QSO', 'SPIDER_AGN', 'SPIDER_BL', 'SPIDER_QSOBL', 'SPIDER_AGNBL',\n             'WISE_Variable', 'Optical_Variable', 'Galex_Variable', 'Turn-on', 'Turn-off', 'TDE')\nnew_queue = {}\nfor k in key_order:\n    new_queue[k] = seen[k]\n\nplt.figure(figsize=(8, 8))\nplt.title(r'Sample consists of:', size=15)\nh = plt.pie(new_queue.values(), labels=new_queue.keys(), autopct=autopct_format(new_queue.values()),\n            textprops={'fontsize': 15}, startangle=30, labeldistance=1.1,\n            wedgeprops={'linewidth': 3, 'edgecolor': 'white'}, colors=colors)\n\n\n\nIn this particular example, the largest subsamples of AGNs, all with a criteria on redshift (z<1), are from the optical spectra by the \n\nSDSS quasar sample DR16Q, the value added SDSS spectra from \n\nSPIDERS, and a subset of AGNs selected in MIR WISE bands based on their variability (\n\ncsv in data folder credit RChary).\nWe also include some smaller samples from the literature to see where they sit compared to the rest of the population and if they are localized on the 2D projection.\nThese include the Changing Look AGNs from the literature (e.g., \n\nLaMassa et al. 2015, \n\nLyu et al. 2022, \n\nHon et al. 2022), a sample which showed variability in Galex UV images (\n\nWasleske et al. 2022), a sample of variable sources identified in optical Palomar observations (\n\nBaldassare et al. 2020), and the optically variable AGNs in the COSMOS field from a three year program on the VLT(\n\nDe Cicco et al. 2019).\nWe also include 30 Tidal Disruption Event coordinates identified from ZTF light curves \n\nHammerstein et al. (2022).\n\nseen = Counter()\nseen = df_lc.reset_index().groupby('band').objectid.nunique().to_dict()\n\nplt.figure(figsize=(20, 4))\nplt.title(r'Number of lightcurves in each waveband in this sample:', size=20)\nh = plt.bar(seen.keys(), seen.values())\nplt.ylabel(r'#', size=20)\n\n\n\nThe histogram shows the number of lightcurves which ended up in the multi-index data frame from each of the archive calls in different wavebands/filters.\n\ncadence = dict((el, []) for el in seen.keys())\ntimerange = dict((el, []) for el in seen.keys())\n\nfor (_, band), times in df_lc.reset_index().groupby([\"objectid\", \"band\"]).time:\n    cadence[band].append(len(times))\n    if times.max() - times.min() > 0:\n        timerange[band].append(np.round(times.max() - times.min(), 1))\n\nplt.figure(figsize=(20, 4))\nplt.title(r'Time range and cadence covered in each in each waveband averaged over this sample:')\nfor el in cadence.keys():\n    plt.scatter(np.mean(timerange[el]), np.mean(cadence[el]), label=el, s=len(timerange[el]))\n    plt.errorbar(np.mean(timerange[el]), np.mean(cadence[el]),\n                 xerr=np.std(timerange[el]), yerr=np.std(cadence[el]), alpha=0.2)\n    plt.annotate(el, (np.mean(timerange[el]), np.mean(cadence[el])+2), size=12, rotation=40)\nplt.ylabel(r'Average number of visits', size=20)\nplt.xlabel(r'Average baseline (days)', size=20)\nplt.xlim([0, 4000])\nplt.yscale('log')\n\n\n\n\n\nWhile from the histogram plot we see which bands have the highest number of observed lightcurves, what might matter more in finding/selecting variability or changing look in lightcurves is the cadence and the average baseline of observations. For instance, Panstarrs has a large number of lightcurve detections in our sample, but from the figure above we see that the average number of visits and the baseline for those observations are considerably less than ZTF. WISE also shows the longest baseline of observations which is suitable to finding longer term variability in objects.","type":"content","url":"/ml-agnzoo#id-1-1-what-is-in-this-sample","position":13},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"2. Preprocess data for ML (ZTF bands)"},"type":"lvl2","url":"/ml-agnzoo#id-2-preprocess-data-for-ml-ztf-bands","position":14},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"2. Preprocess data for ML (ZTF bands)"},"content":"We first look at this sample only in ZTF bands which have the largest number of visits. We start by unifying the time grid of the light curves so oobjects with different start time or number of observations can be compared. We do this by interpolation to a new grid. The choice of the grid resolution and baseline is strictly dependent on the input data, in this case ZTF, to preserve as much as possible all the information from the observations.\nThe unify_lc, or unify_lc_gp functions do the unification of the lightcurve arrays. For details please see the codes. The time arrays are chosen based on the average duration of observations, with ZTF and WISE covering 1600, 4000 days respectively. We note that we disregard the time of observation of each source, by subtracting the initial time from the array and bringing all lightcurves to the same footing. This has to be taken into account if it influences the science of interest. We then interoplate the time arrays with linear or Gaussian Process regression (unift_lc/ unify_lc_gp respectively). We also remove from the sample objects with less than 5 datapoints in their light curve. We measure basic statistics and combine the tree observed ZTF bands into one longer array as input to dimensionailty reduction after deciding on normalization. We also do a shuffling of the sample to be sure that the separations of different classes by ML are not simply due to the order they are seen in training (in case it is not done by the ML routine itself).\n\nbands_inlc = ['zg', 'zr', 'zi']\n\n# nearest neighbor linear interpolation:\nobjects, dobjects, flabels, keeps = unify_lc(df_lc, bands_inlc, xres=60, numplots=5,\n                                             low_limit_size=5)\n\n# Gaussian process unification\n# objects, dobjects, flabels, keeps = unify_lc_gp(df_lc, bands_inlc, xres=60, numplots=5,\n#                                                 low_limit_size=5)\n\n# keeps can be used as index of objects that are kept in \"objects\" from the initial \"df_lc\",\n# in case information about some properties of sample (e.g., redshifts) is of interest\n# this array of indecies would be helpful\n\n# calculate some basic statistics with a sigmaclipping with width 5sigma\nfvar, maxarray, meanarray = stat_bands(objects, dobjects, bands_inlc, sigmacl=5)\n\n# combine different waveband into one array\ndat_notnormal = combine_bands(objects, bands_inlc)\n\n# Normalize the combinde array by maximum of brightness in a waveband after clipping outliers:\ndat = normalize_clipmax_objects(dat_notnormal, maxarray, band=1)\n\n# Normalize the combinde array by mean brightness in a waveband after clipping outliers:\ndatm = normalize_clipmax_objects(dat_notnormal, meanarray, band=1)\n\n# shuffle data incase the ML routines are sensitive to order\ndata, fzr, p = shuffle_datalabel(dat, flabels)\nfvar_arr, maximum_arr, average_arr = fvar[:, p], maxarray[:, p], meanarray[:, p]\n\n# Initialize labc to hold indices of each unique label\nlabc = {}\nfor index, f in enumerate(fzr):\n    lab = translate_bitwise_sum_to_labels(int(f))\n    for label in lab:\n        if label not in labc:\n            # Initialize the list for this label if it's not already in labc\n            labc[label] = []\n        # Append the current index to the list of indices for this label\n        labc[label].append(index)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe combination of the tree bands into one longer arrays in order of increasing wavelength, can be seen as providing both the SED shape as well as variability in each from the light curve. Figure below demonstrates this as well as our normalization choice. We normalize the data in ZTF R band as it has a higher average numbe of visits compared to G and I band. We remove outliers before measuring the mean and max of the light curve and normalizing by it. This normalization can be skipped if one is mearly interested in comparing brightnesses of the data in this sample, but as dependence on flux is strong to look for variability and compare shapes of light curves a normalization helps.\n\nr = np.random.randint(np.shape(dat)[1])\n\n_, axs = plt.subplots(1, 3, figsize=(18, 4))\nztf_data = [dat_notnormal, dat, datm]\nylabels = [r'Flux ($\\mu Jy$)', r'Normalized Flux (max r band)', r'Normalized Flux (mean r band)']\nfig_contents = list(zip(axs, ztf_data, ylabels))\n\nfor i, l in enumerate(bands_inlc):\n    s = int(np.shape(dat)[1]/len(bands_inlc))\n    first = int(i*s)\n    last = first+s\n    for ax, ydata, ylabel in fig_contents:\n        ax.plot(np.linspace(first, last, s), ydata[r, first:last], 'o', linestyle='--', label=l)\n        ax.set_xlabel(r'Time_[w1, w2, w3]', size=15)\n        ax.set_ylabel(ylabel, size=15)\n_ = axs[0].legend(loc=2)\n\n\n\n","type":"content","url":"/ml-agnzoo#id-2-preprocess-data-for-ml-ztf-bands","position":15},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"3. Learn the Manifold"},"type":"lvl2","url":"/ml-agnzoo#id-3-learn-the-manifold","position":16},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"3. Learn the Manifold"},"content":"Now we can train a UMAP with the processed data vectors above. Different choices for the number of neighbors, minimum distance and metric can be made and a parameter space can be explored. We show here our preferred combination given this data. We choose manhattan distance (also called \n\nthe L1 distance) as it is optimal for the kind of grid we interpolated on, for instance we want the distance to not change if there are observations missing. Another metric appropriate for our purpose in time domain analysis is Dynamic Time Warping (\n\nDTW), which is insensitive to a shift in time. This is helpful as we interpolate the observations onto a grid starting from time 0 and when discussing variability we care less about when it happens and more about whether and how strong it happened. As the measurement of the DTW distance takes longer compared to the other metrics we show examples here with manhattan and only show one example exploring the parameter space including a DTW metric in the last cell of this notebook.\n\nplt.figure(figsize=(18, 6))\nmarkersize=200\nmapper = umap.UMAP(n_neighbors=50, min_dist=0.9, metric='manhattan', random_state=20).fit(data)\n\n\nax1 = plt.subplot(1, 3, 2)\nax1.set_title(r'mean brightness', size=20)\ncf = ax1.scatter(mapper.embedding_[:, 0], mapper.embedding_[:, 1], s=markersize,\n                 c=np.log10(np.nansum(meanarray, axis=0)), edgecolor='gray')\nplt.axis('off')\ndivider = make_axes_locatable(ax1)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\nplt.colorbar(cf, cax=cax)\n\n\nax0 = plt.subplot(1, 3, 3)\nax0.set_title(r'mean fractional variation', size=20)\ncf = ax0.scatter(mapper.embedding_[:, 0], mapper.embedding_[:, 1], s=markersize,\n                 c=stretch_small_values_arctan(np.nansum(fvar_arr, axis=0), factor=3),\n                 edgecolor='gray')\nplt.axis('off')\ndivider = make_axes_locatable(ax0)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\nplt.colorbar(cf, cax=cax)\n\nax2 = plt.subplot(1, 3, 1)\nax2.set_title('sample origin', size=20)\ncounts = 1\nfor label, indices in labc.items():\n    cf = ax2.scatter(mapper.embedding_[indices, 0], mapper.embedding_[indices, 1], s=markersize,\n                     c=colors[counts], alpha=0.5, edgecolor='gray', label=label)\n    counts += 1\nplt.legend(fontsize=12)\n#plt.colorbar(cf)\nplt.axis('off')\n\nplt.tight_layout()\n#plt.savefig('umap-ztf.png')\n\n\n\n\n\nThe left panel is colorcoded by the origin of the sample. The middle panel shows the sum of mean brightnesses in three bands (arbitrary unit) demonstrating that after normalization we see no correlation with brightness. The panel on the right is color coded by a statistical measure of variability (i.e. the fractional variation \n\nsee here). As with the plotting above it is not easy to see all the data points and correlations in the next two cells measure probability of belonging to each original sample as well as the mean statistical property on an interpolated grid on this reduced 2D projected surface.\n\n# Define a grid\ngrid_resolution = 15# Number of cells in the grid\nx_min, x_max = mapper.embedding_[:, 0].min(), mapper.embedding_[:, 0].max()\ny_min, y_max = mapper.embedding_[:, 1].min(), mapper.embedding_[:, 1].max()\nx_grid = np.linspace(x_min, x_max, grid_resolution)\ny_grid = np.linspace(y_min, y_max, grid_resolution)\nx_centers, y_centers = np.meshgrid(x_grid, y_grid)\n\n# Calculate mean property in each grid cell\nmean_property1, mean_property2 = np.zeros_like(x_centers), np.zeros_like(x_centers)\npropmean=stretch_small_values_arctan(np.nansum(meanarray, axis=0), factor=2)\npropfvar=stretch_small_values_arctan(np.nansum(fvar_arr, axis=0), factor=2)\nfor i in range(grid_resolution - 1):\n    for j in range(grid_resolution - 1):\n        mask = (\n            (mapper.embedding_[:, 0] >= x_grid[i]) &\n            (mapper.embedding_[:, 0] < x_grid[i + 1]) &\n            (mapper.embedding_[:, 1] >= y_grid[j]) &\n            (mapper.embedding_[:, 1] < y_grid[j + 1])\n        )\n        if np.sum(mask) > 0:\n            mean_property1[j, i] = np.mean(propmean[mask])\n            mean_property2[j, i] = np.mean(propfvar[mask])\n\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.title('mean brightness')\ncf = plt.contourf(x_centers, y_centers, mean_property1, cmap='viridis', alpha=0.9)\nplt.axis('off')\ndivider = make_axes_locatable(plt.gca())\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\nplt.colorbar(cf, cax=cax)\n\nplt.subplot(1, 2, 2)\nplt.title('mean fractional variation')\ncf = plt.contourf(x_centers, y_centers, mean_property2, cmap='viridis', alpha=0.9)\nplt.axis('off')\ndivider = make_axes_locatable(plt.gca())\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\nplt.colorbar(cf, cax=cax)\n\n\n\n","type":"content","url":"/ml-agnzoo#id-3-learn-the-manifold","position":17},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl3":"3.1 Sample comparison on the UMAP","lvl2":"3. Learn the Manifold"},"type":"lvl3","url":"/ml-agnzoo#id-3-1-sample-comparison-on-the-umap","position":18},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl3":"3.1 Sample comparison on the UMAP","lvl2":"3. Learn the Manifold"},"content":"\n\n# Calculate 2D histogram\nhist, x_edges, y_edges = np.histogram2d(mapper.embedding_[:, 0], mapper.embedding_[:, 1], bins=12)\nplt.figure(figsize=(15, 12))\ni=1\nax0 = plt.subplot(4, 4, 12)\nfor label, indices in sorted(labc.items()):\n    hist_per_cluster, _, _ = np.histogram2d(mapper.embedding_[indices, 0],\n                                            mapper.embedding_[indices, 1],\n                                            bins=(x_edges, y_edges))\n    prob = hist_per_cluster / hist\n    plt.subplot(4, 4, i)\n    plt.title(label)\n    plt.contourf(x_edges[:-1], y_edges[:-1], prob.T, levels=20, alpha=0.8, cmap=custom_cmap)\n    plt.colorbar()\n    plt.axis('off')\n    cf = ax0.scatter(mapper.embedding_[indices, 0], mapper.embedding_[indices, 1], s=80,\n                     alpha=0.5, edgecolor='gray', label=label, c=colors[i-1])\n    i += 1\nax0.legend(loc=4, fontsize=7)\nax0.axis('off')\nplt.tight_layout()\n\n\n\n\n\nFigure above shows how with ZTF light curves alone we can separate some of these AGN samples, where they have overlaps. We can do a similar exercise with other dimensionality reduction techniques. Below we show two SOMs one with normalized and another with no normalization. The advantage of Umaps to SOMs is that in practice you may change the parameters to separate classes of vastly different data points, as distance is preserved on a umap. On a SOM however only topology of higher dimensions is preserved and not distance hence, the change on the 2d grid does not need to be smooth and from one cell to next there might be larg jumps. On the other hand, an advantage of the SOM is that by definition it has a grid and no need for a posterior interpolation (as we did above) is needed to map more data or to measure probabilities, etc.","type":"content","url":"/ml-agnzoo#id-3-1-sample-comparison-on-the-umap","position":19},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl3":"3.2 Reduced dimensions on a SOM grid","lvl2":"3. Learn the Manifold"},"type":"lvl3","url":"/ml-agnzoo#id-3-2-reduced-dimensions-on-a-som-grid","position":20},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl3":"3.2 Reduced dimensions on a SOM grid","lvl2":"3. Learn the Manifold"},"content":"\n\n# Initialization and training\nmsz0, msz1 = 15, 15\nsom = MiniSom(msz0, msz1, data.shape[1], sigma=1.5, learning_rate=.5,\n              neighborhood_function='gaussian', random_seed=0, topology='rectangular')\n\nsom.pca_weights_init(data)\nsom.train(data, 100000, verbose=False)  # random training\n\n\n\nlaborder = ['SDSS_QSO', 'SPIDER_AGN', 'SPIDER_BL', 'SPIDER_QSOBL', 'SPIDER_AGNBL',\n            'WISE_Variable', 'Optical_Variable', 'Galex_Variable', 'Turn-on', 'Turn-off', 'TDE']\n\n\n# Create grid to hold mean fvar per SOM node\nmean_fvar_map = np.full((msz0, msz1), np.nan)\n\n# Create helper to accumulate fvar values in each cell\ncell_fvar = defaultdict(list)\n\n# First, map each data point to its BMU and store its fvar\npropfvar=stretch_small_values_arctan(np.nansum(fvar_arr, axis=0), factor=2)\nfor i in range(len(data)):\n    bmu = som.winner(data[i])  # returns (x, y)\n    fvar_value = fvar_arr[i] if np.ndim(fvar_arr) == 1 else np.mean(propfvar[i])\n    cell_fvar[bmu].append(fvar_value)\n\n# Now compute mean per cell\nfor (x, y), values in cell_fvar.items():\n    mean_fvar_map[x, y] = np.nanmean(values)\n\n# apply stretching for visualization\nheatmap = stretch_small_values_arctan(mean_fvar_map)\nncols = 4\nnrows = int(np.ceil(len(laborder) / ncols))\nfig, axs = plt.subplots(nrows, ncols, figsize=(3*ncols, 3*nrows))\naxs = axs.flatten()\n\nfor i, label in enumerate(laborder):\n    ax = axs[i]\n    im = ax.imshow(heatmap.T, origin='lower', cmap='plasma')\n\n    if label in labc:\n        for idx in labc[label]:\n            x, y = som.winner(data[idx])\n            ax.plot(x, y, 'x', color='white', markersize=8, markeredgewidth=2)\n\n    ax.set_title(label)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n# Hide the extra subplot if laborder < nrows * ncols\nif len(laborder) < len(axs):\n    axs[len(laborder)].axis('off')\n\n# Colorbar outside the plot grid\n# Adjust position as needed (here it's to the right)\ncbar_ax = fig.add_axes([0.99, 0.05, 0.02, 0.9])  # [left, bottom, width, height]\ncbar = fig.colorbar(im, cax=cbar_ax)\ncbar.set_label('Mean Fractional Variability')\n\nplt.subplots_adjust(right=0.9)  # Leave space for colorbar\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThe above SOMs are colored by the mean fractional variation of the lightcurves in all bands (a measure of AGN variability). The crosses are different samples mapped to the trained SOM to see if they are distinguishable on a normalized lightcurve som.\n\n# shuffle data in case the ML routines are sensitive to order\ndata, fzr, p = shuffle_datalabel(dat_notnormal, flabels)\nfvar_arr, maximum_arr, average_arr = fvar[:, p], maxarray[:, p], meanarray[:, p]\n# Initialize labc to hold indices of each unique label\nlabc = {}\nfor index, f in enumerate(fzr):\n    lab = translate_bitwise_sum_to_labels(int(f))\n    for label in lab:\n        if label not in labc:\n            # Initialize the list for this label if it's not already in labc\n            labc[label] = []\n        # Append the current index to the list of indices for this label\n        labc[label].append(index)\n\nsom = MiniSom(msz0, msz1, data.shape[1], sigma=1.5, learning_rate=.5,\n              neighborhood_function='gaussian', random_seed=0, topology='rectangular')\n\nsom.pca_weights_init(data)\nsom.train(data, 100000, verbose=False)  # random training\n\n\n\n# Create grid to hold mean fvar per SOM node\nmean_fvar_map = np.full((msz0, msz1), np.nan)\n\n# Create helper to accumulate fvar values in each cell\ncell_fvar = defaultdict(list)\n\n# First, map each data point to its BMU and store its fvar\npropfvar=stretch_small_values_arctan(np.nansum(fvar_arr, axis=0), factor=2)\nfor i in range(len(data)):\n    bmu = som.winner(data[i])  # returns (x, y)\n    fvar_value = fvar_arr[i] if np.ndim(fvar_arr) == 1 else np.mean(propfvar[i])\n    cell_fvar[bmu].append(fvar_value)\n\n# Now compute mean per cell\nfor (x, y), values in cell_fvar.items():\n    mean_fvar_map[x, y] = np.nanmean(values)\n\n# apply stretching for visualization\nheatmap = stretch_small_values_arctan(mean_fvar_map)\nncols = 4\nnrows = int(np.ceil(len(laborder) / ncols))\nfig, axs = plt.subplots(nrows, ncols, figsize=(3*ncols, 3*nrows))\naxs = axs.flatten()\n\nfor i, label in enumerate(laborder):\n    ax = axs[i]\n    im = ax.imshow(heatmap.T, origin='lower', cmap='plasma')\n\n    if label in labc:\n        for idx in labc[label]:\n            x, y = som.winner(data[idx])\n            ax.plot(x, y, 'x', color='white', markersize=8, markeredgewidth=2)\n\n    ax.set_title(label)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n# Hide the extra subplot if laborder < nrows * ncols\nif len(laborder) < len(axs):\n    axs[len(laborder)].axis('off')\n\n# Colorbar outside the plot grid\n# Adjust position as needed (here it's to the right)\ncbar_ax = fig.add_axes([0.99, 0.05, 0.02, 0.9])  # [left, bottom, width, height]\ncbar = fig.colorbar(im, cax=cbar_ax)\ncbar.set_label('Mean Fractional Variability')\n\nplt.subplots_adjust(right=0.9)  # Leave space for colorbar\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nlabels = [None] * len(data)\nfor label in laborder:\n    if label in labc:\n        for idx in labc[label]:\n            labels[idx] = label\n\nlabels_map = defaultdict(Counter)\n\nfor x, label in zip(data, labels):\n    if label is not None:\n        w = som.winner(x)\n        labels_map[w][label] += 1\n\nfig = plt.figure(figsize=(12, 12))\nthe_grid = gridspec.GridSpec(msz0, msz1, fig)\n\nfor position in labels_map.keys():\n    label_counts = labels_map[position]\n    total = sum(label_counts.values())\n\n    # Use consistent order from laborder\n    fracs = [label_counts.get(label, 0) / total for label in laborder]\n\n    ax = plt.subplot(the_grid[msz1 - 1 - position[1], position[0]], aspect=1)\n    patches, _ = ax.pie(fracs)\n    #ax.set_title(f\"{position}\", fontsize=6)\n    ax.axis('equal')\n\n# Legend outside\nplt.legend(patches, laborder, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nskipping the normalization of lightcurves, further separates turn on/off CLAGNs when looking at ZTF lightcurves only.","type":"content","url":"/ml-agnzoo#id-3-2-reduced-dimensions-on-a-som-grid","position":21},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"4. Repeating the above, this time with ZTF + WISE manifold"},"type":"lvl2","url":"/ml-agnzoo#id-4-repeating-the-above-this-time-with-ztf-wise-manifold","position":22},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"4. Repeating the above, this time with ZTF + WISE manifold"},"content":"\n\nbands_inlc = ['zg', 'zr', 'zi', 'W1', 'W2']\nobjects, dobjects, flabels, keeps = unify_lc(df_lc, bands_inlc, xres=30, numplots=3)\n# calculate some basic statistics\nfvar, maxarray, meanarray = stat_bands(objects, dobjects, bands_inlc)\ndat_notnormal = combine_bands(objects, bands_inlc)\ndat = normalize_clipmax_objects(dat_notnormal, maxarray, band=-1)\ndata, fzr, p = shuffle_datalabel(dat, flabels)\nfvar_arr, maximum_arr, average_arr = fvar[:, p], maxarray[:, p], meanarray[:, p]\n\n# Initialize labc to hold indices of each unique label\nlabc = {}\nfor index, f in enumerate(fzr):\n    lab = translate_bitwise_sum_to_labels(int(f))\n    for label in lab:\n        if label not in labc:\n            # Initialize the list for this label if it's not already in labc\n            labc[label] = []\n        # Append the current index to the list of indices for this label\n        labc[label].append(index)\n\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(18, 6))\nmarkersize=200\nmapper = umap.UMAP(n_neighbors=50, min_dist=0.9, metric='manhattan', random_state=4).fit(data)\n# using dtw distance takes a long time\n# mapper = umap.UMAP(n_neighbors=50, min_dist=0.9, metric=dtw_distance, random_state=20).fit(data)\n\n\nax1 = plt.subplot(1, 3, 2)\nax1.set_title(r'mean brightness', size=20)\ncf = ax1.scatter(mapper.embedding_[:, 0], mapper.embedding_[:, 1], s=markersize,\n                 c=np.log10(np.nansum(meanarray, axis=0)), edgecolor='gray')\nplt.axis('off')\ndivider = make_axes_locatable(ax1)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\nplt.colorbar(cf, cax=cax)\n\n\nax0 = plt.subplot(1, 3, 3)\nax0.set_title(r'mean fractional variation', size=20)\ncf = ax0.scatter(mapper.embedding_[:, 0], mapper.embedding_[:, 1], s=markersize,\n                 c=stretch_small_values_arctan(np.nansum(fvar_arr, axis=0), factor=3),\n                 edgecolor='gray')\nplt.axis('off')\ndivider = make_axes_locatable(ax0)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\nplt.colorbar(cf, cax=cax)\n\nax2 = plt.subplot(1, 3, 1)\nax2.set_title('sample origin', size=20)\ncounts = 1\nfor label, indices in labc.items():\n    cf = ax2.scatter(mapper.embedding_[indices, 0], mapper.embedding_[indices, 1], s=markersize,\n                     c=colors[counts], alpha=0.5, edgecolor='gray', label=label)\n    counts += 1\nplt.legend(fontsize=12)\n#plt.colorbar(cf)\nplt.axis('off')\n\nplt.tight_layout()\n\n\n\n\n\n# Calculate 2D histogram\nhist, x_edges, y_edges = np.histogram2d(mapper.embedding_[:, 0], mapper.embedding_[:, 1], bins=12)\nplt.figure(figsize=(15, 12))\ni=1\nax0 = plt.subplot(4, 4, 12)\nfor label, indices in sorted(labc.items()):\n    hist_per_cluster, _, _ = np.histogram2d(mapper.embedding_[indices, 0],\n                                            mapper.embedding_[indices, 1],\n                                            bins=(x_edges, y_edges))\n    prob = hist_per_cluster / hist\n    plt.subplot(4, 4, i)\n    plt.title(label)\n    plt.contourf(x_edges[:-1], y_edges[:-1], prob.T, levels=20, alpha=0.8, cmap=custom_cmap)\n    plt.colorbar()\n    plt.axis('off')\n    cf = ax0.scatter(mapper.embedding_[indices, 0], mapper.embedding_[indices, 1], s=80,\n                     alpha=0.5, edgecolor='gray', label=label, c=colors[i-1])\n    i += 1\nax0.legend(loc=4, fontsize=7)\nax0.axis('off')\nplt.tight_layout()\n\n\n\n\n\n","type":"content","url":"/ml-agnzoo#id-4-repeating-the-above-this-time-with-ztf-wise-manifold","position":23},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"5. Wise bands alone"},"type":"lvl2","url":"/ml-agnzoo#id-5-wise-bands-alone","position":24},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"5. Wise bands alone"},"content":"\n\nbands_inlcw = ['W1', 'W2']\nobjectsw, dobjectsw, flabelsw, keepsw = unify_lc(df_lc, bands_inlc, xres=30)\n# calculate some basic statistics\nfvarw, maxarrayw, meanarrayw = stat_bands(objectsw, dobjectsw, bands_inlcw)\ndat_notnormalw = combine_bands(objects, bands_inlcw)\ndatw = normalize_clipmax_objects(dat_notnormalw, maxarrayw, band=-1)\ndataw, fzrw, pw = shuffle_datalabel(datw, flabelsw)\nfvar_arrw, maximum_arrw, average_arrw = fvarw[:, pw], maxarrayw[:, pw], meanarrayw[:, pw]\n\n# Initialize labc to hold indices of each unique label\nlabcw = {}\nfor index, f in enumerate(fzrw):\n    lab = translate_bitwise_sum_to_labels(int(f))\n    for label in lab:\n        if label not in labcw:\n            # Initialize the list for this label if it's not already in labc\n            labcw[label] = []\n        # Append the current index to the list of indices for this label\n        labcw[label].append(index)\n\n\n\n\n\n\n\nplt.figure(figsize=(18, 6))\nmarkersize=200\nmapp = umap.UMAP(n_neighbors=50, min_dist=0.9, metric='manhattan', random_state=20).fit(dataw)\n\n\nax1 = plt.subplot(1, 3, 2)\nax1.set_title(r'mean brightness', size=20)\ncf = ax1.scatter(mapp.embedding_[:, 0], mapp.embedding_[:, 1], s=markersize,\n                 c=np.log10(np.nansum(meanarrayw, axis=0)), edgecolor='gray')\nplt.axis('off')\ndivider = make_axes_locatable(ax1)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\nplt.colorbar(cf, cax=cax)\n\n\nax0 = plt.subplot(1, 3, 3)\nax0.set_title(r'mean fractional variation', size=20)\ncf = ax0.scatter(mapp.embedding_[:, 0], mapp.embedding_[:, 1], s=markersize,\n                 c=stretch_small_values_arctan(np.nansum(fvar_arrw, axis=0), factor=3),\n                 edgecolor='gray')\nplt.axis('off')\ndivider = make_axes_locatable(ax0)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\nplt.colorbar(cf, cax=cax)\n\nax2 = plt.subplot(1, 3, 1)\nax2.set_title('sample origin', size=20)\ncounts = 1\nfor label, indices in labcw.items():\n    cf = ax2.scatter(mapp.embedding_[indices, 0], mapp.embedding_[indices, 1], s=markersize,\n                     c=colors[counts], alpha=0.5, edgecolor='gray', label=label)\n    counts += 1\nplt.legend(fontsize=12)\n#plt.colorbar(cf)\nplt.axis('off')\n\nplt.tight_layout()\n\n\n\n\n\n# Calculate 2D histogram\nhist, x_edges, y_edges = np.histogram2d(mapp.embedding_[:, 0], mapp.embedding_[:, 1], bins=12)\nplt.figure(figsize=(15, 12))\ni=1\nax0 = plt.subplot(4, 4, 12)\nfor label, indices in sorted(labcw.items()):\n    hist_per_cluster, _, _ = np.histogram2d(mapp.embedding_[indices, 0],\n                                            mapp.embedding_[indices, 1],\n                                            bins=(x_edges, y_edges))\n    prob = hist_per_cluster / hist\n    plt.subplot(4, 4, i)\n    plt.title(label)\n    plt.contourf(x_edges[:-1], y_edges[:-1], prob.T, levels=20, alpha=0.8, cmap=custom_cmap)\n    plt.colorbar()\n    plt.axis('off')\n    cf = ax0.scatter(mapp.embedding_[indices, 0], mapp.embedding_[indices, 1], s=80,\n                     alpha=0.5, edgecolor='gray', label=label, c=colors[i-1])\n    i += 1\nax0.legend(loc=4, fontsize=7)\nax0.axis('off')\nplt.tight_layout()\n\n\n\n\n\n","type":"content","url":"/ml-agnzoo#id-5-wise-bands-alone","position":25},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"6. UMAP with different metrics/distances on ZTF+WISE"},"type":"lvl2","url":"/ml-agnzoo#id-6-umap-with-different-metrics-distances-on-ztf-wise","position":26},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"6. UMAP with different metrics/distances on ZTF+WISE"},"content":"DTW takes a bit longer compared to other metrics, so it is commented out in the cell below.\n\nplt.figure(figsize=(12, 10))\nmarkersize=200\n\nmapper = umap.UMAP(n_neighbors=50, min_dist=0.9, metric='euclidean', random_state=20).fit(data)\nax0 = plt.subplot(2, 2, 1)\nax0.set_title(r'Euclidean Distance, min_d=0.9, n_neighbors=50', size=12)\nfor label, indices in (labc.items()):\n     cf = ax0.scatter(mapper.embedding_[indices, 0], mapper.embedding_[indices, 1], s=80,\n                      alpha=0.5, edgecolor='gray', label=label)\nplt.axis('off')\n\nmapper = umap.UMAP(n_neighbors=50, min_dist=0.9, metric='manhattan', random_state=20).fit(data)\nax0 = plt.subplot(2, 2, 2)\nax0.set_title(r'Manhattan Distance, min_d=0.9, n_neighbors=50', size=12)\nfor label, indices in (labc.items()):\n     cf = ax0.scatter(mapper.embedding_[indices, 0], mapper.embedding_[indices, 1], s=80,\n                      alpha=0.5, edgecolor='gray', label=label)\nplt.axis('off')\n\n\n# This distance takes long\n# mapperg = umap.UMAP(n_neighbors=50, min_dist=0.9, metric=dtw_distance, random_state=20).fit(data)\n# ax2 = plt.subplot(2, 2, 3)\n# ax2.set_title(r'DTW Distance, min_d=0.9, n_neighbors=50', size=12)\n# for label, indices in (labc.items()):\n#      cf = ax2.scatter(mapper.embedding_[indices, 0], mapper.embedding_[indices, 1], s=80,\n#                       alpha=0.5, edgecolor='gray', label=label)\n# plt.axis('off')\n\n\nmapper = umap.UMAP(n_neighbors=50, min_dist=0.1, metric='manhattan', random_state=20).fit(data)\nax0 = plt.subplot(2, 2, 4)\nax0.set_title(r'Manhattan Distance, min_d=0.1, n_neighbors=50', size=12)\nfor label, indices in (labc.items()):\n     cf = ax0.scatter(mapper.embedding_[indices, 0], mapper.embedding_[indices, 1], s=80,\n                      alpha=0.5, edgecolor='gray', label=label)\nplt.legend(fontsize=12)\nplt.axis('off')\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/ml-agnzoo#id-6-umap-with-different-metrics-distances-on-ztf-wise","position":27},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"About this notebook"},"type":"lvl2","url":"/ml-agnzoo#about-this-notebook","position":28},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl2":"About this notebook"},"content":"Authors: Shoubaneh Hemmati (IRSA Research Scientist) and the Fornax team\n\nContact: For help with this notebook, please open a topic in the \n\nFornax Community Forum “Support” category.","type":"content","url":"/ml-agnzoo#about-this-notebook","position":29},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl3":"Acknowledgements","lvl2":"About this notebook"},"type":"lvl3","url":"/ml-agnzoo#acknowledgements","position":30},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl3":"Acknowledgements","lvl2":"About this notebook"},"content":"Parts of this notebook will be presented in Hemmati et al. (in prep)","type":"content","url":"/ml-agnzoo#acknowledgements","position":31},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl3":"References","lvl2":"About this notebook"},"type":"lvl3","url":"/ml-agnzoo#references","position":32},{"hierarchy":{"lvl1":"AGN Zoo: Comparison of AGN selected with different metrics","lvl3":"References","lvl2":"About this notebook"},"content":"minisom\n\numap","type":"content","url":"/ml-agnzoo#references","position":33},{"hierarchy":{"lvl1":"Light Curve Classifier"},"type":"lvl1","url":"/light-curve-classifier","position":0},{"hierarchy":{"lvl1":"Light Curve Classifier"},"content":"","type":"content","url":"/light-curve-classifier","position":1},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"Learning Goals"},"type":"lvl2","url":"/light-curve-classifier#learning-goals","position":2},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will be able to:\n\nprepare data for ML algorithms by cleaning and filtering the dataset\n\nwork with Pandas dataframes as a way of storing and manipulating time domain datasets\n\nuse \n\nsktime algorithms to train a classifier and calculate metrics of accuracy\n\nuse the trained classifier to predict labels on an unlabelled dataset","type":"content","url":"/light-curve-classifier#learning-goals","position":3},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"Introduction"},"type":"lvl2","url":"/light-curve-classifier#introduction","position":4},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"Introduction"},"content":"The science goal of this notebook is to find a classifier that can accurately discern changing look active galactic nuclei (CLAGN) from a broad sample of all Sloan Digital Sky Survey (SDSS) identified Quasars (QSOs) based solely on archival photometry in the form of multiwavelength light curves.\n\nCLAGN are astrophysically interesting objects because they appear to change state.  CLAGN are characterized by the appearance or disappearance of broad emission lines on timescales of order months.  Astronomers would like to understand the physical mechanism behind this apparent change of state.  However, only a few hundered CLAGN are known, and finding CLAGN is observationally expensive, traditionally requiring multiple epochs of spectroscopy.  Being able to identify CLAGN in existing, archival, large, photometric samples would allow us to identify a statisitcally significant sample from which we could better understand the underlying physics.\n\nThis notebook walks through an exercise in using multiwavelength photometry(no spectroscopy) to learn if we can identify CLAGN based on their light curves alone.  If we are able to find a classifier that can differentiate CLAGN from SDSS QSOs, we would then be able to run the entire sample of SDSS QSOs (~500,000) to find additional CLAGN candidates for follow-up verification.\n\nInput to this notebook is output of a previous demo notebook which generates multiwavelength light curves from archival data.  This notebook starts with light curves, does data prep, and runs the light curves through multiple ML classification algorithms.  There are many ML algorthms to choose from; We choose to use sktime algorithms for time domain classification beacuse it is a library of many algorithms specifically tailored to time series datasets.  It is based on the \n\nscikit-learn library so syntax is familiar to many users.\n\nThe challenges of this time-domain dataset for ML work are:\n\nMulti-variate = There are multiple bands of observations per target (13+)\n\nUnequal length = Each band has a light curve with different sampling than other bands\n\nMissing data = Not each object has all observations in all bands","type":"content","url":"/light-curve-classifier#introduction","position":5},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"Input","lvl2":"Introduction"},"type":"lvl3","url":"/light-curve-classifier#input","position":6},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"Input","lvl2":"Introduction"},"content":"Light curve parquet file of multiwavelength light curves from the light_curve_collector.md demo notebook in this same repo.  The format of the light curves is a Pandas multiindex data frame.\n\nWe choose to use a Pandas multiindex dataframe to store and work with the data because it fulfills these requirements:\n\nIt can handle the above challenges of a dataset = multi-variate, unqueal length with missing data.\n\nMultiple targets (multiple rows)\n\nPandas has some built in understanding of time units\n\nCan be scaled up to big data numbers of rows (altough we don’t push to out of memory structures in this use case)\n\nPandas is user friendly with a lot of existing functionality\n\nA useful reference for what sktime expects as input to its ML algorithms: \n\nexamples​/AA​_datatypes​_and​_datasets​.ipynb","type":"content","url":"/light-curve-classifier#input","position":7},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"Output","lvl2":"Introduction"},"type":"lvl3","url":"/light-curve-classifier#output","position":8},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"Output","lvl2":"Introduction"},"content":"Trained classifiers as well as estimates of their accuracy and plots of confusion matrices","type":"content","url":"/light-curve-classifier#output","position":9},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"Runtime","lvl2":"Introduction"},"type":"lvl3","url":"/light-curve-classifier#runtime","position":10},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"Runtime","lvl2":"Introduction"},"content":"As of 2024 August, this notebook takes ~170s to run to completion on Fornax using the ‘Astrophysics Default Image’ and the ‘Large’ server with 16GB RAM/ 4CPU.","type":"content","url":"/light-curve-classifier#runtime","position":11},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"Imports"},"type":"lvl2","url":"/light-curve-classifier#imports","position":12},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"Imports"},"content":"pandas to work with light curve data structure\n\nnumpy for numerical calculations\n\nmatplotlib for plotting\n\nsys for paths\n\nastropy to work with coordinates/units and data structures\n\ntqdm for showing progress meter\n\nsktime ML algorithms specifically for time-domain data\n\nsklearn general use ML algorthims with easy to use interface\n\nscipy for statistical analysis\n\njson for storing intermediate files\n\ngoogle_drive_downloader to access files stored in google drive\n\nThis cell will install them if needed:\n\n# Uncomment the next line to install dependencies if needed.\n# %pip install -r requirements_light_curve_classifier.txt\n\n\n\nimport sys\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport pandas as pd\nfrom astropy.table import Table\nimport googledrivedownloader as gdd\nfrom tqdm.auto import tqdm\nimport json\n\nfrom sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\n\nfrom sktime.classification.deep_learning import CNNClassifier\nfrom sktime.classification.dictionary_based import IndividualTDE\nfrom sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\nfrom sktime.classification.dummy import DummyClassifier\nfrom sktime.classification.ensemble import WeightedEnsembleClassifier\nfrom sktime.classification.feature_based import Catch22Classifier, RandomIntervalClassifier\nfrom sktime.classification.hybrid import HIVECOTEV2\nfrom sktime.classification.interval_based import CanonicalIntervalForest\nfrom sktime.classification.kernel_based import Arsenal, RocketClassifier\nfrom sktime.classification.shapelet_based import ShapeletTransformClassifier\nfrom sktime.registry import all_estimators, all_tags\nfrom sktime.datatypes import check_is_mtype\n\n# local code imports\nsys.path.append('code_src/')\nfrom classifier_functions import sigmaclip_lightcurves, remove_objects_without_band, \\\nremove_incomplete_data, missingdata_to_zeros, missingdata_drop_bands, \\\nuniform_length_spacing, reformat_df, local_normalization_max, mjd_to_datetime\n\n#improves memory usage and avoids problems that trigger warnings\npd.options.mode.copy_on_write = True\n\n\n\n","type":"content","url":"/light-curve-classifier#imports","position":13},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"1. Read in a dataset of archival light curves"},"type":"lvl2","url":"/light-curve-classifier#id-1-read-in-a-dataset-of-archival-light-curves","position":14},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"1. Read in a dataset of archival light curves"},"content":"We use here a sample of AGN including known CLAGN & random SDSS AGN\n\nIf you want to use your own sample, you can use the code from the \n\nlight curve collector notebook in this same repo to make the required pandas dataframe which you will need to run this notebook.\n\n# First we want to load light curves made in the light_curve_collector notebook\n\n# The data is on google drive, this will download it for you and read it into\n# a pandas dataframe\nsavename_df_lc = './data/small_CLAGN_SDSS_df_lc.parquet'\ngdd.download_file_from_google_drive(file_id='1DrB-CWdBBBYuO0WzNnMl5uQnnckL7MWH',\n                                    dest_path=savename_df_lc,\n                                    unzip=True)\n\n#load the data into a pandas dataframe\ndf_lc = pd.read_parquet(savename_df_lc)\n\n\n\n\n\n\n\n#get rid of indices set in the light curve code and reset them as needed\n#before sktime algorithms\ndf_lc = df_lc.reset_index()\n\n#what does the dataset look like at the start?\ndf_lc\n\n\n\n","type":"content","url":"/light-curve-classifier#id-1-read-in-a-dataset-of-archival-light-curves","position":15},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"2. Data Prep"},"type":"lvl2","url":"/light-curve-classifier#id-2-data-prep","position":16},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"2. Data Prep"},"content":"The majority of work in all ML projects is preparing and cleaning the data.  As most do, this dataset needs significant work before it can be fed into a ML algorithm.  Data preparation includes everything from removing statistical outliers to putting it in the correct data format for the algorithms.\n\n","type":"content","url":"/light-curve-classifier#id-2-data-prep","position":17},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.1 Remove bands with not enough data","lvl2":"2. Data Prep"},"type":"lvl3","url":"/light-curve-classifier#id-2-1-remove-bands-with-not-enough-data","position":18},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.1 Remove bands with not enough data","lvl2":"2. Data Prep"},"content":"For this use case of CLAGN classification, we don’t need to include some of the bands\nthat are known to be sparse.  Most ML algorithms cannot handle sparse data so one way to accomodate that\nis to remove the sparsest datasets.\n\n##what are the unique set of bands included in our light curves\ndf_lc.band.unique()\n\n#get rid of some of the bands that don't have enough data for all the sources\n#CLAGN are generall fainter targets, and therefore mostly not found\n#in datasets like TESS & K2\n\nbands_to_drop = [\"IceCube\", \"TESS\", \"FERMIGTRIG\", \"K2\"]\ndf_lc = df_lc[~df_lc[\"band\"].isin(bands_to_drop)]\n\n\n\n","type":"content","url":"/light-curve-classifier#id-2-1-remove-bands-with-not-enough-data","position":19},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.2 Combine Labels for a Simpler Classification","lvl2":"2. Data Prep"},"type":"lvl3","url":"/light-curve-classifier#id-2-2-combine-labels-for-a-simpler-classification","position":20},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.2 Combine Labels for a Simpler Classification","lvl2":"2. Data Prep"},"content":"All CLAGN start in the dataset as having labels based on their discovery paper.  Because we want one sample with all known CLAGN, we change those discovery names to be simply “CLAGN” for all CLAGN, regardless of origin.\n\ndf_lc['label'] = df_lc.label.str.replace('MacLeod 16', 'CLAGN')\ndf_lc['label'] = df_lc.label.str.replace('LaMassa 15', 'CLAGN')\ndf_lc['label'] = df_lc.label.str.replace('Yang 18', 'CLAGN')\ndf_lc['label'] = df_lc.label.str.replace('Lyu 22', 'CLAGN')\ndf_lc['label'] = df_lc.label.str.replace('Hon 22', 'CLAGN')\ndf_lc['label'] = df_lc.label.str.replace('Sheng 20', 'CLAGN')\ndf_lc['label'] = df_lc.label.str.replace('MacLeod 19', 'CLAGN')\ndf_lc['label'] = df_lc.label.str.replace('Green 22', 'CLAGN')\ndf_lc['label'] = df_lc.label.str.replace('Lopez-Navas 22', 'CLAGN')\n\n\n\n\n\nprint(df_lc.groupby([\"objectid\"]).ngroups, \"n objects before removing missing band data\")\n\n\n\n","type":"content","url":"/light-curve-classifier#id-2-2-combine-labels-for-a-simpler-classification","position":21},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.3 Data Visualization","lvl2":"2. Data Prep"},"type":"lvl3","url":"/light-curve-classifier#id-2-3-data-visualization","position":22},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.3 Data Visualization","lvl2":"2. Data Prep"},"content":"can we see any trends by examining plots of a subset of the data?\n\n#chhose your own adventure, the bands from which you can choose are:\ndf_lc.band.unique()\n\n\n\n\n\n#plot a single band for all objects\nband_of_interest = 'zr'\nband_lc = df_lc[df_lc['band'] == band_of_interest]\n#reset zero time to be start of that mission\nband_lc[\"time\"] = band_lc[\"time\"] - band_lc[\"time\"].min()\nband_lc.time.min()\n\nband_lc.set_index('time', inplace = True)  #helps with the plotting\n\n#drop some objects to try to clear up plot\nquerystring1 = 'objectid < 162'\nquerystring2 = 'objectid > 200'\nband_lc = band_lc.drop(band_lc.query(querystring1 ).index)\nband_lc = band_lc.drop(band_lc.query(querystring2 ).index)\n\n#quick normalization for plotting\n#we normalize for real after cleaning the data\n# make a new column with max_r_flux for each objectid\nband_lc['mean_band'] = band_lc.groupby('objectid', sort=False)[\"flux\"].transform('mean')\nband_lc['sigma_band'] = band_lc.groupby('objectid', sort=False)[\"flux\"].transform('std')\n\n#choose to normalize (flux - mean) / sigma\nband_lc['flux'] = (band_lc['flux'] - band_lc['mean_band']).div(band_lc['sigma_band'], axis=0)\n\n#want to have two different sets so I can color code\nclagn_df = band_lc[band_lc['label'] == 'CLAGN']\nsdss_df = band_lc[band_lc['label'] == 'SDSS']\nprint(clagn_df.groupby([\"objectid\"]).ngroups, \"n objects CLAGN \")\nprint(sdss_df.groupby([\"objectid\"]).ngroups, \"n objects SDSS \")\n\n#groupy objectid & plot flux vs. time\nfig, ax = plt.subplots(figsize=(10,6))\nlc_sdss = sdss_df.groupby(['objectid'])['flux'].plot(kind='line', ax=ax, color = 'gray', label = 'SDSS', linewidth = 0.3)\nlc_clagn = clagn_df.groupby(['objectid'])['flux'].plot(kind='line', ax=ax, color = 'orange', label = 'CLAGN', linewidth = 1)\n\n#add legend and labels/titles\nlegend_elements = [Line2D([0], [0], color='orange', lw=4, label='CLAGN'),\n                   Line2D([0], [0], color='gray', lw=4, label='SDSS')]\nax.legend(handles=legend_elements, loc='best')\n\nax.set_ylabel('Normalized Flux')\nax.set_xlabel('Time in days since start of mission')\nplt.title(f\"{band_of_interest} light curves\")\n\n#tailored to ZTF r band with lots of data\nax.set_ylim([-2, 4])\nax.set_xlim([1000, 1250])\n\n\n\n\n\n\n\n","type":"content","url":"/light-curve-classifier#id-2-3-data-visualization","position":23},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.4 Clean the dataset of unwanted data","lvl2":"2. Data Prep"},"type":"lvl3","url":"/light-curve-classifier#id-2-4-clean-the-dataset-of-unwanted-data","position":24},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.4 Clean the dataset of unwanted data","lvl2":"2. Data Prep"},"content":"“unwanted” includes:\n\nNaNs\n\nSKtime does not work with NaNs\n\nzero flux\n\nthere are a few flux measurements that come into our dataframe with zeros.  It is not clear what these are, and zero will be used to mean lack of observation in the rest of this notebook, so want to drop these rows at the outset.\n\noutliers in uncertainty\n\nThis is a tricky job because we want to keep astrophysical sources that are variable objects, but remove instrumental noise and CR (ground based).  The user will need to choose a sigma clipping threshold, and there is some plotting functionality available to help users make that decision\n\nobjects with no measurements in WISE W1 band\n\nBelow we want to normalize all light curves by W1, so we neeed to remove those objects without W1 fluxes because there will be nothing to normalize those light curves with.  We don’t want to have un-normalized data.\n\nobjects with incomplete data\n\nIncomplete is defined here as not enough flux measurements to make a good light curve.  Some bands in some objects have only a few datapoints. Three data points is not large enough for KNN interpolation, so we will consider any array with fewer than 4 photometry points to be incomplete data.  Another way of saying this is that we choose to remove those light curves with 3 or\nfewer data points.\n\n#drop rows which have Nans\ndf_lc.dropna(inplace = True, axis = 0)\n\n#drop rows with zero flux\nquerystring = 'flux < 0.000001'\ndf_lc = df_lc.drop(df_lc.query(querystring).index)\n\n#remove outliers\nsigmaclip_value = 10.0\ndf_lc = sigmaclip_lightcurves(df_lc, sigmaclip_value, include_plot = False)\nprint(df_lc.groupby([\"objectid\"]).ngroups, \"n objects after sigma clipping\")\n\n#remove incomplete data\nthreshold_too_few = 3\ndf_lc = remove_incomplete_data(df_lc, threshold_too_few, verbose = False)\n\n#remove objects without W1 fluxes\ndf_lc = remove_objects_without_band(df_lc, 'W1', verbose=True)\n\nprint(df_lc.groupby([\"objectid\"]).ngroups, \"n objects after cleaning the data\")\n\n\n\n\n\n","type":"content","url":"/light-curve-classifier#id-2-4-clean-the-dataset-of-unwanted-data","position":25},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.5 Missing Data","lvl2":"2. Data Prep"},"type":"lvl3","url":"/light-curve-classifier#id-2-5-missing-data","position":26},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.5 Missing Data","lvl2":"2. Data Prep"},"content":"Some objects do not have light curves in all bands.  Some ML algorithms can handle mising data, but not all, so we try to do something intentional and sensible to handle this missing data up front.\n\nThere are two options here:\n\nWe will add light curves with zero flux and err values for the missing data.  SKtime does not like NaNs, so we choose zeros.  This option has the benefit of including more bands and therefore more information, but the drawback of having some objects have bands with entire arrays of zeros.\n\nRemove bands which have less data from all objects so that there are no objects with missing data.  This has the benefit of less zeros, but the disadvantage of throwing away some information for the few objects which do have light curves in the bands which will be removed.\n\nFunctions are inlcuded for both options.\n\n#choose what to do with missing data...\n#df_lc = missingdata_to_zeros(df_lc)\n#or\nbands_to_keep = ['W1','W2','panstarrs g','panstarrs i', 'panstarrs r','panstarrs y','panstarrs z','zg','zr']\ndf_lc = missingdata_drop_bands(df_lc, bands_to_keep, verbose = True)\n\n\n\n","type":"content","url":"/light-curve-classifier#id-2-5-missing-data","position":27},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.6  Make all objects and bands have identical time arrays (uniform length and spacing)","lvl2":"2. Data Prep"},"type":"lvl3","url":"/light-curve-classifier#id-2-6-make-all-objects-and-bands-have-identical-time-arrays-uniform-length-and-spacing","position":28},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.6  Make all objects and bands have identical time arrays (uniform length and spacing)","lvl2":"2. Data Prep"},"content":"It is very hard to find time-domain ML algorithms which can work with non uniform length datasets. Therefore we make the light curves uniform by interpolating using KNN from scikit-learn which fills in the uniform length arrays with a final frequency chosen by the user.  We choose KNN as very straightforward method. This function also shows the framework in case the user wants to choose a different scikit-learn function to do the interpolation.  Another natural choice would be to use gaussian processes (GP) to do the interpolation, but this is not a good solution for our task because the flux values go to zero at times before and after the observations.  Because we include the entire time array from beginning of the first mission to end of the last mission, most individual bands require interpolation before and after their particular observations.  In other words, our light curves span the entire range from 2010 with the start of panstarrs and WISE to the most recent ZTF data release (at least 2023), even though most individual missions do not cover that full range of time.\n\nIt is important to choose the frequency over which the data is interpolated wisely.  Experimentation with treating this variable like a hyperparam and testing sktime algorithms shows slightly higher accuracy values for a suite of algorithms for a frequency of one interpolated observation per 60 days.\n\n#what does the dataframe look like at this point in the code?\ndf_lc\n\n\n\n\n\n#this cell takes 13seconds to run on the sample of 458 sources\n#change this to change the frequency of the time array\nfinal_freq_interpol = 60  #this is the timescale of interpolation in units of days\n\n#make all light curves have the same time array\ndf_interpol = uniform_length_spacing(df_lc, final_freq_interpol, include_plot = True )\n\n# df_lc_interpol has one row per dict in lc_interpol. time and flux columns store arrays.\n# \"explode\" the dataframe to get one row per light curve point. time and flux columns will now store floats.\ndf_lc = df_interpol.explode([\"time\", \"flux\",\"err\"], ignore_index=True)\ndf_lc = df_lc.astype({col: \"float\" for col in [\"time\", \"flux\", \"err\"]})\n\n\n\n","type":"content","url":"/light-curve-classifier#id-2-6-make-all-objects-and-bands-have-identical-time-arrays-uniform-length-and-spacing","position":29},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.7  Restructure dataframe","lvl2":"2. Data Prep"},"type":"lvl3","url":"/light-curve-classifier#id-2-7-restructure-dataframe","position":30},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.7  Restructure dataframe","lvl2":"2. Data Prep"},"content":"Make columns have band names in them and then remove band from the index\n\npivot the dataframe so that SKTIME understands its format\n\nthis will put it in the format expected by sktime\n\n#reformat the data to have columns be the different flux bands\ndf_lc = reformat_df(df_lc)\n\n\n\n\n\n#look at a single object to see what this array looks like\nob_of_interest = 12\nsingleob = df_lc[df_lc['objectid'] == ob_of_interest]\nsingleob\n\n\n\n","type":"content","url":"/light-curve-classifier#id-2-7-restructure-dataframe","position":31},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.8 Normalize","lvl2":"2. Data Prep"},"type":"lvl3","url":"/light-curve-classifier#id-2-8-normalize","position":32},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.8 Normalize","lvl2":"2. Data Prep"},"content":"Normalizing is required so that the CLAGN and it’s comparison SDSS sample don’t have different flux levels.  ML algorithms will easily choose to classify based on overall flux levels, so we want to prevent that by normalizing the fluxes. Normalization with a multiband dataset requires extra thought.  The idea here is that we normalize across each object.  We want the algorithms to know, for example, that within one object W1 will be brighter than ZTF bands but from one object to the next, it will not know that one is brighter than the other.\n\nWe do the normalization at this point in the code, rather than before interpolating over time, so that the final light curves are normalized since that is the chunk of information which goes into the ML algorithms.\n\nWe chose to normalize by the maximum flux in one band, and not median or mean because there are some objects where the median flux = 0.0 if we did a replacement by zeros for missing data.\n\n#normalize by W1 band\ndf_lc = local_normalization_max(df_lc, norm_column = \"flux_W1\")\n\n\n\n","type":"content","url":"/light-curve-classifier#id-2-8-normalize","position":33},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.9 Cleanup","lvl2":"2. Data Prep"},"type":"lvl3","url":"/light-curve-classifier#id-2-9-cleanup","position":34},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"2.9 Cleanup","lvl2":"2. Data Prep"},"content":"Make \n\ndatetime column\n\nSKTime wants a datetime column\n\nSave dataframe\n\n# need to make a datetime column\ndf_lc['datetime'] = mjd_to_datetime(df_lc)\n\n\n\n\n\n#save this dataframe to use for the ML below so we don't have to make it every time\nparquet_savename = 'output/df_lc_ML.parquet'\n#df_lc.to_parquet(parquet_savename)\n#print(\"file saved!\")\n\n\n\n","type":"content","url":"/light-curve-classifier#id-2-9-cleanup","position":35},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"3. Prep for ML algorithms"},"type":"lvl2","url":"/light-curve-classifier#id-3-prep-for-ml-algorithms","position":36},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"3. Prep for ML algorithms"},"content":"\n\n# could load a previously saved file in order to plot\n#parquet_loadname = 'output/df_lc_ML.parquet'\n#df_lc = MultiIndexDFObject()\n#df_lc.data = pd.read_parquet(parquet_loadname)\n#print(\"file loaded!\")\n\n\n\n\n\n#try dropping the uncertainty columns as variables for sktime\ndf_lc.drop(columns = ['err_panstarrs_g',\t'err_panstarrs_i',\t'err_panstarrs_r',\t'err_panstarrs_y',\n                      'err_panstarrs_z',\t'err_W1',\t'err_W2',\t'err_zg',\t'err_zr'], inplace = True)\n\n#drop also the time column because time shouldn't be a feature\ndf_lc.drop(columns = ['time'],inplace = True)\n\n\n\n","type":"content","url":"/light-curve-classifier#id-3-prep-for-ml-algorithms","position":37},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"3.1 Train test split","lvl2":"3. Prep for ML algorithms"},"type":"lvl3","url":"/light-curve-classifier#id-3-1-train-test-split","position":38},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"3.1 Train test split","lvl2":"3. Prep for ML algorithms"},"content":"We use sklearn’s train test split to randomly split the data into training and testing datasets.  Because thre are uneven numbers of each type (many more SDSS than CLAGN), we want to make sure to stratify evenly by type.\n\n#divide the dataframe into features and labels for ML algorithms\nlabels = df_lc[[\"objectid\", \"label\"]].drop_duplicates().set_index(\"objectid\").label\ndf_lc = df_lc.drop(columns=[\"label\"]).set_index([\"objectid\", \"datetime\"])\n\n\n\n\n\ntest_size = 0.25\n\n#want a stratified split based on label\ntrain_ix, test_ix = train_test_split(df_lc.index.levels[0], stratify = labels, shuffle = True, random_state = 43, test_size = test_size)\n\n#X is defined to be the features\n#y is defined to be the labels\nX_train = df_lc.loc[train_ix]\ny_train = labels.loc[train_ix]\nX_test = df_lc.loc[test_ix]\ny_test = labels.loc[test_ix]\n\n#plot to show how many of each type of object in the test dataset\nplt.figure(figsize=(6,4))\nplt.title(\"Objects in the Test dataset\")\nh = plt.hist(y_test, histtype='stepfilled', orientation='horizontal')\n\n\n\n","type":"content","url":"/light-curve-classifier#id-3-1-train-test-split","position":39},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"4. Run sktime algorithms on the light curves"},"type":"lvl2","url":"/light-curve-classifier#id-4-run-sktime-algorithms-on-the-light-curves","position":40},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"4. Run sktime algorithms on the light curves"},"content":"We choose to use \n\nsktime algorithms beacuse it is a library of many algorithms specifically tailored to time series datasets.  It is based on the sklearn library so syntax is familiar to many users.\n\nTypes of classifiers are listed \n\nhere.\n\nThis notebook will first show you an example of a single algorithm classifier. Then it will show how to write a for loop over a bunch of classifiers while outputting some metrics of accuracy.\n\n","type":"content","url":"/light-curve-classifier#id-4-run-sktime-algorithms-on-the-light-curves","position":41},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"4.1 Check that the data types are ok for sktime","lvl2":"4. Run sktime algorithms on the light curves"},"type":"lvl3","url":"/light-curve-classifier#id-4-1-check-that-the-data-types-are-ok-for-sktime","position":42},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"4.1 Check that the data types are ok for sktime","lvl2":"4. Run sktime algorithms on the light curves"},"content":"This test needs to pass in order for sktime to run\n\n#ask sktime if it likes the data type of X_train\n#if you change any of the functions or cells above this one, it is a good idea to\n# look at the below output to make sure you haven't introduced any NaNs or unequal length series\ncheck_is_mtype(X_train, mtype=\"pd-multiindex\", scitype=\"Panel\", return_metadata=True)\n\n\n\n\n\n#show the list of all possible classifiers that work with multivariate data\n#all_tags(estimator_types = 'classifier')\n#classifiers = all_estimators(\"classifier\", filter_tags={'capability:multivariate':True})\n#classifiers\n\n\n\n","type":"content","url":"/light-curve-classifier#id-4-1-check-that-the-data-types-are-ok-for-sktime","position":43},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"4.2 A single Classifier","lvl2":"4. Run sktime algorithms on the light curves"},"type":"lvl3","url":"/light-curve-classifier#id-4-2-a-single-classifier","position":44},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"4.2 A single Classifier","lvl2":"4. Run sktime algorithms on the light curves"},"content":"\n\n#this cell takes 35s to run on a sample of 267 light curves\n\n#setup the classifier\n#n_jobs is the number of jobs to run in parallel. some environments have trouble with this.\n#if you encounter an error such as 'BrokenProcessPool' while training or predicting, you may\n#want to either set n_jobs = 1 or use a different compute environment.\nclf = Arsenal(time_limit_in_minutes=1, n_jobs = -1)  # '-1' n_jobs means use all processors\n\n#fit the classifier on the training dataset\nclf.fit(X_train, y_train)\n\n#make predictions on the test dataset using the trained model\ny_pred = clf.predict(X_test)\n\nprint(f\"Accuracy of Random Interval Classifier: {accuracy_score(y_test, y_pred)}\\n\", flush=True)\n\n#plot a confusion matrix\ncm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)\ndisp.plot()\n\nplt.show()\n\n\n\n\n\n","type":"content","url":"/light-curve-classifier#id-4-2-a-single-classifier","position":45},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"4.3 Loop over a bunch of classifiers","lvl2":"4. Run sktime algorithms on the light curves"},"type":"lvl3","url":"/light-curve-classifier#id-4-3-loop-over-a-bunch-of-classifiers","position":46},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"4.3 Loop over a bunch of classifiers","lvl2":"4. Run sktime algorithms on the light curves"},"content":"Our method is to do a cursory check of a bunch of classifiers and then later drill down deeper on anything with good initial results.  We choose to run a loop over ~10 classifiers that seem promising and check the accuracy scores for each one.  Any classifier with a promising accuracy score could then be followed up with detailed hyperparameter tuning, or potentially with considering other classifiers in that same type.#This cell is currently not being run because it takes a long time\n\n#which classifiers are we interestd in\n#roughly one from each type of classifier\n\nnames = [\"Arsenal\",                     #kernel based\n        \"RocektClassifier\",             #kernel based\n        \"CanonicalIntervalForest\",      #interval based\n        \"HIVECOTEV2\",                   #hybrid\n#        \"CNNClassifier\",               #Deep Learning  - **requires tensorflow which is giving import errors\n#        \"WeightedEnsembleClassifier\",   #Ensemble - **maybe use in the future if we find good options\n        \"IndividualTDE\",               #Dictionary-based\n        \"KNeighborsTimeSeriesClassifier\", #Distance Based\n        \"RandomIntervalClassifier\",     #Feature based\n        \"Catch22Classifier\",            #Feature based\n        \"ShapeletTransformClassifier\"   #Shapelet based\n        \"DummyClassifier\"]             #Dummy - ignores input\n\n#for those with an impossible time limit, how long to let them run for before cutting off\nnmins = 3\n\n#these could certainly be more tailored\nclassifier_call = [Arsenal(time_limit_in_minutes=nmins, n_jobs = -1),\n                  RocketClassifier(num_kernels=2000),\n                  CanonicalIntervalForest(n_jobs = -1),\n                  HIVECOTEV2(time_limit_in_minutes=nmins, n_jobs = -1),\n#                  CNNClassifier(),\n#                  WeightedEnsembleClassifier(),\n                  IndividualTDE(n_jobs=-1),\n                  KNeighborsTimeSeriesClassifier(n_jobs = -1),\n                  RandomIntervalClassifier(n_intervals = 20, n_jobs = -1, random_state = 43),\n                  Catch22Classifier(outlier_norm = True, n_jobs = -1, random_state = 43),\n                  ShapeletTransformClassifier(time_limit_in_minutes=nmins,n_jobs = -1),\n                  DummyClassifier()]\n\n#setup to store the accuracy scores\naccscore_dict = {}\n\n# iterate over classifiers\nfor name, clf in tqdm(zip(names, classifier_call)):\n    #fit the classifier\n    clf.fit(X_train, y_train)\n\n    #make predictions on the test dataset\n    y_pred = clf.predict(X_test)\n\n    #calculate and track accuracy score\n    accscore = accuracy_score(y_test, y_pred)\n    print(f\"Accuracy of {name} classifier: {accscore}\\n\", flush=True)\n    accscore_dict[name] = accscore\n\n    #plot confusion matrix\n    cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)\n    disp.plot()\n    plt.show()\n\n#show the summary of the algorithms used and their accuracy score\n#accscore_dict\n\n\n\n\n\n#save statistics from these runs\n\n# Serialize data into file:\n#json.dump( accscore_dict, open( \"output/accscore.json\", 'w' ) )\n#json.dump( completeness_dict, open( \"output/completeness.json\", 'w' ) )\n#json.dump( homogeneity_dict, open( \"output/homogeneity.json\", 'w' ) )\n\n# Read data from file:\n#accscore_dict = json.load( open( \"output/accscore.json\") )\n\n\n\n","type":"content","url":"/light-curve-classifier#id-4-3-loop-over-a-bunch-of-classifiers","position":47},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"5. Create a candidate list"},"type":"lvl2","url":"/light-curve-classifier#id-5-create-a-candidate-list","position":48},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"5. Create a candidate list"},"content":"Lets assume we now have a classifier which can accurately differentiate CLAGN from SDSS QSOs based on their archival light curves.  Next, we would like to use that classifier on our favorite unlabeled sample to identify CLAGN candidates.  To do this, we need to:\n\nread in a dataframe of our new sample\n\nget that dataset in the same format as what was fed into the classifiers\n\nuse your trained classifier to predict labels for the new sample\n\nretrace those objectids to an ra & dec\n\nwrite an observing proposal (ok, you have to do that one yourself)\n\n#read in a dataframe of our new sample:\n# we are going to cheat here and use the same file as we used for input to the above, but you should\n# replace this with your favorite sample run through the light_curve_collector in this same repo.\npath_to_sample = './data/small_CLAGN_SDSS_df_lc.parquet'\nmy_sample = pd.read_parquet(path_to_sample)\n\n\n\n\n\n#get dataset in same format as what was run through sktime\n#This is not exactly the same as re-running the whole notebook on a different sample,\n#but it is pretty close.  We don't need to do all of the same cleaning because some of that\n#was to appease sktime in training the algorithms.\n\n\n#get rid of indices set in the light curve code and reset them as needed before sktime algorithms\nmy_sample = my_sample.reset_index()\n\n# get rid of some of the bands that don't have enough data for all the sources\n#CLAGN are generall fainter targets, and therefore mostly not found in datasets like TESS & K2\n#make sure your sample has the same bands as were run to train the classifier\nmy_sample = my_sample[~my_sample[\"band\"].isin(bands_to_drop)]\n\n#drop rows which have Nans\nmy_sample.dropna(inplace = True, axis = 0)\n\n#remove outliers\n#make sure your sample has the same sigmaclip_value as was run to train the classifier\nmy_sample = sigmaclip_lightcurves(my_sample, sigmaclip_value, include_plot = False, verbose= False)\n\n#remove objects without W1 fluxes\nmy_sample = remove_objects_without_band(my_sample, 'W1', verbose=False)\n\n#remove incomplete data\n#make sure your sample has the same threshold_too_few as were run to train the classifier\nmy_sample = remove_incomplete_data(my_sample, threshold_too_few, verbose = False)\n\n#drop missing bands\nmy_sample = missingdata_drop_bands(my_sample, bands_to_keep, verbose = False)\n\n#make arrays have uniform length and spacing\n#make sure your sample has the same final_feq_interpol as was run to train the classifier\ndf_interpol = uniform_length_spacing(my_sample, final_freq_interpol, include_plot = False )\nmy_sample = df_interpol.explode([\"time\", \"flux\",\"err\"], ignore_index=True)\nmy_sample = my_sample.astype({col: \"float\" for col in [\"time\", \"flux\", \"err\"]})\n\n#reformat the data to have columns be the different flux bands\nmy_sample = reformat_df(my_sample)\n\n#normalize\nmy_sample = local_normalization_max(my_sample, norm_column = \"flux_W1\")\n\n#make datetime column\nmy_sample['datetime'] = mjd_to_datetime(my_sample)\n\n#set index expected by sktime\nmy_sample = my_sample.set_index([\"objectid\", \"label\", \"datetime\"])\n\n#drop the uncertainty and time columns\nmy_sample.drop(columns = ['err_panstarrs_g',\t'err_panstarrs_i',\t'err_panstarrs_r',\t'err_panstarrs_y',\n                          'err_panstarrs_z',\t'err_W1',\t'err_W2',\t'err_zg',\t'err_zr','time'], inplace = True)\n\n #make X\nX_mysample  = my_sample.droplevel('label')\n\n\n\n\n\n#what does this look like to make sure we are on track\nX_mysample\n\n\n\n\n\n#use the trained sktime algorithm to make predictions on the test dataset\ny_mysample = clf.predict(X_mysample)\n\n\n\n\n\n#access the sample_table made in the light_curve_collector notebook\n#has information about the sample including ra & dec\nsavename_sample = './data/small_CLAGN_SDSS_sample.ecsv'\ngdd.download_file_from_google_drive(file_id='1pSEKVP4LbrdWQK9ws3CaI90m3Z_2fazL',\n                                    dest_path=savename_sample,\n                                    unzip=True)\nsample_table = Table.read(savename_sample, format='ascii.ecsv')\n\n\n\n\n\n\n\n\n\n#associate these predicted CLAGN with RA & Dec\n\n#need to first associate objectid with each of y_mysample\n#make a new df with a column = objectid which\n#includes all the unique objectids.\ntest = X_mysample.reset_index()\nmysample_CLAGN = pd.DataFrame(test.objectid.unique(), columns = ['objectid'])\nmysample_CLAGN[\"predicted_label\"] = pd.Series(y_mysample)\n\n#if I am only interested in the CLAGN, could drop anything with label = SDSS\nquerystring = 'predicted_label == \"SDSS\"'\nmysample_CLAGN = mysample_CLAGN.drop(mysample_CLAGN.query(querystring ).index)\n\n#then will need to merge candidate_CLAGN with sample_table along objectid\nsample_table_df = sample_table.to_pandas()\ncandidate_CLAGN = pd.merge(mysample_CLAGN, sample_table_df, on = \"objectid\", how = \"inner\")\n\n\n\n\n\n#show the CLAGN candidates ra & dec\ncandidate_CLAGN\n\n\n\n","type":"content","url":"/light-curve-classifier#id-5-create-a-candidate-list","position":49},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"Conclusions"},"type":"lvl2","url":"/light-curve-classifier#conclusions","position":50},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"Conclusions"},"content":"Depending on your comfort level with the accuracy of the classifier you have trained, you could now write an observing proposal to confirm those targets prediced to be CLAGN based on their multiwavelength light curves.\n\n","type":"content","url":"/light-curve-classifier#conclusions","position":51},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"About this notebook"},"type":"lvl2","url":"/light-curve-classifier#about-this-notebook","position":52},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl2":"About this notebook"},"content":"Authors: Jessica Krick, Shoubaneh Hemmati, Troy Raen, Brigitta Sipőcz, Andreas Faisst, Vandana Desai, David Shupe, and the Fornax team\n\nContact: For help with this notebook, please open a topic in the \n\nFornax Community Forum “Support” category.","type":"content","url":"/light-curve-classifier#about-this-notebook","position":53},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"Acknowledgements","lvl2":"About this notebook"},"type":"lvl3","url":"/light-curve-classifier#acknowledgements","position":54},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"Acknowledgements","lvl2":"About this notebook"},"content":"Stephanie La Massa","type":"content","url":"/light-curve-classifier#acknowledgements","position":55},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"References","lvl2":"About this notebook"},"type":"lvl3","url":"/light-curve-classifier#references","position":56},{"hierarchy":{"lvl1":"Light Curve Classifier","lvl3":"References","lvl2":"About this notebook"},"content":"“sktime: A Unified Interface for Machine Learning with Time Series”\nMarkus Löning, Tony Bagnall, Sajaysurya Ganesh, George Oastler, Jason Lines, ViktorKaz, …, Aadesh Deshmukh (2020). sktime/sktime. Zenodo. \n\nFranz Király et al. (2025)\n\n“Scikit-learn: Machine Learning in Python”, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n\n“pandas-dev/pandas: Pandas” The pandas development team, 2020. Zenodo. \n\nteam (2025)\n\nThis work made use of \n\nAstropy a community-developed core Python package and an ecosystem of tools and resources for astronomy (astropy:2013, astropy:2018, astropy:2022).","type":"content","url":"/light-curve-classifier#references","position":57},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data"},"type":"lvl1","url":"/light-curve-collector","position":0},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data"},"content":"","type":"content","url":"/light-curve-collector","position":1},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"Learning Goals"},"type":"lvl2","url":"/light-curve-collector#learning-goals","position":2},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will be able to:\n\nAutomatically load a catalog of target sources\n\nAutomatically & efficiently search NASA and non-NASA resources for the light curves of up to ~500 targets\n\nStore & manipulate light curves in a Pandas MultiIndex dataframe\n\nPlot all light curves on the same plot","type":"content","url":"/light-curve-collector#learning-goals","position":3},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"Introduction"},"type":"lvl2","url":"/light-curve-collector#introduction","position":4},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"Introduction"},"content":"A user has a sample of interesting targets for which they would like to see a plot of available archival light curves.  We start with a small set of changing look AGN from Yang et al., 2018, which are automatically downloaded. Changing look AGN are cases where the broad emission lines appear or disappear (and not just that the flux is variable).\n\nWe model light curve plots after van Velzen et al. 2021.  We search through a curated list of time-domain NASA holdings as well as non-NASA sources.  HEASARC catalogs used are Fermi and Beppo-Sax, IRSA catalogs used are ZTF and WISE, and MAST catalogs used are Pan-STARRS, TESS, Kepler, and K2.  Non-NASA sources are Gaia and IceCube. This list is generalized enough to include many types of targets to make this notebook interesting for many types of science.  All of these time-domain archives are searched in an automated and efficient fashion using astroquery, pyvo, pyarrow or APIs.\n\nLight curve data storage is a tricky problem.  Currently we are using a MultiIndex Pandas dataframe, as the best existing choice for right now.  One downside is that we need to manually track the units of flux and time instead of relying on an astropy storage scheme which would be able to do some of the units worrying for us (even astropy can’t do all magnitude to flux conversions).  Astropy does not currently have a good option for multi-band light curve storage.\n\nThis notebook walks through the individual steps required to collect the targets and their light curves and create figures. It also shows how to speed up the collection of light curves using python’s multiprocessing. This is expected to be sufficient for up to ~500 targets. For a larger number of targets, consider using the bash script demonstrated in the neighboring notebook \n\nscale_up.\n\nML work using these time-series light curves is in two neighboring notebooks: \n\nML_AGNzoo and \n\nlight​_curve​_classifier.","type":"content","url":"/light-curve-collector#introduction","position":5},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"Input","lvl2":"Introduction"},"type":"lvl3","url":"/light-curve-collector#input","position":6},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"Input","lvl2":"Introduction"},"content":"choose from a list of known changing look AGN from the literature\nOR -\n\ninput your own sample","type":"content","url":"/light-curve-collector#input","position":7},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"Output","lvl2":"Introduction"},"type":"lvl3","url":"/light-curve-collector#output","position":8},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"Output","lvl2":"Introduction"},"content":"an archival optical + IR + neutrino light curve","type":"content","url":"/light-curve-collector#output","position":9},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"Runtime","lvl2":"Introduction"},"type":"lvl3","url":"/light-curve-collector#runtime","position":10},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"Runtime","lvl2":"Introduction"},"content":"As of 2025 May, this notebook takes ~1000s (17 min.) to run to completion on Fornax using the ‘Astrophysics Default Image’ and the ‘Large’ server with 64GB RAM/ 16CPU.","type":"content","url":"/light-curve-collector#runtime","position":11},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"Imports"},"type":"lvl2","url":"/light-curve-collector#imports","position":12},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"Imports"},"content":"acstools to work with HST magnitude to flux conversion\n\nastropy to work with coordinates/units and data structures\n\nastroquery to interface with archives APIs\n\nhpgeom to locate coordinates in HEALPix space\n\nlightkurve to search TESS, Kepler, and K2 archives\n\nmatplotlib for plotting\n\nmultiprocessing to use the power of multiple CPUs to get work done faster\n\nnumpy for numerical processing\n\npandas with their [aws] extras for their data structure DataFrame and all the accompanying functions\n\npyarrow to work with Parquet files for WISE and ZTF\n\npyvo for accessing Virtual Observatory(VO) standard data\n\nrequests to get information from URLs\n\nscipy to do statistics\n\ntqdm to track progress on long running jobs\n\nurllib to handle archive searches with website interface\n\nThis cell will install them if needed:\n\n# Uncomment the next line to install dependencies if needed.\n# %pip install -r requirements_light_curve_collector.txt\n\n\n\nimport multiprocessing as mp\nimport sys\nimport time\n\nimport astropy.units as u\nimport pandas as pd\nfrom astropy.table import Table\n\n# local code imports\nsys.path.append('code_src/')\nfrom data_structures import MultiIndexDFObject\nfrom gaia_functions import gaia_get_lightcurves\nfrom hcv_functions import hcv_get_lightcurves\nfrom heasarc_functions import heasarc_get_lightcurves\nfrom icecube_functions import icecube_get_lightcurves\nfrom panstarrs_functions import panstarrs_get_lightcurves\nfrom plot_functions import create_figures\nfrom sample_selection import (clean_sample, get_green_sample, get_hon_sample, get_lamassa_sample, get_lopeznavas_sample,\n    get_lyu_sample, get_macleod16_sample, get_macleod19_sample, get_ruan_sample, get_sdss_sample, get_sheng_sample, get_yang_sample)\nfrom tess_kepler_functions import tess_kepler_get_lightcurves\nfrom wise_functions import wise_get_lightcurves\nfrom rubin_functions import rubin_get_lightcurves\nfrom ztf_functions import ztf_get_lightcurves\n\n# You may see a warning about \"tpfmodel submodule\".  This can safely be ignored.\n\n\n\n","type":"content","url":"/light-curve-collector#imports","position":13},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"1. Define the sample"},"type":"lvl2","url":"/light-curve-collector#id-1-define-the-sample","position":14},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"1. Define the sample"},"content":"We define here a “gold” sample of spectroscopically confirmed changing look AGN and quasars. This sample includes both objects which change from type 1 to type 2 and also the opposite.  Future studies may want to treat these as separate objects or separate QSOs from AGN.  Bibcodes for the samples used are listed next to their functions for reference.\n\nSignificant work went into the functions which grab the samples from the papers.  They use Astroquery, NED, SIMBAD, Vizier, and in a few cases grab the tables from the html versions of the paper.  There are trickeries involved in accessing coordinates from tables in the literature. Not every literature table is stored in its entirety in all of these resources, so be sure to check that your chosen method is actually getting the information that you see in the paper table.  Warning: You will get false results if using NED or SIMBAD on a table that has more rows than are printed in the journal.\n\n# Build up the sample\n# Initially set up lists to hold the coordinates and their reference paper name as a label\ncoords =[]\nlabels = []\n\n# Choose your own adventure:\n\n#get_lamassa_sample(coords, labels)  #2015ApJ...800..144L\n#get_macleod16_sample(coords, labels) #2016MNRAS.457..389M\n#get_ruan_sample(coords, labels) #2016ApJ...826..188R\n#get_macleod19_sample(coords, labels)  #2019ApJ...874....8M\n#get_sheng_sample(coords, labels)  #2020ApJ...889...46S\n#get_green_sample(coords, labels)  #2022ApJ...933..180G\n#get_lyu_sample(coords, labels)  #z32022ApJ...927..227L\n#get_lopeznavas_sample(coords, labels)  #2022MNRAS.513L..57L\n#get_hon_sample(coords, labels)  #2022MNRAS.511...54H\nget_yang_sample(coords, labels)   #2018ApJ...862..109Y\n\n# Get some \"normal\" QSOs\n# there are ~500K of these, so choose the number based on\n# a balance between speed of running the light curves and whatever\n# the ML algorithms would like to have\n\n# num_normal_QSO = 5000\n# zmin, zmax = 0, 10\n# randomize_z = False\n#get_sdss_sample(coords, labels, num=num_normal_QSO, zmin=zmin, zmax=zmax, randomize_z=randomize_z)\n\n# Remove duplicates, attach an objectid to the coords,\n# convert to astropy table to keep all relevant info together\nsample_table = clean_sample(coords, labels)\n\n#give this sample a name for use in saving files\nsample_name = \"yang_CLAGN\"\n\n\n\nsample_table\n\n\n\n","type":"content","url":"/light-curve-collector#id-1-define-the-sample","position":15},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"1.1 Build your own sample","lvl2":"1. Define the sample"},"type":"lvl3","url":"/light-curve-collector#id-1-1-build-your-own-sample","position":16},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"1.1 Build your own sample","lvl2":"1. Define the sample"},"content":"To build your own sample, you can follow the examples of functions above to grab coordinates from your favorite literature resource,\n\nor\n\nYou can use \n\nastropy’s read function to read in an input table\nto an \n\nastropy table\n\nIf you want to build your own sample_table, your table must contain one row per source and include exactly three required pieces of information: a coord column holding an astropy.coordinates.SkyCoord position for each object, a unique integer objectid, and a label column giving a short string describing that source’s origin (e.g., literature reference, sample name, or provenance tag).\n\n# Run this cell if you build your own sample to validate that it has the \n# required structure:\n# it must contain `coord` (SkyCoord), `objectid` (unique sequential ints),\n# and `label` (strings). Raises informative errors if anything is malformed.\n\nvalidate_sample_table(sample_table)\n\n\n\n","type":"content","url":"/light-curve-collector#id-1-1-build-your-own-sample","position":17},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"1.2 Write out your sample to disk","lvl2":"1. Define the sample"},"type":"lvl3","url":"/light-curve-collector#id-1-2-write-out-your-sample-to-disk","position":18},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"1.2 Write out your sample to disk","lvl2":"1. Define the sample"},"content":"At this point you may wish to write out your sample to disk and reuse that in future work sessions, instead of creating it from scratch again.\n\nFor the format of the save file, we would suggest to choose from various formats that fully support astropy objects(eg., SkyCoord).  One example that works is Enhanced Character-Separated Values or \n\n‘ecsv’\n\nsavename_sample = f\"output/{sample_name}_sample.ecsv\"\nsample_table.write(savename_sample, format='ascii.ecsv', overwrite = True)\n\n\n\n","type":"content","url":"/light-curve-collector#id-1-2-write-out-your-sample-to-disk","position":19},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"1.3 Load the sample table from disk","lvl2":"1. Define the sample"},"type":"lvl3","url":"/light-curve-collector#id-1-3-load-the-sample-table-from-disk","position":20},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"1.3 Load the sample table from disk","lvl2":"1. Define the sample"},"content":"Do only this step from this section when you have a previously generated sample table\n\n#sample_table = Table.read(savename_sample, format='ascii.ecsv')\n\n\n\n","type":"content","url":"/light-curve-collector#id-1-3-load-the-sample-table-from-disk","position":21},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"1.4 Initialize data structure to hold the light curves","lvl2":"1. Define the sample"},"type":"lvl3","url":"/light-curve-collector#id-1-4-initialize-data-structure-to-hold-the-light-curves","position":22},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"1.4 Initialize data structure to hold the light curves","lvl2":"1. Define the sample"},"content":"\n\n# We wrote our own class for a Pandas MultiIndex [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) for storing the light curves\n# This class helps simplify coding of common uses for the DataFrame.\ndf_lc = MultiIndexDFObject()\n\n\n\n","type":"content","url":"/light-curve-collector#id-1-4-initialize-data-structure-to-hold-the-light-curves","position":23},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"2. Find light curves for these targets in NASA catalogs"},"type":"lvl2","url":"/light-curve-collector#id-2-find-light-curves-for-these-targets-in-nasa-catalogs","position":24},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"2. Find light curves for these targets in NASA catalogs"},"content":"We search a curated list of time-domain catalogs from NASA astrophysics archives.  Because each archive is different, and in many cases each catalog is different, each function to access a catalog is necessarily specialized to the location and format of that particular catalog.\n\n","type":"content","url":"/light-curve-collector#id-2-find-light-curves-for-these-targets-in-nasa-catalogs","position":25},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"2.1 HEASARC: FERMI & Beppo SAX","lvl2":"2. Find light curves for these targets in NASA catalogs"},"type":"lvl3","url":"/light-curve-collector#id-2-1-heasarc-fermi-beppo-sax","position":26},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"2.1 HEASARC: FERMI & Beppo SAX","lvl2":"2. Find light curves for these targets in NASA catalogs"},"content":"The function to retrieve HEASARC data accesses the HEASARC archive using a pyvo search with a table upload.  This is the fastest way to access data from HEASARC catalogs at scale.\n\nWhile these aren’t strictly light curves, we would like to track if there are gamma rays detected in advance of any change in the CLAGN light curves. We store these gamma ray detections as single data points.  Because gamma ray detections typically have very large error radii, our current technique is to keep matches in the catalogs within some manually selected error radius, currently defaulting to 1 degree for Fermi and 3 degrees for Beppo SAX.  These values are chosen based on a histogram of all values for those catalogs.\n\nstart_serial = time.time()  #keep track of all serial archive calls to compare later with parallel archive call time\nheasarcstarttime = time.time()\n\n# What is the size of error_radius for the catalogs that we will accept for our cross-matching?\n# in degrees; chosen based on histogram of all values for these catalogs\nmax_fermi_error_radius = str(1.0)\nmax_sax_error_radius = str(3.0)\n\n# catalogs to query and their corresponding max error radii\nheasarc_catalogs = {\"FERMIGTRIG\": max_fermi_error_radius, \"SAXGRBMGRB\": max_sax_error_radius}\n\n# get heasarc light curves in the above curated list of catalogs\ndf_lc_HEASARC = heasarc_get_lightcurves(sample_table, catalog_error_radii=heasarc_catalogs)\n\n# add the resulting dataframe to all other archives\ndf_lc.append(df_lc_HEASARC)\n\nprint('heasarc search took:', time.time() - heasarcstarttime, 's')\n\n\n\n","type":"content","url":"/light-curve-collector#id-2-1-heasarc-fermi-beppo-sax","position":27},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"2.2 IRSA: WISE","lvl2":"2. Find light curves for these targets in NASA catalogs"},"type":"lvl3","url":"/light-curve-collector#id-2-2-irsa-wise","position":28},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"2.2 IRSA: WISE","lvl2":"2. Find light curves for these targets in NASA catalogs"},"content":"We use the unWISE light curves catalog (\n\nMeisner et al., 2023) which ties together all WISE & NEOWISE 2010 - 2020 epochs.  Specifically it combines all observations at a single epoch to achieve deeper mag limits than individual observations alone.\n\nThe function to retrieve WISE light curves accesses an IRSA generated version of the catalog in parquet format being stored in the AWS cloud \n\nOpen Data Repository\n\nWISEstarttime = time.time()\n\nbandlist = ['WISE_W1', 'WISE_W2']  #list of the WISE band names\nWISE_radius = 1.0  # arcsec\n# get WISE light curves\ndf_lc_WISE = wise_get_lightcurves(sample_table, radius=WISE_radius, bandlist=bandlist)\n\n# add the resulting dataframe to all other archives\ndf_lc.append(df_lc_WISE)\n\nprint('WISE search took:', time.time() - WISEstarttime, 's')\n\n\n\n","type":"content","url":"/light-curve-collector#id-2-2-irsa-wise","position":29},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"2.3 MAST: Pan-STARRS","lvl2":"2. Find light curves for these targets in NASA catalogs"},"type":"lvl3","url":"/light-curve-collector#id-2-3-mast-pan-starrs","position":30},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"2.3 MAST: Pan-STARRS","lvl2":"2. Find light curves for these targets in NASA catalogs"},"content":"The function to retrieve lightcurves from Pan-STARRS uses \n\nLSDB to access versions of the object and light curve catalogs that are stored in the cloud.  This function is efficient at large scale (sample sizes > ~1000).\n\nSome warnings are expected.\n\npanstarrsstarttime = time.time()\n\npanstarrs_search_radius = 1.0 # search radius = 1 arcsec\n# get panstarrs light curves\ndf_lc_panstarrs = panstarrs_get_lightcurves(sample_table, radius=panstarrs_search_radius)\n\n# add the resulting dataframe to all other archives\ndf_lc.append(df_lc_panstarrs)\n\nprint('Panstarrs search took:', time.time() - panstarrsstarttime, 's')\n\n# Warnings from the panstarrs query about both NESTED and margins are known issues\n\n\n\n","type":"content","url":"/light-curve-collector#id-2-3-mast-pan-starrs","position":31},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"2.4 MAST: TESS, Kepler and K2","lvl2":"2. Find light curves for these targets in NASA catalogs"},"type":"lvl3","url":"/light-curve-collector#id-2-4-mast-tess-kepler-and-k2","position":32},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"2.4 MAST: TESS, Kepler and K2","lvl2":"2. Find light curves for these targets in NASA catalogs"},"content":"The function to retrieve lightcurves from these three missions currently uses the open source package \n\nLightkurve.  This search is not efficient at scale and we expect it to be replaced in the future.\n\nlightkurvestarttime = time.time()\n\nTESS_search_radius = 1.0  #arcseconds\n# get TESS/Kepler/K2 light curves\ndf_lc_TESS = tess_kepler_get_lightcurves(sample_table, radius=TESS_search_radius)\n\n# add the resulting dataframe to all other archives\ndf_lc.append(df_lc_TESS)\n\nprint('TESS/Kepler/K2 search took:', time.time() - lightkurvestarttime, 's')\n\n# LightKurve will return an \"Error\" when it doesn't find a match for a target\n# These are not real errors and can be safely ignored.\n\n\n\n","type":"content","url":"/light-curve-collector#id-2-4-mast-tess-kepler-and-k2","position":33},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"2.5 MAST: Hubble Catalog of Variables","lvl2":"2. Find light curves for these targets in NASA catalogs"},"type":"lvl3","url":"/light-curve-collector#id-2-5-mast-hubble-catalog-of-variables","position":34},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"2.5 MAST: Hubble Catalog of Variables","lvl2":"2. Find light curves for these targets in NASA catalogs"},"content":"The function to retrieve lightcurves from \n\nHCV currently uses their API; based on this \n\nexample. This search is not efficient at scale and we expect it to be replaced in the future.\n\nHCVstarttime = time.time()\n\nHCV_radius = 1.0/3600.0 # radius = 1 arcsec\n# get HCV light curves\ndf_lc_HCV = hcv_get_lightcurves(sample_table, radius=HCV_radius)\n\n# add the resulting dataframe to all other archives\ndf_lc.append(df_lc_HCV)\n\nprint('HCV search took:', time.time() - HCVstarttime, 's')\n\n\n\n","type":"content","url":"/light-curve-collector#id-2-5-mast-hubble-catalog-of-variables","position":35},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"3. Find light curves for these targets in relevant, non-NASA catalogs"},"type":"lvl2","url":"/light-curve-collector#id-3-find-light-curves-for-these-targets-in-relevant-non-nasa-catalogs","position":36},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"3. Find light curves for these targets in relevant, non-NASA catalogs"},"content":"\n\n","type":"content","url":"/light-curve-collector#id-3-find-light-curves-for-these-targets-in-relevant-non-nasa-catalogs","position":37},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"3.1 IRSA: ZTF","lvl2":"3. Find light curves for these targets in relevant, non-NASA catalogs"},"type":"lvl3","url":"/light-curve-collector#id-3-1-irsa-ztf","position":38},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"3.1 IRSA: ZTF","lvl2":"3. Find light curves for these targets in relevant, non-NASA catalogs"},"content":"The function to retrieve ZTF light curves accesses a \n\nHATS parquet version of the ZTF catalog stored in the cloud using \n\nLSDB. This is the simplest way to access this dataset at scale.  The ZTF \n\nAPI is available for small sample searches.  One unique thing about this function is that it has parallelization built in to the function itself because lsdb uses dask under the hood.\n\nExpect to see many INFO messages from dask. These are normal.\nThe traceback for CommClosedErrors is also expected and is just a dask housekeeping issue, the function is still running to completion and returning light curves.\n\nZTFstarttime = time.time()\n\n# get ZTF lightcurves\nztf_search_radius = 1.0 #  arcsec  (Graham et al., 2024 use 1\" with good results)\ndf_lc_ZTF = ztf_get_lightcurves(sample_table, radius = ztf_search_radius)\n\n# add the resulting dataframe to all other archives\ndf_lc.append(df_lc_ZTF)\n\nprint('ZTF search took:', time.time() - ZTFstarttime, 's')\n\n\n\n","type":"content","url":"/light-curve-collector#id-3-1-irsa-ztf","position":39},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"3.2 Gaia","lvl2":"3. Find light curves for these targets in relevant, non-NASA catalogs"},"type":"lvl3","url":"/light-curve-collector#id-3-2-gaia","position":40},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"3.2 Gaia","lvl2":"3. Find light curves for these targets in relevant, non-NASA catalogs"},"content":"The function to retrieve Gaia light curves accesses the Gaia DR3 “source lite” catalog using an astroquery search with a table upload to do the join with the Gaia photometry. This is currently the fastest way to access light curves from Gaia at scale.\n\ngaiastarttime = time.time()\n\n# get Gaia light curves\ndf_lc_gaia = gaia_get_lightcurves(sample_table, search_radius=1/3600, verbose=0)\n\n# add the resulting dataframe to all other archives\ndf_lc.append(df_lc_gaia)\n\nprint('gaia search took:', time.time() - gaiastarttime, 's')\n\n\n\n","type":"content","url":"/light-curve-collector#id-3-2-gaia","position":41},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"3.3 IceCube neutrinos","lvl2":"3. Find light curves for these targets in relevant, non-NASA catalogs"},"type":"lvl3","url":"/light-curve-collector#id-3-3-icecube-neutrinos","position":42},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"3.3 IceCube neutrinos","lvl2":"3. Find light curves for these targets in relevant, non-NASA catalogs"},"content":"There are several \n\ncatalogs (basically one for each year of IceCube data from 2008 - 2018). The following code creates a large catalog by combining\nall the yearly catalogs.\nThe IceCube catalog contains Neutrino detections with associated energy and time and approximate direction (which is uncertain by half-degree scales....). Usually, for active events only one or two Neutrinos are detected, which makes matching quite different compared to “photons”. For our purpose, we will list the top 3 events in energy that are within a given distance to the target.\n\nThis time series (time vs. neutrino energy) information is similar to photometry. We choose to storing time and energy in our data structure, leaving error = 0. What is not stored in this format is the distance or angular uncertainty of the event direction.\n\nicecubestarttime = time.time()\n\n# get icecube data points\ndf_lc_icecube = icecube_get_lightcurves(sample_table, icecube_select_topN=3)\n\n# add the resulting dataframe to all other archives\ndf_lc.append(df_lc_icecube)\n\nprint('icecube search took:', time.time() - icecubestarttime, 's')\n\n\n\n","type":"content","url":"/light-curve-collector#id-3-3-icecube-neutrinos","position":43},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"3.4 Rubin","lvl2":"3. Find light curves for these targets in relevant, non-NASA catalogs"},"type":"lvl3","url":"/light-curve-collector#id-3-4-rubin","position":44},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"3.4 Rubin","lvl2":"3. Find light curves for these targets in relevant, non-NASA catalogs"},"content":"Before running this section, you need to have both 1) a login to the Rubin Science Platform (RSP) and 2) an authenticating token setup in your Fornax home directory.\n\nTo see if you have Rubin data rights, and if you do, to get that login setup, follow these \n\ninstructions\n\nAfter logging in to RSP, follow these \n\ninstructions to get a token and store it in your home directory.\n\nThis code will not work without the above information, so we have commented it out for now.  Uncomment when you are ready to proceed.\n\nOnce that setup is complete, this code access Rubin data from the Rubin Science Platform which is hosting their catalogs in Google Cloud.  Specifically, the code uses pyvo and adql to access a TAP server.\n\n#uncomment the next 5 lines if you have RSP login and authentication setup\n\n#rspstarttime = time.time()\n#rsp_search_radius = 0.001  # degrees\n\n#df_lc_rsp = rubin_get_lightcurves(sample_table, rsp_search_radius)\n#df_lc.append(df_lc_rsp) # add the resulting dataframe to all other archives\n\n#print('RSP search took:', time.time() - rspstarttime, 's')\n\n\n\n\n\n# benchmarking\nend_serial = time.time()\nprint('total time for serial archive calls is ', end_serial - start_serial, 's')\n\n\n\n","type":"content","url":"/light-curve-collector#id-3-4-rubin","position":45},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"4. Parallel processing the archive calls"},"type":"lvl2","url":"/light-curve-collector#id-4-parallel-processing-the-archive-calls","position":46},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"4. Parallel processing the archive calls"},"content":"This section shows how to increase the speed of the multi-archive search by running the calls in parallel using python’s multiprocessing library.\nThis can be a convenient and efficient method for small to medium sample sizes.\nOne drawback is that error messages tend to get lost in the background and never displayed for the user.\nRunning this on very large samples may fail because of the way the platform is setup to cull sessions which appear to be inactive.\nFor sample sizes >~500 and/or improved logging and monitoring options, consider using the bash script demonstrated in the related tutorial notebook \n\nscale_up.\n\n# number of workers to use in the parallel processing pool\n# this should equal the total number of archives called\nn_workers = 6\n\n# keyword arguments for the archive calls\nheasarc_kwargs = dict(catalog_error_radii={\"FERMIGTRIG\": \"1.0\", \"SAXGRBMGRB\": \"3.0\"})\nztf_search_radius = 1.0 #  arcsec\nwise_kwargs = dict(radius=1.0, bandlist=['W1', 'W2'])\npanstarrs_search_radius = 1.0 # arcsec\ntess_kepler_kwargs = dict(radius=1.0)\nhcv_kwargs = dict(radius=1.0/3600.0)\ngaia_kwargs = dict(search_radius=1/3600, verbose=0)\nicecube_kwargs = dict(icecube_select_topN=3)\nrsp_search_radius = 0.001\nrsp_kwargs = dict(search_radius=rsp_search_radius)\n\n\n\n\n\nparallel_starttime = time.time()\n\n# start a multiprocessing pool and run all the archive queries\nparallel_df_lc = MultiIndexDFObject()  # to collect the results\ncallback = parallel_df_lc.append  # will be called once on the result returned by each archive\nwith mp.Pool(processes=n_workers) as pool:\n\n    # start the processes that call the archives\n    pool.apply_async(heasarc_get_lightcurves, args=(sample_table,), kwds=heasarc_kwargs, callback=callback)\n    pool.apply_async(wise_get_lightcurves, args=(sample_table,), kwds=wise_kwargs, callback=callback)\n    pool.apply_async(tess_kepler_get_lightcurves, args=(sample_table,), kwds=tess_kepler_kwargs, callback=callback)\n    pool.apply_async(hcv_get_lightcurves, args=(sample_table,), kwds=hcv_kwargs, callback=callback)\n    pool.apply_async(gaia_get_lightcurves, args=(sample_table,), kwds=gaia_kwargs, callback=callback)\n    pool.apply_async(icecube_get_lightcurves, args=(sample_table,), kwds=icecube_kwargs, callback=callback)\n#    pool.apply_async(rubin_get_lightcurves, args=(sample_table,), kwds=rsp_kwargs, callback=callback)\n\n    pool.close()  # signal that no more jobs will be submitted to the pool\n    pool.join()  # wait for all jobs to complete, including the callback\n\n# run ZTF and panstarrs queries outside of multiprocessing since they\n# are using dask distributed under the hood,\n# which doesn't work with multiprocessing, and dask is already parallelized\n\ndf_lc_ZTF = ztf_get_lightcurves(sample_table, radius = ztf_search_radius)\nparallel_df_lc.append(df_lc_ZTF)# add the resulting dataframe to all other archives\n\ndf_lc_panstarrs = panstarrs_get_lightcurves(sample_table, radius=panstarrs_search_radius)\nparallel_df_lc.append(df_lc_panstarrs) # add the panstarrs dataframe to all other archives\n\nparallel_endtime = time.time()\n\n# LightKurve will return an \"Error\" when it doesn't find a match for a target\n# These are not real errors and can be safely ignored.\n\n# Warnings from the panstarrs query about both NESTED and margins are known issues\n\n\n\n\n\n# How long did parallel processing take?\n# and look at the results\nprint('parallel processing took', parallel_endtime - parallel_starttime, 's')\nparallel_df_lc.data\n\n\n\n\n\n# Save the data for future use with ML notebook\n#parquet_filename = f\"output/{sample_name}_df_lc.parquet\"\n#parallel_df_lc.data.to_parquet(parquet_savename)\n#print(\"file saved!\")\n\n\n\n\n\n# Could load a previously saved file in order to plot\n#parallel_df_lc = MultiIndexDFObject()\n#parallel_df_lc.data = pd.read_parquet(parquet_filename)\n#print(\"file loaded!\")\n\n\n\n","type":"content","url":"/light-curve-collector#id-4-parallel-processing-the-archive-calls","position":47},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"5. Make plots of luminosity as a function of time"},"type":"lvl2","url":"/light-curve-collector#id-5-make-plots-of-luminosity-as-a-function-of-time","position":48},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"5. Make plots of luminosity as a function of time"},"content":"These plots are modelled after \n\nvan Velzen et al., 2021. We show flux in mJy as a function of time for all available bands for each object. show_nbr_figures controls how many plots are actually generated and returned to the screen.  If you choose to save the plots with save_output, they will be put in the output directory and labelled by sample number.\n\nNote that in the following, we can either plot the results from df_lc (from the serial call) or parallel_df_lc (from the parallel call). By default (see next cell) the output of the parallel call is used.\n\n_ = create_figures(df_lc = parallel_df_lc, # either df_lc (serial call) or parallel_df_lc (parallel call)\n                   show_nbr_figures = 5,  # how many plots do you actually want to see?\n                   save_output = False ,  # should the resulting plots be saved?\n                  )\n\n\n\n","type":"content","url":"/light-curve-collector#id-5-make-plots-of-luminosity-as-a-function-of-time","position":49},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"About this notebook"},"type":"lvl2","url":"/light-curve-collector#about-this-notebook","position":50},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl2":"About this notebook"},"content":"Authors: Jessica Krick, Shoubaneh Hemmati, Andreas Faisst, Troy Raen, Brigitta Sipőcz, David Shupe, and the Fornax team\n\nContact: For help with this notebook, please open a topic in the \n\nFornax Community Forum “Support” category.","type":"content","url":"/light-curve-collector#about-this-notebook","position":51},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"Acknowledgements","lvl2":"About this notebook"},"type":"lvl3","url":"/light-curve-collector#acknowledgements","position":52},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"Acknowledgements","lvl2":"About this notebook"},"content":"Suvi Gezari, Antara Basu-zych, Stephanie LaMassa\n\nMAST, HEASARC, & IRSA Fornax teams","type":"content","url":"/light-curve-collector#acknowledgements","position":53},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"References","lvl2":"About this notebook"},"type":"lvl3","url":"/light-curve-collector#references","position":54},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves Using Archival Data","lvl3":"References","lvl2":"About this notebook"},"content":"This work made use of:\n\nAstroquery; Ginsburg et al., 2019, 2019AJ....157...98G\n\nAstropy; Astropy Collaboration 2022, Astropy Collaboration 2018, Astropy Collaboration 2013,    2022ApJ...935..167A, 2018AJ....156..123A, 2013A&A...558A..33A\n\nLightkurve; Lightkurve Collaboration 2018, 2018ascl.soft12013L\n\nacstools; \n\nP. L. Lim et al. (2022)\n\nunWISE light curves; Meisner et al., 2023, 2023AJ....165...36M","type":"content","url":"/light-curve-collector#references","position":55},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples"},"type":"lvl1","url":"/scale-up","position":0},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples"},"content":"","type":"content","url":"/scale-up","position":1},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"Learning Goals"},"type":"lvl2","url":"/scale-up#learning-goals","position":2},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will be able to:\n\nParallelize the code demonstrated in the \n\nlight​_curve​_collector notebook to get multi-wavelength light curves.\n\nLaunch a run using a large sample of objects, monitor the run’s progress automatically, and understand its resource usage (CPU and RAM).\n\nUnderstand some general challenges and requirements when scaling up code.\n\n","type":"content","url":"/scale-up#learning-goals","position":3},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"Introduction"},"type":"lvl2","url":"/scale-up#introduction","position":4},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"Introduction"},"content":"\n\nThis notebook shows how to collect multi-wavelength light curves for a large sample of target objects.\nThis is a scaling-up of the \n\nlight​_curve​_collector and assumes you are familiar with the content of that notebook.\n\nNotebook sections are:\n\nOverview: Describes functionality of the included bash script and python helper functions. Compares some parallel processing options. Introduces top and what be aware of.\n\nExample 1: Shows how to launch a large-scale run using the bash script, monitor its progress automatically, and diagnose a problem (out of RAM). This method is recommended for sample sizes greater than a few hundred.\n\nExample 2: Shows how to parallelize the example from the light_curve_collector notebook using the helper and python’s multiprocessing library.\n\nExample 3: Details the helper parameter options and how to use them in python and bash.\n\nAppendix: Contains background information including a discussion of the challenges, needs, and wants encountered when scaling up this code, and general advice for the user which can be applied to other use cases.\n\nAs written, this notebook is expected to require at least 1 CPU and 6G RAM.\n\nMany of the bash commands below are shown in non-executable cells because they are not intended to be run in this notebook.\nBash commands that are not executed below begin with the symbol $ , and those that are executed begin with !.\nBoth types can be called from the command-line -- open a new terminal and copy/paste the cell text without the beginning symbol. The bash script is not intended to be executed from within a notebook and may behave strangely if attempted.\nAlso be aware that the script path shown in the commands below assumes you are in the same directory as this notebook. Adjust it if needed.\n\n","type":"content","url":"/scale-up#introduction","position":5},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"Parallel processing methods: bash script vs. python’s multiprocessing","lvl2":"Introduction"},"type":"lvl3","url":"/scale-up#parallel-processing-methods-bash-script-vs-pythons-multiprocessing","position":6},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"Parallel processing methods: bash script vs. python’s multiprocessing","lvl2":"Introduction"},"content":"Bash script: Recommended for most runs with medium to large sample sizes (>~500). Allows ZTF to use additional parallelization internally, and so is often faster (ZTF often takes the longest and returns the most data for AGN-like samples). Writes stdout and stderr to log files, useful for monitoring jobs and resource usage. Can save top output to a file to help identify CPU and RAM usage/needs.\n\nPython’s multiprocessing library: Can be convenient for runs with small to medium sample sizes, up to ~500. Has drawbacks that may be significant including the inability to use ZTF’s internal parallelization and that it does not save the log output (stdout and stderr) to file. An advantage of the multiprocessing example in this notebook over the light_curve_collector is that it automatically saves the sample and light curve data to disk after loading them and can automatically skip those functions in subsequent calls and use the files instead.\n\n","type":"content","url":"/scale-up#parallel-processing-methods-bash-script-vs-pythons-multiprocessing","position":7},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"The python helper","lvl2":"Introduction"},"type":"lvl3","url":"/scale-up#the-python-helper","position":8},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"The python helper","lvl2":"Introduction"},"content":"The python “helper” is a set of wrapper functions around the same ‘code_src/’ functions used in the light_curve_collector notebook.\n\nThe wrappers facilitate parallelization and large-scale runs by automating tasks like saving the function outputs to files.\n\nThe helper does not actually implement parallelization and can only run one function per call.\n\nThe helper can be used in combination with any parallel processing method.\n\nThe helper can load top output from a file to pandas DataFrames and make some figures.\n\n","type":"content","url":"/scale-up#the-python-helper","position":9},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"The bash script","lvl2":"Introduction"},"type":"lvl3","url":"/scale-up#the-bash-script","position":10},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"The bash script","lvl2":"Introduction"},"content":"The bash script allows the user to collect light curves from a large scale sample with a single command and provides options to help manage and monitor the run.\nIn a nutshell, the script does the following when called using flags that “launch a run”:\n\ncalls the helper to gather the requested sample and then launches jobs for each archive query in separate, parallel processes.\n\nredirects stdout and stderr to log files.\n\ntells the user what the process PIDs are and where the log and data files are.\n\nexits, leaving the archive jobs running in the background.\n\nIn case all the archive calls need to be canceled after the script exits, the user can call the script again with a different flag to have it find and kill all the processes it launched.\n\nWhile the jobs are running, the user can call the script again with a different flag to have it save top output to a log file at a user-defined interval.\nThe helper can be used to load this file to pandas DataFrames and make some figures.\n\n","type":"content","url":"/scale-up#the-bash-script","position":11},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"Interpreting top","lvl2":"Introduction"},"type":"lvl3","url":"/scale-up#interpreting-top","position":12},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"Interpreting top","lvl2":"Introduction"},"content":"If you are unfamiliar with top, the answer to this \n\nStackExchange question contains a basic description of the fields in the output.\nAlso, beware that top can be configured to display values in different ways (e.g., as a percentage of either a single CPU or of all available CPUs).\nTo understand the local configuration, read the man page (run man top in a terminal), particularly the sections “SUMMARY Display” and “FIELDS / Columns Display”.\n\n","type":"content","url":"/scale-up#interpreting-top","position":13},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"Fornax Science Console","lvl2":"Introduction"},"type":"lvl3","url":"/scale-up#fornax-science-console","position":14},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"Fornax Science Console","lvl2":"Introduction"},"content":"There are a couple of things to be aware of when running on the Fornax Science Console.\n\nTake note of the amount of CPU and RAM available to the server type you choose when starting the session.\nIn particular, beware that top can show a larger amount of total RAM than is actually accessible to your server due to resource sharing between users.\n\nThe Science Console is primarily intended for interactive use and will cull sessions which appear to be inactive.\nIf you want a notebook or script to run for longer than about 30 minutes and you will not be interacting with the Console, running top during that time can help keep the session active.\n\n","type":"content","url":"/scale-up#fornax-science-console","position":15},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"Imports"},"type":"lvl2","url":"/scale-up#imports","position":16},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"Imports"},"content":"This cell will install the dependencies, if needed:\n\n# Uncomment the next line to install dependencies if needed.\n# %pip install -r requirements_scale_up.txt\n\n\n\n\n\nimport multiprocessing  # python parallelization method\nimport pandas as pd  # use a DataFrame to work with light-curve and other data\nimport sys  # add code directories to the path\n\nsys.path.append(\"code_src/\")\nimport helpers.scale_up  # python \"helper\" for parallelization and large scale runs\nimport helpers.top  # load `top` output to DataFrames and make figures\nfrom data_structures import MultiIndexDFObject  # load light curve data as a MultiIndexDFObject\nfrom plot_functions import create_figures  # make light curve figures\n\n\n\n","type":"content","url":"/scale-up#imports","position":17},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"1. Multi-wavelength light curves for 500,000 SDSS AGN"},"type":"lvl2","url":"/scale-up#id-1-multi-wavelength-light-curves-for-500-000-sdss-agn","position":18},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"1. Multi-wavelength light curves for 500,000 SDSS AGN"},"content":"\n\nThis example shows how to launch a large-scale collection of light curves using the bash script, monitor its performance, and diagnose a problem (out of RAM).\nThis run collects light curves for 500,000 SDSS objects and takes several hours to complete, but is not actually executed here.\nInstead, we show the bash commands and then look at logs that were generated by running the commands on 2024/03/01.\nIf executed, the run is expected to require at least 2 CPUs and 100G RAM.\n\n","type":"content","url":"/scale-up#id-1-multi-wavelength-light-curves-for-500-000-sdss-agn","position":19},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"1.1 Launch the run","lvl2":"1. Multi-wavelength light curves for 500,000 SDSS AGN"},"type":"lvl3","url":"/scale-up#id-1-1-launch-the-run","position":20},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"1.1 Launch the run","lvl2":"1. Multi-wavelength light curves for 500,000 SDSS AGN"},"content":"\n\n# This will launch a very large scale run.\n# See Example 3 for a detailed explanation of parameter options.\n# If you are running this for the first time, reduce the sample size 500000 -> 50.\n\n# run_id will be used in future calls to manage this run, and also determines the name of the output directory.\n# If you are executing this, change run_id to your own value to avoid overwriting the demo logs.\n$ run_id=\"demo-SDSS-500k\"\n\n# Execute the run.\n# This will get the sample and then call all archives specified by the '-a' flag.\n# Defaults will be used for all keyword arguments that are not specified.\n$ bash code_src/helpers/scale_up.sh \\\n    -r \"$run_id\" \\\n    -j '{\"get_samples\": {\"SDSS\": {\"num\": 500000}}, \"archives\": {\"ZTF\": {\"nworkers\": 8}}}' \\\n    -a \"Gaia HEASARC IceCube WISE ZTF\"\n\nThe script will run the ‘get sample’ job, then launch the archive query jobs in parallel and exit.\nArchive jobs will continue running in the background until they either complete or encounter an error.\nExample 2 shows how to load the data.\n\nCommand output from 2024/03/01 logs:\n\n!cat output/lightcurves-demo-SDSS-500k/logs/scale_up.sh.log\n# There is a warning that the SDSS 'specObjID' column was converted from an int to a string.\n# The column is immediately dropped. It is not used in the code.\n\n\n\n","type":"content","url":"/scale-up#id-1-1-launch-the-run","position":21},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"1.2 Cancel","lvl2":"1. Multi-wavelength light curves for 500,000 SDSS AGN"},"type":"lvl3","url":"/scale-up#id-1-2-cancel","position":22},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"1.2 Cancel","lvl2":"1. Multi-wavelength light curves for 500,000 SDSS AGN"},"content":"\n\nYou can cancel jobs at any time.\n\nIf the script is still running, press Control-C.\n\nIf the script has exited, there are two options.\n\nTo cancel an individual job, kill the job’s process using:\n\n$ pid=0000  # get the number from script output\n$ kill $pid\n\nTo cancel the entire run, use the -k (kill) flag:\n\n$ bash code_src/helpers/scale_up.sh -r \"$run_id\" -k\n\n","type":"content","url":"/scale-up#id-1-2-cancel","position":23},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"1.3 Restart","lvl2":"1. Multi-wavelength light curves for 500,000 SDSS AGN"},"type":"lvl3","url":"/scale-up#id-1-3-restart","position":24},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"1.3 Restart","lvl2":"1. Multi-wavelength light curves for 500,000 SDSS AGN"},"content":"\n\nIf you want to restart and skip step(s) that previously completed, run the first command again and add one or both “overwrite” flags set to false:\n\n# use the same run_id as before\n$ bash code_src/helpers/scale_up.sh \\\n    -r \"$run_id\" \\\n    -j '{\"get_samples\": {\"SDSS\": {\"num\": 500000}}, \"archives\": {\"ZTF\": {\"nworkers\": 8}}}' \\\n    -a \"Gaia HEASARC IceCube WISE ZTF\" \\\n    -d \"overwrite_existing_sample=false\" \\\n    -d \"overwrite_existing_lightcurves=false\"\n\n","type":"content","url":"/scale-up#id-1-3-restart","position":25},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"1.4 Monitor","lvl2":"1. Multi-wavelength light curves for 500,000 SDSS AGN"},"type":"lvl3","url":"/scale-up#id-1-4-monitor","position":26},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"1.4 Monitor","lvl2":"1. Multi-wavelength light curves for 500,000 SDSS AGN"},"content":"\n\nThere are at least three places to look for information about a run’s status.\n\nCheck the logs for job status or errors. The bash script will redirect stdout and stderr to log files and print out the paths for you.\n\nCheck for light curve (parquet) data. The script will print out the “parquet_dir”. ls this directory. You will see a subdirectory for each archive call that has completed successfully, assuming it retrieved data for the sample.\n\nWatch top. The script will print the job PIDs. The script can also monitor top for you and save the output to a log file.\n\n","type":"content","url":"/scale-up#id-1-4-monitor","position":27},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl4":"1.4.1 Logs","lvl3":"1.4 Monitor","lvl2":"1. Multi-wavelength light curves for 500,000 SDSS AGN"},"type":"lvl4","url":"/scale-up#id-1-4-1-logs","position":28},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl4":"1.4.1 Logs","lvl3":"1.4 Monitor","lvl2":"1. Multi-wavelength light curves for 500,000 SDSS AGN"},"content":"\n\n# Gaia log from 2024/03/01 (success)\n!cat output/lightcurves-demo-SDSS-500k/logs/gaia.log\n\n\n\n\n\n# ZTF log from 2024/03/01 (failure)\n!cat output/lightcurves-demo-SDSS-500k/logs/ztf.log\n\n\n\nThe logs above show that the ZTF job loaded the light curve data successfully (“100%”) but exited without writing the parquet file (no “Light curves saved” message like Gaia).\nThe data was lost.\nThere is also no indication of an error; the job just ends.\nWe can diagnose what happened by looking at the top output.\n\n","type":"content","url":"/scale-up#id-1-4-1-logs","position":29},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl4":"1.4.2 top","lvl3":"1.4 Monitor","lvl2":"1. Multi-wavelength light curves for 500,000 SDSS AGN"},"type":"lvl4","url":"/scale-up#id-1-4-2-top","position":30},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl4":"1.4.2 top","lvl3":"1.4 Monitor","lvl2":"1. Multi-wavelength light curves for 500,000 SDSS AGN"},"content":"While the jobs are running, you can monitor top and save the output to a log file by calling the script again with the -t flag.\nThis call must be separate from the call that launches the run.\nIf you want to capture the sample step as well as the archive queries, open a second terminal and call the script with -t right after launching the run in the first terminal.\n\n$ interval=10m  # choose your interval\n$ bash code_src/helpers/scale_up.sh -r \"run_id\" -t \"$interval\"\n\nThe script will continue running until after all of run_id’s jobs have completed.\nYou can cancel at anytime with Control-C and start it again with a new interval.\n\nOnce saved to file, the helper can parse the top output into pandas DataFrames and make some figures.\n\ntop output from 2024/03/01:\n\nrun_id = \"demo-SDSS-500k\"\nlogs_dir = helpers.scale_up.run(build=\"logs_dir\", run_id=run_id)\n\ntoplog = helpers.top.load_top_output(toptxt_dir=logs_dir, run_id=run_id)\n\n\n\n\n\n# System summary information from lines 1-5 of `top` output. One row per time step.\ntoplog.summary_df.sample(5)\n\n\n\n\n\n# Information about running processes from lines 6+ of `top` output. One row per process, per time step.\ntoplog.pids_df.sample(5)\n\n\n\n\n\nfig = toplog.plot_overview()\n\n\n\nIn the figure, panels 1 and 2 show CPU usage while 3 and 4 show memory usage.\nPanels 1 and 3 show system totals while panels 2 and 4 show usage by individual processes.\nSpecifically:\n\nPanel 1: System CPU load, averaged over the previous one minute. A value of “1” means that all compute tasks could have been completed by one CPU working 100% of the time (in reality, the tasks would have been split between the available CPUs). If this value exceeds the number of CPUs available, some tasks will wait in a queue before being executed.\n\nPanel 2: Percentage of time the given process used a CPU. This can be greater than 100% if a process uses multiple threads.\n\nPanel 3: Amount of unused memory available to the system. If this nears zero, the system will start killing processes so that the entire machine doesn’t go down.\n\nPanel 4: Percentage of total RAM used by the given process.\n\nThere are many interesting features in the figure that the reader may want to look at in more detail.\nFor example, other than ZTF’s memory spike at the end, we see that the full run collecting multi-wavelength light curves for the SDSS 500k sample could be completed with about 2 CPU and 60G RAM.\nWe also see that the IceCube call completes very quickly but requires a significant amount of memory.\nAnother observation is that the WISE call’s CPU activity seems to slow down when the ZTF workers are running, indicating that a different number of ZTF workers may be more efficient overall.\n\nWe want to learn why the ZTF job failed.\nLet’s zoom in on that time period:\n\nfig = toplog.plot_overview(between_time=(\"22:55\", \"23:10\"))\n\n\n\nIn the second panel above we see the ZTF worker processes (which load the light curve data) ending just after 23:00 and then the ZTF parent process continues by itself.\nAround 23:06 in the fourth panel, we see the ZTF job’s memory usage rise sharply to almost 100% and then drop immediately to zero when the job terminates.\nThis coincides with the total available memory dropping to near zero in the third panel.\nThis shows that the machine did not have enough memory for the ZTF call to successfully transform the light curve data collected by the workers into a MultiIndexDFObject and write it as a parquet file, so the machine killed the job.\n\nThe solution is to rerun the ZTF archive call on a machine with more RAM.\n\nTo learn exactly which step in ztf_get_lightcurves was causing this and how much memory it actually needed, additional print statements were inserted into the code similar to the following:\n\n# this was inserted just before \"exploding\" the dataframe in ztf_functions.py\n# helpers.scale_up._now() prints the current timestamp\nprint(f\"{helpers.scale_up._now()} | starting explode\", flush=True)\n# ztf_df = ztf_df.explode(...  # this is the next line in ztf_functions.py\n\n\n\nZTF was then rerun on a large machine and top output was saved.\nAfter the run, we manually compared timestamps between the ZTF log and top output and tagged relevant top timestamps with corresponding step names by appending the name to the ‘----’ delineator, like this for the “explode” step:\n\n!cat output/lightcurves-demo-SDSS-500k/logs/top.tag-ztf.txt | grep -A12 explode\n\n\n\nThe helper can recognize these tags and show them on a figure:\n\nztf_toplog = helpers.top.load_top_output(toptxt_file=\"top.tag-ztf.txt\", toptxt_dir=logs_dir, run_id=run_id)\n\nfig = ztf_toplog.plot_time_tags(summary_y=\"used_GiB\")\n# (This run starts by reading a previously cached parquet file containing the raw data returned by workers.)\n\n\n\nThis figure shows that almost 100G RAM is required for the ZTF job to succeed.\nIt further shows that the “explode” step requires the most memory, followed by creating the MultiIndexDFObject.\nFrom here, the user can choose an appropriately sized machine and/or consider whether ztf_get_lightcurves could be made to use less memory.\n\n","type":"content","url":"/scale-up#id-1-4-2-top","position":31},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"2. Parallelizing the light_curve_collector notebook"},"type":"lvl2","url":"/scale-up#id-2-parallelizing-the-light-curve-collector-notebook","position":32},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"2. Parallelizing the light_curve_collector notebook"},"content":"\n\nThis example shows how to parallelize the example from the light_curve_collector notebook using the helper and python’s multiprocessing.\nAn advantage of the method shown here is that it automatically saves the sample and light curve data to disk after loading them and can automatically skip those steps in subsequent calls and use the files instead.\nThis is a small sample (Yang, 30 objects).\nIf you want a sample larger than a few hundred, consider using the bash script instead.\n\nDefine the keyword arguments for the run:\n\nkwargs_dict = {\n    \"run_id\": \"demo-Yang-sample\",\n    # Paper names to gather the sample from.\n    \"get_samples\": [\"Yang\"],\n    # Keyword arguments for *_get_lightcurves archive calls.\n    \"archives\": {\n        \"Gaia\": {\"search_radius\": 1 / 3600, \"verbose\": 0},\n        \"HEASARC\": {\"catalog_error_radii\": {\"FERMIGTRIG\": 1.0, \"SAXGRBMGRB\": 3.0}},\n        \"IceCube\": {\"icecube_select_topN\": 3, \"max_search_radius\": 2.0},\n        \"WISE\": {\"radius\": 1.0, \"bandlist\": [\"W1\", \"W2\"]},\n        \"ZTF\": {\"match_radius\": 1 / 3600, \"nworkers\": None},\n    },\n}\n# See Example 3 for a detailed explanation of parameter options.\nkwargs_dict\n\n\n\nDecide which archives to query.\nThis is a separate list because the helper can only run one archive call at a time.\nWe will iterate over this list and launch each job separately.\n\n# archive_names = [\"PanSTARRS\", \"WISE\"]  # choose your own list\narchive_names = helpers.scale_up.ARCHIVE_NAMES[\"all\"]  # predefined list\narchive_names\n\n\n\nIn the next cell, we:\n\ncollect the sample and write it as a .ecsv file; then\n\nquery the archives in parallel using a multiprocessing.Pool and write the light curve data as .parquet files.\n\nNote: Since the next cell launches each archive call in a child process (through multiprocessing.Pool),\nthe archive calls themselves will not be allowed to launch child processes of their own.\nWe’ll just skip the affected archive calls here and encourage the user towards the bash script since this\nis one of the main reasons that it was developed -- it launches the archive jobs in parallel, but in a\ndifferent way, so that the calls are also allowed to parallelize their own internal code and thus run much faster.\n\n# Skip archive calls that use internal parallelization.\n_archive_names = [name for name in archive_names if name.lower() not in [\"panstarrs\", \"ztf\"]]\n\nsample_table = helpers.scale_up.run(build=\"sample\", **kwargs_dict)\n# sample_table is returned if you want to look at it but it is not used below\n\nwith multiprocessing.Pool(processes=len(_archive_names)) as pool:\n    # submit one job per archive\n    for archive in _archive_names:\n        pool.apply_async(helpers.scale_up.run, kwds={\"build\": \"lightcurves\", \"archive\": archive, **kwargs_dict})\n    pool.close()  # signal that no more jobs will be submitted to the pool\n    pool.join()  # wait for all jobs to complete\n\n# Note: The console output from different archive calls gets jumbled together below.\n# Worse, error messages tend to get lost in the background and never displayed.\n# If you have trouble, consider running an archive call individually without the Pool\n# or using the bash script instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe light curves are saved in the “parquet_dir” directory.\nEach archive call writes data to a parquet file in its own subdirectory.\nThese can be loaded together as a single dataset:\n\n# copy/paste the directory path from the output above, or ask the helper for it like this:\nparquet_dir = helpers.scale_up.run(build=\"parquet_dir\", **kwargs_dict)\ndf_lc = pd.read_parquet(parquet_dir)\n\ndf_lc.sample(10)\n\n\n\nNow we can make figures:\n\n_ = create_figures(df_lc=MultiIndexDFObject(data=df_lc), show_nbr_figures=1, save_output=False)\n\n\n\n\n\n","type":"content","url":"/scale-up#id-2-parallelizing-the-light-curve-collector-notebook","position":33},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"3. Keyword arguments and script flags"},"type":"lvl2","url":"/scale-up#id-3-keyword-arguments-and-script-flags","position":34},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"3. Keyword arguments and script flags"},"content":"\n\nThis example shows the python kwargs_dict and bash script flag options in more detail.","type":"content","url":"/scale-up#id-3-keyword-arguments-and-script-flags","position":35},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"3.1 Python kwargs_dict","lvl2":"3. Keyword arguments and script flags"},"type":"lvl3","url":"/scale-up#id-3-1-python-kwargs-dict","position":36},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"3.1 Python kwargs_dict","lvl2":"3. Keyword arguments and script flags"},"content":"kwargs_dict is a dictionary containing all keyword arguments for the run. It can contain:\n\nnames and keyword arguments for any of the get_*_sample functions.\n\nkeyword arguments for any of the *_get_lightcurves functions.\n\nother keyword arguments used directly by the helper.\nThese options and their defaults are shown below, further documented in the helper’s run function.\n\n# show kwargs_dict defaults\nhelpers.scale_up.DEFAULTS\n\n\n\n\n\n# show parameter documentation\nprint(helpers.scale_up.run.__doc__)\n\n\n\n","type":"content","url":"/scale-up#id-3-1-python-kwargs-dict","position":37},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"3.2 Bash script flags","lvl2":"3. Keyword arguments and script flags"},"type":"lvl3","url":"/scale-up#id-3-2-bash-script-flags","position":38},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"3.2 Bash script flags","lvl2":"3. Keyword arguments and script flags"},"content":"Use the -h (help) flag to view the script’s flag options:\n\n# show flag documentation\n!bash code_src/helpers/scale_up.sh -h\n\n\n\n","type":"content","url":"/scale-up#id-3-2-bash-script-flags","position":39},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"3.3 Using a yaml file","lvl2":"3. Keyword arguments and script flags"},"type":"lvl3","url":"/scale-up#id-3-3-using-a-yaml-file","position":40},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"3.3 Using a yaml file","lvl2":"3. Keyword arguments and script flags"},"content":"It can be convenient to save the parameters in a yaml file, especially when using the bash script or in cases where you want to store parameters for later reference or re-use.\n\nDefine an extended set of parameters and save it as a yaml file:\n\nyaml_run_id = \"demo-kwargs-yaml\"\n\nget_samples = {\n    \"green\": {},\n    \"ruan\": {},\n    \"papers_list\": {\n        \"paper_kwargs\": [\n            {\"paper_link\": \"2022ApJ...933...37W\", \"label\": \"Galex variable 22\"},\n            {\"paper_link\": \"2020ApJ...896...10B\", \"label\": \"Palomar variable 20\"},\n        ]\n    },\n    \"SDSS\": {\"num\": 10, \"zmin\": 0.5, \"zmax\": 2, \"randomize_z\": True},\n    \"ZTF_objectid\": {\"objectids\": [\"ZTF18aabtxvd\", \"ZTF18aahqkbt\", \"ZTF18abxftqm\", \"ZTF18acaqdaa\"]},\n}\n\narchives = {\n    \"Gaia\": {\"search_radius\": 2 / 3600},\n    \"HEASARC\": {\"catalog_error_radii\": {\"FERMIGTRIG\": 1.0, \"SAXGRBMGRB\": 3.0}},\n    \"IceCube\": {\"icecube_select_topN\": 4, \"max_search_radius\": 2.0},\n    \"WISE\": {\"radius\": 1.5, \"bandlist\": [\"W1\", \"W2\"]},\n    \"ZTF\": {\"nworkers\": 6, \"match_radius\": 2 / 3600},\n}\n\nyaml_kwargs_dict = {\n    \"get_samples\": get_samples,\n    \"consolidate_nearby_objects\": False,\n    \"archives\": archives,\n}\n\nhelpers.scale_up.write_kwargs_to_yaml(run_id=yaml_run_id, **yaml_kwargs_dict)\n\n\n\nThe path to the yaml file is printed in the output above.\nYou can alter the contents of the file as you like.\nTo use the file, set the kwarg use_yaml=True.\n\nPython example for the get-sample step:\n\nsample_table = helpers.scale_up.run(build=\"sample\", run_id=yaml_run_id, use_yaml=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBash example:\n\n$ yaml_run_id=demo-kwargs-yaml\n$ bash code_src/helpers/scale_up.sh -r \"$yaml_run_id\" -d \"use_yaml=true\" -a \"Gaia HEASARC IceCube PanSTARRS WISE ZTF\"\n\n","type":"content","url":"/scale-up#id-3-3-using-a-yaml-file","position":41},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"Appendix: What to expect"},"type":"lvl2","url":"/scale-up#appendix-what-to-expect","position":42},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"Appendix: What to expect"},"content":"\n\n","type":"content","url":"/scale-up#appendix-what-to-expect","position":43},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"Challenges of large-scale runs","lvl2":"Appendix: What to expect"},"type":"lvl3","url":"/scale-up#challenges-of-large-scale-runs","position":44},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"Challenges of large-scale runs","lvl2":"Appendix: What to expect"},"content":"\n\nScaling up to large sample sizes brings new challenges.\nEven if things go smoothly, each function call may need to use a lot more resources like CPU, RAM, and bandwidth, and may take much longer to complete.\nAlso, we’ll want to run some functions in parallel to save time overall, but that will mean they must compete with each other for resources.\n\nThese issues are complicated by the fact that different combinations of samples and archive calls can trigger different problems.\nInefficiencies in any part of the process -- our code, archive backends, etc. -- which may have been negligible at small scale can balloon into significant hurdles.\n\nProblems can manifest in different ways.\nFor example, progress may slow to a crawl, or it may run smoothly for several hours and then crash suddenly.\nIf the job is running in the background, print statements and error messages may get lost and never be displayed for the user if they are not redirected to a file.\n\n","type":"content","url":"/scale-up#challenges-of-large-scale-runs","position":45},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"Needs and wants for large-scale runs","lvl2":"Appendix: What to expect"},"type":"lvl3","url":"/scale-up#needs-and-wants-for-large-scale-runs","position":46},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"Needs and wants for large-scale runs","lvl2":"Appendix: What to expect"},"content":"\n\nThe main goal is to reduce the total time it takes to run the full code, so we want to look for opportunities to parallelize.\nWe can group the light_curve_collector code into two main steps: (1) gather the target object sample; then (2) generate light curves by querying the archives and standardizing the returned data.\nAll of the archive calls have to wait for the sample to be available before starting, but then they can run independently in parallel.\nThis is fortunate, since gathering the sample does not take long compared to the archive calls.\n\nIt is useful to be able to monitor the run’s resource usage and capture print statements, error messages, etc. to log files in order to understand if/when something goes wrong.\nEven with parallelization, gathering light curves for a large sample of objects is likely to take a few hours at least.\nSo we want to automate the monitoring tasks as much as possible.\n\nIf the run fails, we’d like to be able to restart it without having to redo steps that were previously successful.\nTo accomplish this, the inputs and outputs need to be less tightly coupled than they are in the light_curve_collector notebook.\nSpecifically, we want to save the sample and light curve data to file as soon as each piece is collected, and we want the archive functions to be able to get the sample_table input from file.\n\nThe python helper and bash script were specifically designed to fulfill many of these wants and needs.\n\n","type":"content","url":"/scale-up#needs-and-wants-for-large-scale-runs","position":47},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"Advice for the user","lvl2":"Appendix: What to expect"},"type":"lvl3","url":"/scale-up#advice-for-the-user","position":48},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl3":"Advice for the user","lvl2":"Appendix: What to expect"},"content":"\n\nGetting started:\n\nSkim this notebook to understand the process, available options, and potential sticking points.\n\nTry things with a small sample size first, then scale up to your desired full sample.\n\nDon’t be surprised if something goes wrong.\nEvery new combination of factors can present different challenges and reasons for the code to fail.\nThis includes the sample selection, which archives are called and what parameters are used, runtime environment, machine CPU and RAM capabilities, network bandwidth, etc.\nScaling up any code base comes with challenges, and some of these cannot be fully managed by pre-written code.\nYou may need to observe how the code performs, diagnose a problem, and adapt the input parameters, machine size, etc. in order to successfully execute.\n\nTo execute a run:\n\nDefine all of the get-sample and get-lightcurve parameters.\n\nLaunch the run by calling the bash script or some other multi-processing method.\nCapturing stdout, stderr and top output to log files is highly recommended.\n\nIf a get-lightcurve (get-sample) job exits without writing a .parquet (.ecsv) file, inspect the logs and top output to try to determine the reason it failed.\nIt could be anything from a missing python library (install it), to an archive encountering an internal error (wait a bit and try again), to the job getting killed prematurely because its needs exceeded the available RAM (try a machine with more RAM, a smaller sample size, or running fewer archive calls at a time), to many other things.\n\n","type":"content","url":"/scale-up#advice-for-the-user","position":49},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"About this notebook"},"type":"lvl2","url":"/scale-up#about-this-notebook","position":50},{"hierarchy":{"lvl1":"Make Multi-Wavelength Light Curves for Large Samples","lvl2":"About this notebook"},"content":"Authors: Troy Raen, Jessica Krick, Brigitta Sipőcz, Shoubaneh Hemmati, Andreas Faisst, David Shupe, and the Fornax team\n\nContact: For help with this notebook, please open a topic in the \n\nFornax Community Forum “Support” category.","type":"content","url":"/scale-up#about-this-notebook","position":51},{"hierarchy":{"lvl1":"Time Domain"},"type":"lvl1","url":"/time-domain","position":0},{"hierarchy":{"lvl1":"Time Domain"},"content":"In this set of Use Case Scenario we work towards creating multi-band light curves from multiple archival and publication resources at scale and classifying and analyzing them with machine learning tools. Tutorials included in this folder are:\n\nlight​_curve​_collector: This notebook automatically retrieves target positions from the literature and then queries multiple data archives for light curves of those targets.\n\nscale_up: This notebook builds on the code demonstrated in light_curve_collector and is recommended for >~500 targets. It is able to generate light curves for a large number of targets (500,000+) and provides additional monitoring options.\n\nlight​_curve​_classifier: This notebook takes output from light_curve_collector and trains a ML classifier to be able to differentiate amongst the targets based on their light curves.\n\nML_AGNzoo: This notebook takes output from the light_curve_collector and visualizes/compares different labelled targets on a reduced dimension grid.","type":"content","url":"/time-domain","position":1},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data"},"type":"lvl1","url":"/spectra-collector","position":0},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data"},"content":"","type":"content","url":"/spectra-collector","position":1},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl2":"Learning Goals"},"type":"lvl2","url":"/spectra-collector#learning-goals","position":2},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will be able to:\n\n• automatically load a catalog of sources\n\n• search NASA and non-NASA resources for fully reduced spectra and load them using specutils\n\n• store the spectra in a Pandas multiindex dataframe\n\n• plot all the spectra of a given source","type":"content","url":"/spectra-collector#learning-goals","position":3},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl2":"Introduction"},"type":"lvl2","url":"/spectra-collector#introduction","position":4},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl2":"Introduction"},"content":"","type":"content","url":"/spectra-collector#introduction","position":5},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"Motivation","lvl2":"Introduction"},"type":"lvl3","url":"/spectra-collector#motivation","position":6},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"Motivation","lvl2":"Introduction"},"content":"A user has a source (or a sample of sources) for which they want to obtain spectra covering ranges\nof wavelengths from the UV to the far-IR. The large amount of spectra available enables\nmulti-wavelength spectroscopic studies, which is crucial to understand the physics of stars,\ngalaxies, and AGN. However, gathering and analysing spectra is a difficult endeavor as the spectra\nare distributed over different archives and in addition they have different formats which\ncomplicates their handling. This notebook showcases a tool for the user to conveniently query the\nspectral archives and collect the spectra for a set of objects in a format that can be read in\nusing common software such as the Python specutils package. For simplicity, we limit the tool to\nquery already reduced and calibrated spectra.\nThe notebook may focus on the COSMOS field for now, which has a large overlap of spectroscopic\nsurveys such as with SDSS, DESI, Keck, HST, JWST, Spitzer, and Herschel. In addition, the tool\nenables the capability to search and ingest spectra from Euclid and SPHEREx in the feature. For\nthis to work, the specutils functions may have to be update or a wrapper has to be implemented.","type":"content","url":"/spectra-collector#motivation","position":7},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"List of Spectroscopic Archives and Status","lvl2":"Introduction"},"type":"lvl3","url":"/spectra-collector#list-of-spectroscopic-archives-and-status","position":8},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"List of Spectroscopic Archives and Status","lvl2":"Introduction"},"content":"Archive\n\nSpectra\n\nDescription\n\nAccess point\n\nStatus\n\nIRSA\n\nKeck\n\nAbout 10,000 spectra on the COSMOS field from \n\nHasinger et al. (2018)\n\nIRSA Archive\n\nImplemented with astroquery.ipac.irsa. (Table gives URLs to spectrum FITS files.) Note: only implemented for absolute calibrated spectra.\n\nIRSA\n\nSpitzer IRS\n\n~17,000 merged low-resolution IRS spectra\n\nIRS Enhanced Product\n\nImplemented with astroquery.ipac.irsa. (Table gives URLs to spectrum IPAC tables.)\n\nIRSA\n\nIRTF*\n\nLarge library of stellar spectra\n\n\n\ndoes astroquery.ipac.irsa work??\n\nESA\n\nHerschel*\n\nSome spectra\n\nastroquery.esa.hsa\n\nimplemented with \n\nastroquery\n\nIRSA\n\nEuclid\n\nSpectra hosted at IRSA in FY25\n\n\n\nUsing Q1 Spectra in the red grism (RGS) ingested in FY25, to be updated to DR1\n\nIRSA\n\nSPHEREx\n\nSpectra/cubes will be hosted at IRSA, first release in FY25 -> preparation for ingestion\n\n\n\nWill use mock spectra with correct format for testing\n\nMAST\n\nHST*\n\nSlitless spectra would need reduction and extraction. There are some reduced slit spectra from COS in the Hubble Archive\n\nastroquery.mast\n\nImplemented using astroquery.mast\n\nMAST\n\nJWST*\n\nReduced slit MSA and Slit spectra that can be queried\n\nastroquery.mast\n\nImplemented using astroquery.mast\n\nSDSS\n\nSDSS optical\n\nOptical spectra that are reduced\n\nSky Server or astroquery.sdss (preferred)\n\nImplemented using astroquery.sdss.\n\nDESI\n\nDESI*\n\nOptical spectra\n\nDESI public data release\n\nImplemented with SPARCL library.  Currently commented out because SPARCL library is incompatible with numpy > 2 which we need for other modules\n\nBOSS\n\nBOSS*\n\nOptical spectra\n\nBOSS webpage (part of SDSS)\n\nImplemented with SPARCL library together with DESI\n\nHEASARC\n\nNone\n\nCould link to Chandra observations to check AGN occurrence.\n\nastroquery.heasarc\n\nMore thoughts on how to include scientifically.\n\nThe ones with an asterisk (*) are the challenging ones.","type":"content","url":"/spectra-collector#list-of-spectroscopic-archives-and-status","position":9},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"Input","lvl2":"Introduction"},"type":"lvl3","url":"/spectra-collector#input","position":10},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"Input","lvl2":"Introduction"},"content":"• Coordinates for a single source or a sample on the COSMOS field","type":"content","url":"/spectra-collector#input","position":11},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"Output","lvl2":"Introduction"},"type":"lvl3","url":"/spectra-collector#output","position":12},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"Output","lvl2":"Introduction"},"content":"• A Pandas data frame including the spectra from different facilities\n\n• A plot comparing the different spectra extracted for each source","type":"content","url":"/spectra-collector#output","position":13},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"Runtime","lvl2":"Introduction"},"type":"lvl3","url":"/spectra-collector#runtime","position":14},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"Runtime","lvl2":"Introduction"},"content":"As of 2025 July, this notebook takes about 18 minutes to run to completion on Fornax using\na server with 16GB RAM/4 CPU’ and Environment: ‘Default Astrophysics’ (image).\n\n","type":"content","url":"/spectra-collector#runtime","position":15},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"Datasets that were considered but didn’t end up being used","lvl2":"Introduction"},"type":"lvl3","url":"/spectra-collector#datasets-that-were-considered-but-didnt-end-up-being-used","position":16},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"Datasets that were considered but didn’t end up being used","lvl2":"Introduction"},"content":"IRTF:\n\nhttps://​irsa​.ipac​.caltech​.edu​/Missions​/irtf​.html\n\nThe IRTF is a 3.2 meter telescope, optimized for infrared observations, and located at the summit\nof Mauna Kea, Hawaiʻi.\n\nlarge library of stellar spectra\n\nNot included here because the data are not currently available in an easily accessible,\nsearchable format","type":"content","url":"/spectra-collector#datasets-that-were-considered-but-didnt-end-up-being-used","position":17},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl2":"Imports"},"type":"lvl2","url":"/spectra-collector#imports","position":18},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl2":"Imports"},"content":"This cell will install them if needed:\n\n# Uncomment the next line to install dependencies if needed.\n# %pip install -r requirements_spectra_collector.txt\n\n\n\n\n\nimport os\nimport sys\n\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.table import Table\nfrom astropy.utils.data import conf\n\nsys.path.append('code_src/')\nfrom data_structures_spec import MultiIndexDFObject\n#from desi_functions import DESIBOSS_get_spec\nfrom herschel_functions import Herschel_get_spec\nfrom keck_functions import KeckDEIMOS_get_spec\nfrom mast_functions import HST_get_spec, JWST_get_spec\nfrom plot_functions import create_figures\nfrom sample_selection import clean_sample\nfrom sdss_functions import SDSS_get_spec\nfrom spitzer_functions import SpitzerIRS_get_spec\nfrom euclid_functions import euclid_get_spec\n\n# The Euclid spectrum files are large and the time it takes to read\n# them can exceed astropy's default timeout limit. Increase it.\nconf.remote_timeout = 120\n\n\n\n","type":"content","url":"/spectra-collector#imports","position":19},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl2":"1. Define the sample"},"type":"lvl2","url":"/spectra-collector#id-1-define-the-sample","position":20},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl2":"1. Define the sample"},"content":"Here we will define the sample of galaxies. For now, we just enter some “random” coordinates to\ntest the code.\n\ncoords = []\nlabels = []\n\ncoords.append(SkyCoord(\"{} {}\".format(\"09 54 49.40\", \"+09 16 15.9\"), unit=(u.hourangle, u.deg)))\nlabels.append(\"NGC3049\")\n\ncoords.append(SkyCoord(\"{} {}\".format(\"12 45 17.44 \", \"27 07 31.8\"), unit=(u.hourangle, u.deg)))\nlabels.append(\"NGC4670\")\n\ncoords.append(SkyCoord(\"{} {}\".format(\"14 01 19.92\", \"−33 04 10.7\"), unit=(u.hourangle, u.deg)))\nlabels.append(\"Tol_89\")\n\ncoords.append(SkyCoord(233.73856, 23.50321, unit=u.deg))\nlabels.append(\"Arp220\")\n\ncoords.append(SkyCoord(150.091, 2.2745833, unit=u.deg))\nlabels.append(\"COSMOS1\")\n\ncoords.append(SkyCoord(150.1024475, 2.2815559, unit=u.deg))\nlabels.append(\"COSMOS2\")\n\ncoords.append(SkyCoord(\"{} {}\".format(\"150.000\", \"+2.00\"), unit=(u.deg, u.deg)))\nlabels.append(\"COSMOS3\")\n\ncoords.append(SkyCoord(\"{} {}\".format(\"+53.15508\", \"-27.80178\"), unit=(u.deg, u.deg)))\nlabels.append(\"JADESGS-z7-01-QU\")\n\ncoords.append(SkyCoord(\"{} {}\".format(\"+53.15398\", \"-27.80095\"), unit=(u.deg, u.deg)))\nlabels.append(\"TestJWST\")\n\ncoords.append(SkyCoord(\"{} {}\".format(\"268.48058743\", \"64.78064676\"), unit=(u.deg, u.deg)))\nlabels.append(\"TestEuclid\")\n\ncoords.append(SkyCoord(\"{} {}\".format(\"+150.33622\", \"+55.89878\"), unit=(u.deg, u.deg)))\nlabels.append(\"Twin Quasar\")\n\nsample_table = clean_sample(coords, labels, precision=2.0 * u.arcsecond, verbose=1)\n\n\n\n","type":"content","url":"/spectra-collector#id-1-define-the-sample","position":21},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"1.2 Write out your sample to disk","lvl2":"1. Define the sample"},"type":"lvl3","url":"/spectra-collector#id-1-2-write-out-your-sample-to-disk","position":22},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"1.2 Write out your sample to disk","lvl2":"1. Define the sample"},"content":"At this point you may wish to write out your sample to disk and reuse that in future work sessions,\ninstead of creating it from scratch again. Note that we first check if the data directory exists\nand if not, we will create one.\n\nFor the format of the save file, we would suggest to choose from various formats that fully support\nastropy objects(eg., SkyCoord).  One example that works is Enhanced Character-Separated Values or\n\n\n‘ecsv’\n\nif not os.path.exists(\"./data\"):\n    os.mkdir(\"./data\")\nsample_table.write('data/input_sample.ecsv', format='ascii.ecsv', overwrite=True)\n\n\n\n","type":"content","url":"/spectra-collector#id-1-2-write-out-your-sample-to-disk","position":23},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"1.3 Load the sample table from disk","lvl2":"1. Define the sample"},"type":"lvl3","url":"/spectra-collector#id-1-3-load-the-sample-table-from-disk","position":24},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"1.3 Load the sample table from disk","lvl2":"1. Define the sample"},"content":"Do only this step from this section when you have a previously generated sample table\n\nsample_table = Table.read('data/input_sample.ecsv', format='ascii.ecsv')\n\n\n\n","type":"content","url":"/spectra-collector#id-1-3-load-the-sample-table-from-disk","position":25},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"1.4 Initialize data structure to hold the spectra","lvl2":"1. Define the sample"},"type":"lvl3","url":"/spectra-collector#id-1-4-initialize-data-structure-to-hold-the-spectra","position":26},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"1.4 Initialize data structure to hold the spectra","lvl2":"1. Define the sample"},"content":"Here, we initialize the MultiIndex data structure that will hold the spectra.\n\ndf_spec = MultiIndexDFObject()\n\n\n\n","type":"content","url":"/spectra-collector#id-1-4-initialize-data-structure-to-hold-the-spectra","position":27},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl2":"2. Find spectra for these targets in NASA and other ancillary catalogs"},"type":"lvl2","url":"/spectra-collector#id-2-find-spectra-for-these-targets-in-nasa-and-other-ancillary-catalogs","position":28},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl2":"2. Find spectra for these targets in NASA and other ancillary catalogs"},"content":"We search a curated list of NASA astrophysics archives.  Because each archive is different, and in\nmany cases each catalog is different, each function to access a catalog is necesarily specialized\nto the location and format of that particular catalog.\n\n","type":"content","url":"/spectra-collector#id-2-find-spectra-for-these-targets-in-nasa-and-other-ancillary-catalogs","position":29},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"2.1 IRSA Archive","lvl2":"2. Find spectra for these targets in NASA and other ancillary catalogs"},"type":"lvl3","url":"/spectra-collector#id-2-1-irsa-archive","position":30},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"2.1 IRSA Archive","lvl2":"2. Find spectra for these targets in NASA and other ancillary catalogs"},"content":"This archive includes spectra taken by\n\n• Keck\n\n• Spitzer/IRS\n\n• Euclid/NISP\n\n# Get Keck Spectra (COSMOS only)\ndf_spec_DEIMOS = KeckDEIMOS_get_spec(sample_table=sample_table, search_radius_arcsec=1)\ndf_spec.append(df_spec_DEIMOS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Get Spitzer IRS Spectra\ndf_spec_IRS = SpitzerIRS_get_spec(sample_table, search_radius_arcsec=1, COMBINESPEC=False)\ndf_spec.append(df_spec_IRS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Get Euclid Spectra\ndf_spec_Euclid = euclid_get_spec(sample_table=sample_table, search_radius_arcsec=1)\ndf_spec.append(df_spec_Euclid)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/spectra-collector#id-2-1-irsa-archive","position":31},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"2.2 MAST Archive","lvl2":"2. Find spectra for these targets in NASA and other ancillary catalogs"},"type":"lvl3","url":"/spectra-collector#id-2-2-mast-archive","position":32},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"2.2 MAST Archive","lvl2":"2. Find spectra for these targets in NASA and other ancillary catalogs"},"content":"This archive includes spectra taken by\n\n• HST (including slit spectroscopy)\n\n• JWST (including MSA and slit spectroscopy)\n\n# Get Spectra for HST\ndf_spec_HST = HST_get_spec(\n    sample_table,\n    search_radius_arcsec=0.5,\n    datadir=\"./data/\",\n    verbose=False,\n    delete_downloaded_data=True\n)\ndf_spec.append(df_spec_HST)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Get Spectra for JWST\ndf_jwst = JWST_get_spec(\n    sample_table,\n    search_radius_arcsec=0.5,\n    verbose=False,\n    max_spectra_per_source = 5\n)\ndf_spec.append(df_jwst)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/spectra-collector#id-2-2-mast-archive","position":33},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"2.3 ESA Archive","lvl2":"2. Find spectra for these targets in NASA and other ancillary catalogs"},"type":"lvl3","url":"/spectra-collector#id-2-3-esa-archive","position":34},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"2.3 ESA Archive","lvl2":"2. Find spectra for these targets in NASA and other ancillary catalogs"},"content":"\n\n# Herschel PACS & SPIRE from ESA TAP using astroquery\n# This search is fully functional, but is commented out because it takes\n# ~4 hours to run to completion\nherschel_radius = 1.1\nherschel_download_directory = 'data/herschel'\n\n# if not os.path.exists(herschel_download_directory):\n#    os.makedirs(herschel_download_directory, exist_ok=True)\n# df_spec_herschel =  Herschel_get_spec(sample_table, herschel_radius, herschel_download_directory, delete_downloaded_data=True)\n# df_spec.append(df_spec_herschel)\n\n\n\n","type":"content","url":"/spectra-collector#id-2-3-esa-archive","position":35},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"2.4 SDSS Archive","lvl2":"2. Find spectra for these targets in NASA and other ancillary catalogs"},"type":"lvl3","url":"/spectra-collector#id-2-4-sdss-archive","position":36},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"2.4 SDSS Archive","lvl2":"2. Find spectra for these targets in NASA and other ancillary catalogs"},"content":"\n\n# Get SDSS Spectra\ndf_spec_SDSS = SDSS_get_spec(sample_table, search_radius_arcsec=5, data_release=17)\ndf_spec.append(df_spec_SDSS)\n\n\n\n\n\n\n\n","type":"content","url":"/spectra-collector#id-2-4-sdss-archive","position":37},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"2.5 DESI Archive","lvl2":"2. Find spectra for these targets in NASA and other ancillary catalogs"},"type":"lvl3","url":"/spectra-collector#id-2-5-desi-archive","position":38},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"2.5 DESI Archive","lvl2":"2. Find spectra for these targets in NASA and other ancillary catalogs"},"content":"This includes DESI spectra. Here, we use the SPARCL query. Note that this can also be used\nfor SDSS searches, however, according to the SPARCL webpage, only up to DR16 is included.\nTherefore, we will not include SDSS DR16 here (this is treated in the SDSS search above).\n\nThe DESI search is currently commented out because SPARCL is not compatible with numpy > 2 which we require for the other modules to run.\n\n## Get DESI and BOSS spectra with SPARCL\n#df_spec_DESIBOSS = DESIBOSS_get_spec(sample_table, search_radius_arcsec=5)\n#df_spec.append(df_spec_DESIBOSS)\n\n\n\n","type":"content","url":"/spectra-collector#id-2-5-desi-archive","position":39},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl2":"3. Make plots of luminosity as a function of time"},"type":"lvl2","url":"/spectra-collector#id-3-make-plots-of-luminosity-as-a-function-of-time","position":40},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl2":"3. Make plots of luminosity as a function of time"},"content":"We show flux in mJy as a function of time for all available bands for each object.\nshow_nbr_figures controls how many plots are actually generated and returned to the screen.\nIf you choose to save the plots with save_output, they will be put in the output directory and\nlabelled by sample number.\n\n### Plotting ####\ncreate_figures(df_spec=df_spec,\n               bin_factor=1,\n               show_nbr_figures=10,\n               save_output=False,\n               )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/spectra-collector#id-3-make-plots-of-luminosity-as-a-function-of-time","position":41},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl2":"About this notebook"},"type":"lvl2","url":"/spectra-collector#about-this-notebook","position":42},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl2":"About this notebook"},"content":"Authors: Andreas Faisst, Jessica Krick, Shoubaneh Hemmati, Troy Raen, Brigitta Sipőcz, David Shupe, and the Fornax team\n\nContact: For help with this notebook, please open a topic in the \n\nFornax Community Forum “Support” category.","type":"content","url":"/spectra-collector#about-this-notebook","position":43},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"Acknowledgements","lvl2":"About this notebook"},"type":"lvl3","url":"/spectra-collector#acknowledgements","position":44},{"hierarchy":{"lvl1":"Extract Multi-Wavelength Spectroscopy from Archival Data","lvl3":"Acknowledgements","lvl2":"About this notebook"},"content":"AI: This notebook was created with assistance from OpenAI’s ChatGPT o4-mini-high model.","type":"content","url":"/spectra-collector#acknowledgements","position":45},{"hierarchy":{"lvl1":"Spectroscopy"},"type":"lvl1","url":"/spectroscopy","position":0},{"hierarchy":{"lvl1":"Spectroscopy"},"content":"In this User Case Scenario we work towards creating a spectroscopic library\nfrom multiple archival and publication resources and classifying and analyzing\nthem.","type":"content","url":"/spectroscopy","position":1}]}