{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e35c43b",
   "metadata": {},
   "source": [
    "# Light Curve Classifier\n",
    "***\n",
    "\n",
    "## Learning Goals\n",
    "By the end of this tutorial, you will be able to:\n",
    "- do some basic data cleaning and filtering to prepare the data for the ML algorithms\n",
    "- work with Pandas data frames as a way of storing time domain datasets\n",
    "- use sktime & pyts algorithms to train a classifier and predict values on a test dataset\n",
    "\n",
    "## Introduction\n",
    "This notebook takes output of a previous demo notebook which generates light curves from archival data, does data prep, and runs the light curves through multiple [`sktime`](https://www.sktime.net/en/stable/) classifiers.  We choose to use [sktime](https://www.sktime.net/en/stable/index.html) algorithms beacuse it is a library of many algorithms specifically tailored to time series datasets.  It is based on the sklearn library so syntax is familiar to many users.\n",
    "\n",
    "The goal of the classifiers is to be able to differentiate changing look active galactic nucleii (CLAGN) from an SDSS quasar sample based on multiband light curves.  CLAGN are quite interested objects in that they appear to change state, but only a few hundred are currently known, and finding them is quite expensive requiring spectroscopic follow up.  Being able to identify CLAGN in existing large samples would allow us to identify a statisitcal sample from which we could better understand the physics of what is occuring in these systems.\n",
    "\n",
    "The challenges of this time-domain dataset are:\n",
    "1. Multi-variate = There are multiple bands of observations per target (13+)\n",
    "2. Unequal length = Each band has a light curve with different sampling than other bands\n",
    "3. Missing data = Not each object has all observations in all bands\n",
    "\n",
    "We choose to use a Pandas multiindex dataframe to store and work with the data because it fulfills these requirements:\n",
    "1. It can handle the above challenges of a dataset = multi-variate, unqueal length with missing data.\n",
    "2. Multiple targets (multiple rows)\n",
    "3. Pandas has some built in understanding of time units\n",
    "4. Can be scaled up to big data numbers of rows (altough we don't push to out of memory structures in this use case)\n",
    "5. Pandas is user friendly\n",
    "\n",
    "\n",
    "## Input\n",
    "Light curve parquet file of multiband light curves from the mulitband_lc.ipynb demo notebook.  The format of the light curves is a Pandas multiindex data frame\n",
    "\n",
    "A useful reference for what sktime expects as input to its ML algorithms: https://github.com/sktime/sktime/blob/main/examples/AA_datatypes_and_datasets.ipynb\n",
    "\n",
    "## Output\n",
    "Trained classifiers as well as estimates of their accuracy and plots of confusion matrices\n",
    "\n",
    "## Imports\n",
    "- `pandas` to work with light curve data structure\n",
    "- `numpy` for numerical calculations\n",
    "- `matplotlib` for plotting\n",
    "- `sys` for paths\n",
    "- `astropy` to work with coordinates/units and data structures\n",
    "- `tqdm` for showing progress meter\n",
    "- `sktime` ML algorithms specifically for time-domain data\n",
    "- `sklearn` general use ML algorthims with easy to use interface\n",
    "\n",
    "## Authors\n",
    "\n",
    "## Acknowledgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d809e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure all dependencies are installed\n",
    "!pip install -r requirements-lc_classifier.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.time import Time\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "from scipy.stats import sigmaclip\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "from sklearn.metrics.cluster import completeness_score, homogeneity_score\n",
    "\n",
    "from sktime.classification.deep_learning import CNNClassifier\n",
    "from sktime.classification.dictionary_based import IndividualTDE\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.classification.dummy import DummyClassifier\n",
    "from sktime.classification.ensemble import WeightedEnsembleClassifier\n",
    "from sktime.classification.feature_based import Catch22Classifier, RandomIntervalClassifier\n",
    "from sktime.classification.hybrid import HIVECOTEV2\n",
    "from sktime.classification.interval_based import CanonicalIntervalForest\n",
    "from sktime.classification.kernel_based import Arsenal, RocketClassifier\n",
    "from sktime.classification.shapelet_based import ShapeletTransformClassifier\n",
    "from sktime.registry import all_estimators, all_tags\n",
    "\n",
    "from pyts.classification import KNeighborsClassifier\n",
    "from pyts.classification import SAXVSM\n",
    "from pyts.classification import BOSSVS\n",
    "from pyts.classification import LearningShapelets\n",
    "from pyts.classification import TimeSeriesForest\n",
    "\n",
    "# local code imports\n",
    "sys.path.append('code_src/')\n",
    "from fluxconversions import mjd_to_jd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106535f7",
   "metadata": {},
   "source": [
    "## 1. Read in a dataset of archival light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe7d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#access structure of light curves made in the light curve notebook\n",
    "# has CLAGN & SDSS small sample, all bands\n",
    "#https://drive.google.com/file/d/13RiPODiz2kI8j1OKpP1vfh6ByIUNsKEz/view?usp=share_link\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='13RiPODiz2kI8j1OKpP1vfh6ByIUNsKEz',\n",
    "                                    dest_path='./data/df_lc_458sample.parquet',\n",
    "                                    unzip=True)\n",
    "\n",
    "df_lc = pd.read_parquet(\"./data/df_lc_458sample.parquet\")\n",
    "\n",
    "#get rid of indices set in the light curve code and reset them as needed before sktime algorithms\n",
    "df_lc = df_lc.reset_index()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c2958",
   "metadata": {},
   "source": [
    "## 2. Data Prep\n",
    "This dataset needs significant work before it can be fed into a ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what does the dataset look like anyway?\n",
    "df_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1dc013",
   "metadata": {},
   "source": [
    "### 2.1 Remove bands with not enough data\n",
    "For this use case of CLAGN classification, we don't need to include some of the bands\n",
    "that are known to be sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "##what are the unique set of bands included in our light curves\n",
    "df_lc.band.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f3a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of some of the bands that don't have enough data for all the sources\n",
    "\n",
    "bands_to_drop = [\"IceCube\", \"TESS\", \"FERMIGTRIG\", \"K2\"]\n",
    "df_lc = df_lc[~df_lc[\"band\"].isin(bands_to_drop)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f47b4c",
   "metadata": {},
   "source": [
    "### 2.2 Combine Labels for a Simpler Classification\n",
    "All CLAGN start in the dataset as having labels based on their discovery paper.  Because we want one sample with all known CLAGN, change those discoery names to be simply \"CLAGN\" for all CLAGN, regardless of origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lc['label'] = df_lc.label.str.replace('MacLeod 16', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('LaMassa 15', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('Yang 18', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('Lyu 21', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('Hon 22', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('Sheng 20', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('MacLeod 19', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('Green 22', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('Lopez-Navas 22', 'CLAGN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b535703a",
   "metadata": {},
   "source": [
    "### 2.3 Remove \"bad\"  data\n",
    "\"bad\" includes:\n",
    "- errant values\n",
    "- NaNs\n",
    "- zero flux\n",
    "- outliers in uncertainty\n",
    "- objects with not enough flux measurements to make a good light curve\n",
    "- objects with no measurements in WISE W1 band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmaclip_lightcurves(df_lc, sigmaclip_value = 10.0, include_plot = False):\n",
    "    \"\"\"\n",
    "    Sigmaclip to remove bad values from the light curves; optionally plots histograms of uncertainties\n",
    "        to help determine sigmaclip_value from the data. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_lc: Pandas dataframe with light curve info\n",
    "   \n",
    "    sigmaclip_value: float\n",
    "        what value of sigma should be used to make the cuts\n",
    "\n",
    "    include_plot: bool\n",
    "        have the function plot histograms of uncertainties for each band\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    df_lc: MultiIndexDFObject with all  light curves\n",
    "        \n",
    "    \"\"\"\n",
    "    #keep track of how many rows this removes\n",
    "    start_len = len(df_lc.index)\n",
    "\n",
    "    #setup to collect the outlier thresholds per band to later reject\n",
    "    nsigmaonmean= {}\n",
    "\n",
    "    if include_plot:\n",
    "        #create the figure and axes\n",
    "        fig, axs = plt.subplots(5, 3, figsize = (12, 12))\n",
    "\n",
    "        # unpack all the axes subplots\n",
    "        axe = axs.ravel()\n",
    "\n",
    "    #for each band\n",
    "    for count, (bandname, singleband) in enumerate(df_lc.groupby(\"band\")):\n",
    "    \n",
    "        #use scipy sigmaclip to iteratively determine sigma on the dataset\n",
    "        clippedarr, lower, upper = sigmaclip(singleband.err, low = sigmaclip_value, high = sigmaclip_value)\n",
    "    \n",
    "        #store this value for later\n",
    "        nsigmaonmean[bandname] = upper\n",
    "    \n",
    "        if include_plot:        \n",
    "            #plot distributions and print stddev\n",
    "            singleband.err.plot(kind = 'hist', bins = 30, subplots =True, ax = axe[count],label = bandname+' '+str(upper), legend=True)\n",
    "\n",
    "    #remove data that are outside the sigmaclip_value\n",
    "    #make one large querystring joined by \"or\" for all bands in df_lc\n",
    "    querystring = \" | \".join(f'(band == {bandname!r} & err > {cut})' for bandname, cut in nsigmaonmean.items())\n",
    "    clipped_df_lc = df_lc.drop(df_lc.query(querystring).index)\n",
    "\n",
    "    #how much data did we remove with this sigma clipping?\n",
    "    #This should inform our choice of sigmaclip_value.\n",
    "\n",
    "    end_len = len(clipped_df_lc.index)\n",
    "    fraction = (start_len - end_len) / start_len\n",
    "    print(f\"This {sigmaclip_value} sigma clipping removed {fraction}% of the rows in df_lc\")\n",
    "\n",
    "    return clipped_df_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_objects_without_band(df_lc, bandname_to_drop, verbose=False):\n",
    "    \"\"\"\n",
    "    Get rid of the light curves which do not have W1 data.  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_lc: Pandas dataframe with light curve info\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    df_lc: MultiIndexDFObject with all  light curves\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    #maka a copy so we can work with it\n",
    "    dropW1_df_lc = df_lc\n",
    "    \n",
    "    #keep track of how many get dropped\n",
    "    dropcount = 0\n",
    "\n",
    "    #for each object\n",
    "    for oid , singleoid in dropW1_df_lc.groupby(\"objectid\"):\n",
    "        #what bands does that object have\n",
    "        bandname = singleoid.band.unique().tolist()\n",
    "    \n",
    "        #if it doesn't have W1:\n",
    "        if bandname_to_drop not in bandname:\n",
    "            #delete this oid from the dataframe of light curves\n",
    "            indexoid = dropW1_df_lc[ (dropW1_df_lc['objectid'] == oid)].index\n",
    "            dropW1_df_lc.drop(indexoid , inplace=True)\n",
    "        \n",
    "            #keep track of how many are being deleted\n",
    "            dropcount = dropcount + 1\n",
    "        \n",
    "    if verbose:    \n",
    "        print( dropcount, \"objects do not have W1 fluxes and were removed\")\n",
    "\n",
    "    return dropW1_df_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5415a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_incomplete_data(df_lc, threshold_too_few = 3):\n",
    "    \"\"\"\n",
    "    Remove those light curves that don't have enough data for classification.\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_lc: Pandas dataframe with light curve info\n",
    "\n",
    "    threshold_too_few: Int\n",
    "        Define what the threshold is for too few datapoints.\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    df_lc: MultiIndexDFObject with all  light curves\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    #how many groups do we have before we start\n",
    "    print(df_lc.groupby([\"band\", \"objectid\"]).ngroups, \"n groups before\")\n",
    "\n",
    "    #use pandas .filter to remove small groups\n",
    "    complete_df_lc = df_lc.groupby([\"band\", \"objectid\"]).filter(lambda x: len(x) > threshold_too_few)\n",
    "\n",
    "    #how many groups do we have after culling?\n",
    "    print(complete_df_lc.groupby([\"band\", \"objectid\"]).ngroups, \"n groups after\")\n",
    "\n",
    "    return complete_df_lc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee04c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows which have Nans\n",
    "df_lc.dropna(inplace = True, axis = 0)\n",
    "\n",
    "#drop rows with zero flux\n",
    "querystring = 'flux < 0.000001'\n",
    "df_lc = df_lc.drop(df_lc.query(querystring).index)\n",
    "\n",
    "#remove outliers\n",
    "#This is a tricky job because we want to keep astrophysical outliers of \n",
    "#variable objects, but remove instrumental noise and CR (ground based).\n",
    "sigmaclip_value = 10.0\n",
    "df_lc = sigmaclip_lightcurves(df_lc, sigmaclip_value, include_plot = True)\n",
    "\n",
    "#remove objects without W1 fluxes\n",
    "#We want to normalize all light curves by W1, so we neeed to remove those \n",
    "#without W1 fluxes as there will be nothing to normalize those light curves \n",
    "#with and we don't want to have un-normalized data or data that has been \n",
    "#normalized by a different band.  \n",
    "df_lc = remove_objects_without_band(df_lc, 'w1', verbose=True)\n",
    "\n",
    "#remove incomplete data\n",
    "#Some bands in some objects have only a few datapoints. Three data points \n",
    "#is not large enough for KNN interpolation, so we will consider any array \n",
    "#with fewer than 4 photometry points to be incomplete data.  Another way \n",
    "#of saying this is that we choose to remove those light curves with 3 or \n",
    "#fewer data points.\n",
    "threshold_too_few = 3\n",
    "df_lc = remove_incomplete_data(df_lc, threshold_too_few)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c361f0a6",
   "metadata": {},
   "source": [
    "### 2.4 Missing Data\n",
    "Some objects do not have light curves in all bands.  Some ML algorithms can handle mising data, but not all, so it would be useful to handle this missing data up front.\n",
    "\n",
    "There are two options here\n",
    "1) We will add light curves with zero flux and err values for the missing data.  SKtime does not like NaNs, so we chose zeros.\n",
    "2) Remove bands which have less data from all objects so that there are no objects with missing data\n",
    "\n",
    "Functions are inlcuded for both options (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7d46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zero_light_curve(oid, band, label):\n",
    "    \"\"\"\n",
    "    Make placeholder light curves with flux and err values = 0.0.\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    oid: Int\n",
    "        objectid\n",
    "    band: string\n",
    "        photometric band name\n",
    "    label: string\n",
    "        value for ML algorithms which details what kind of object this is\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    zerosingle: dictionary with light curve info\n",
    "        \n",
    "    \"\"\"\n",
    "    #randomly choose some times during the WISE survey\n",
    "    #these will all get fleshed out in the section on making uniform length time arrays\n",
    "    #so the specifics are not important now\n",
    "    timelist = [55230.0,57054.0, 57247.0, 57977.0, 58707.0]  \n",
    "    \n",
    "    #make a dictionary to hold the light curve\n",
    "    zerosingle = {\"objectid\": oid, \"label\": label, \"band\": band, \"time\": timelist, \n",
    "                  \"flux\": np.zeros(len(timelist)), \"err\":np.zeros(len(timelist))}\n",
    "    \n",
    "    return zerosingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e58d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingdata_to_zeros(df_lc):\n",
    "    \"\"\"\n",
    "    Convert mising data into zero fluxes and uncertainties\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_lc: Pandas dataframe with light curve info\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    df_lc: MultiIndexDFObject with all  light curves\n",
    "        \n",
    "    \"\"\"\n",
    "    #what is the full set of unique band names?\n",
    "    full_bandname = df_lc.band.unique()\n",
    "\n",
    "    #setup a list to store empty light curves\n",
    "    zerosingle_list = [] \n",
    "\n",
    "    #for each object in each band\n",
    "    for oid , singleoid in df_lc.groupby(\"objectid\"):\n",
    "                                  \n",
    "        #this is the list of bandnames for that object                                \n",
    "        oid_bandname = singleoid.band.unique()\n",
    "    \n",
    "        #figure out which bands are missing\n",
    "        missing = list(set(full_bandname).difference(oid_bandname))\n",
    "    \n",
    "        #if it is not the complete list, ie some bandnames are missing:                            \n",
    "        if len(missing) > 0:\n",
    "    \n",
    "            #make new dataframe for this object with zero flux and err values\n",
    "            for band in missing:\n",
    "                label = str(singleoid.label.unique().squeeze())\n",
    "                zerosingle = make_zero_light_curve(oid, band, label)\n",
    "                #keep track of these empty light curces in a list\n",
    "                zerosingle_list.append(zerosingle)\n",
    "    \n",
    "    #turn the empty light curves into a dataframe\n",
    "    df_empty = pd.DataFrame(zerosingle_list)\n",
    "    # df_empty has one row per dict. time,flux, and err columns store arrays.\n",
    "    # \"explode\" the dataframe to get one row per light curve point. time, flux, and err columns will now store floats.\n",
    "    df_empty = df_empty.explode([\"time\", \"flux\",\"err\"], ignore_index=True)\n",
    "    df_empty = df_empty.astype({col: \"float\" for col in [\"time\", \"flux\", \"err\"]})\n",
    "\n",
    "\n",
    "    #now put the empty light curves back together with the main light curve dataframe\n",
    "    zeros_df_lc = pd.concat([df_lc, df_empty])\n",
    "\n",
    "    return(zeros_df_lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingdata_drop_bands(df_lc, verbose = False):\n",
    "    \"\"\"\n",
    "    Drop bands with the most missing data and objects without all remaining bands so that there is no missing data going forward.\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_lc: Pandas dataframe with light curve info\n",
    "    \n",
    "    verbose: bool\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    df_lc: MultiIndexDFObject with all  light curves\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    #try instead to require that all objects have bands = df_lc.band.unique()\n",
    "\n",
    "    #first drop the bands not included from all objects\n",
    "    bands_to_drop = ['zi', 'Gaia g', 'Gaia bp', 'Gaia rp']\n",
    "    drop_df_lc = df_lc[~df_lc[\"band\"].isin(bands_to_drop)]\n",
    "    \n",
    "    #now a list of all remaining bands that we want to keep\n",
    "    bands_to_keep = drop_df_lc.band.unique()\n",
    "    \n",
    "    if verbose:\n",
    "        #how many objects did we start with?\n",
    "        print(drop_df_lc.groupby([\"objectid\"]).ngroups, \"n objects before removing missing band data\")\n",
    "\n",
    "    # Identify objects with all bands that we want to keep\n",
    "    complete_objects = drop_df_lc.groupby('objectid')['band'].apply(lambda x: set(x) == set(bands_to_keep))\n",
    "\n",
    "    # Filter the DataFrame based on complete objects\n",
    "    filter_df_lc = drop_df_lc[drop_df_lc['objectid'].isin(complete_objects[complete_objects].index)]\n",
    "\n",
    "    if verbose:\n",
    "        # How many objects are left?\n",
    "        print(filter_df_lc.groupby([\"objectid\"]).ngroups, \"n objects after removing missing band data\")\n",
    "\n",
    "    return(filter_df_lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0120847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose what to do with missing data...\n",
    "#df_lc = missingdata_to_zeros(df_lc)\n",
    "df_lc = missingdata_drop_bands(df_lc, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601857c",
   "metadata": {},
   "source": [
    "### 2.5  Make all objects and bands have identical time arrays (uniform length and spacing)\n",
    "\n",
    "It is very hard to find time-domain ML algorithms which can handle non uniform length datasets. Therefore we make them uniform using Pandas [reindex](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html) which fills in the uniform length arrays with values according to the method chosen by the user.  We implement a nearest neighbor to fill the arrays.  \n",
    "\n",
    "try a sklearn regressor to fit the shape of the light curve and do interpolation\n",
    "\n",
    "Potential other options for uniformizing the time series dataset:\n",
    "- pandas.dataframe.interpolate with many methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b93de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what does the dataframe look like at this point in the code?\n",
    "df_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell takes 13seconds to run on the sample of 458 sources\n",
    "def uniform_length_spacing(df_lc, final_freq_interpol, include_plot = True):\n",
    "    \"\"\"\n",
    "    Drop bands with the most missing data and objects without all remaining bands so that there is no missing data going forward.\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_lc: Pandas dataframe with light curve info\n",
    "    \n",
    "    verbose: bool\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    df_interpol: MultiIndexDFObject with interpolated light curves\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #make a time array with the minimum and maximum of all light curves in the sample\n",
    "    x_interpol = np.arange(df_lc.time.min(), df_lc.time.max(), final_freq_interpol)\n",
    "    x_interpol = x_interpol.reshape(-1, 1) # needed for sklearn\n",
    "    lc_interpol = []  # list to store interpolated light curves\n",
    "\n",
    "    #look at each object in each band\n",
    "    for (band,oid) , singleband_oid in df_lc.groupby([\"band\", \"objectid\"]):\n",
    "        #singleband_oid is now a dataframe with just one object and one band\n",
    "        X = np.array(singleband_oid[\"time\"]).reshape(-1, 1)\n",
    "        y = np.array(singleband_oid[\"flux\"])\n",
    "        dy = np.array(singleband_oid[\"err\"])\n",
    "\n",
    "        #could imagine using GP to make the arrays equal length and spacing\n",
    "        #however this sends the flux values to zero at the beginning and end of \n",
    "        #the arrays if there is time without observations.  This is not ideal\n",
    "        #because it significantly changes the shape of the light curves.\n",
    "        \n",
    "        #kernel = 1.0 * RBF(length_scale=30)\n",
    "        #gp = GaussianProcessRegressor(kernel=kernel, alpha=dy**2, normalize_y = False)\n",
    "        #gp.fit(X, y)\n",
    "        #mean_prediction,std_prediction = gp.predict(x_interpol, return_std=True)\n",
    "\n",
    "        #try KNN\n",
    "        KNN = KNeighborsRegressor(n_neighbors = 3)\n",
    "        KNN.fit(X, y)\n",
    "        mean_prediction = KNN.predict(x_interpol)\n",
    "        \n",
    "        #KNN doesnt output an uncertainty array, so make our own:\n",
    "        #an array of the same length as mean_prediction\n",
    "        #having values equal to the mean of the original uncertainty array\n",
    "        err = np.full_like(mean_prediction, singleband_oid.err.mean())  \n",
    "        \n",
    "        #get these values into the dataframe\n",
    "        # append the results as a dict. the list will be converted to a dataframe later.\n",
    "        lc_interpol.append(\n",
    "            {\"objectid\": oid, \"label\": str(singleband_oid.label.unique().squeeze()), \"band\": band, \"time\": x_interpol.reshape(-1), \n",
    "             \"flux\": mean_prediction, \"err\": err}\n",
    "        )\n",
    "    \n",
    "        if include_plot:\n",
    "            #see what this looks like on just a single light curve for now\n",
    "            if (band == 'zr') and (oid == 9) :  \n",
    "                #see if this looks reasonable\n",
    "                plt.errorbar(X,y,dy,linestyle=\"None\",color=\"tab:blue\",marker=\".\")\n",
    "                plt.plot(x_interpol, mean_prediction, label=\"Mean prediction\")\n",
    "                #plt.fill_between(\n",
    "                #    x_interpol.ravel(),\n",
    "                #    mean_prediction - 1.96 * std_prediction,\n",
    "                #    mean_prediction + 1.96 * std_prediction,\n",
    "                #    color=\"tab:orange\",\n",
    "                #    alpha=0.5,\n",
    "                #    label=r\"95% confidence interval\",\n",
    "                #)\n",
    "                plt.legend()\n",
    "                plt.xlabel(\"time\")\n",
    "                plt.ylabel(\"flux\")\n",
    "                _ = plt.title(\"KNN regression\")\n",
    "        \n",
    "        \n",
    "    # create a dataframe of the interpolated light curves\n",
    "    df_interpol = pd.DataFrame(lc_interpol)\n",
    "    return df_interpol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c5386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this to change the frequency of the time array\n",
    "final_freq_interpol = 30  #this is the timescale of interpolation in units of days\n",
    "\n",
    "df_interpol = uniform_length_spacing(df_lc, final_freq_interpol )\n",
    "# df_lc_interpol has one row per dict in lc_interpol. time and flux columns store arrays.\n",
    "# \"explode\" the dataframe to get one row per light curve point. time and flux columns will now store floats.\n",
    "df_lc = df_interpol.explode([\"time\", \"flux\",\"err\"], ignore_index=True)\n",
    "df_lc = df_lc.astype({col: \"float\" for col in [\"time\", \"flux\", \"err\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88ab67",
   "metadata": {},
   "source": [
    "### 2.6  Restructure dataframe \n",
    "- Make columns have band names in them and then remove band from the index\n",
    "- pivot the dataframe so that SKTIME understands its format\n",
    "- this will put it in the format expected by sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162c3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep some columns out of the mix when doing the pivot by bandname\n",
    "#set them as indices and they won't get pivoted into\n",
    "df_lc = df_lc.set_index([\"objectid\", \"label\", \"time\"])\n",
    "\n",
    "#attach bandname to all the fluxes and uncertainties\n",
    "df_lc = df_lc.pivot(columns = \"band\")\n",
    "\n",
    "#rename the columns\n",
    "df_lc.columns = [\"_\".join(col) for col in df_lc.columns.values]\n",
    "\n",
    "#many of these flux columns still have a space in them from the bandnames, \n",
    "#convert that space to underscore\n",
    "df_lc.columns = df_lc.columns.str.replace(' ', '_') \n",
    "\n",
    "#and get rid of that index to cleanup\n",
    "df_lc = df_lc.reset_index()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d439e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at a single object to see what this array looks like\n",
    "ob_of_interest = 4\n",
    "singleob = df_lc[df_lc['objectid'] == ob_of_interest]\n",
    "singleob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b6807",
   "metadata": {},
   "source": [
    "### 2.7 Normalize \n",
    "- this is normalizing across all bands\n",
    "- think this is the right place to do this, rather than interpolate over time \n",
    "    so that the final light curves are normalized since that is the chunk of information \n",
    "    which goes into the ML algorithms.\n",
    "- chose max and not median or mean because there are some objects where the median flux = 0.0\n",
    "    - if we did this before the interpolating, the median might be a non-zero value\n",
    "- normalizing is required so that the CLAGN and it's comparison SDSS sample don't have different flux levels.\n",
    "\n",
    "\n",
    "Idea here is that we normalize across each object.  So the algorithms will know, for example, that within one object W1 will be brighter than ZTF bands but from one object to the next, it will not know that one is brighter than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49071ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new column with max_r_flux for each objectid\n",
    "df_lc['max_W1'] = df_lc.groupby('objectid', sort=False)[\"flux_w1\"].transform('max')\n",
    "\n",
    "#figure out which columns in the dataframe are flux columns\n",
    "flux_cols = [col for col in df_lc.columns if 'flux' in col]\n",
    "\n",
    "# make new normalized flux columns for all fluxes\n",
    "df_lc[flux_cols] = df_lc[flux_cols].div(df_lc['max_W1'], axis=0)\n",
    "\n",
    "#now drop max_W1 as a column so it doesn't get included as a variable in multivariate analysis\n",
    "df_lc.drop(columns = ['max_W1'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc0820",
   "metadata": {},
   "source": [
    "### 2.8 Make datetime column\n",
    "https://docs.python.org/3/library/datetime.html#module-datetime\n",
    "\n",
    "SKTime wants a datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0174e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to convert df_lc time into datetime\n",
    "mjd = df_lc.time\n",
    "\n",
    "#convert to JD\n",
    "jd = mjd_to_jd(mjd)\n",
    "\n",
    "#convert to individual components\n",
    "t = Time(jd, format = 'jd' )\n",
    "\n",
    "#t.datetime is now an array of type datetime\n",
    "#make it a column in the dataframe\n",
    "df_lc['datetime'] = t.datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef32aca7",
   "metadata": {},
   "source": [
    "### 2.9 Save this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this dataframe to use for the ML below so we don't have to make it every time\n",
    "parquet_savename = 'output/df_lc_ML.parquet'\n",
    "df_lc.to_parquet(parquet_savename)\n",
    "#print(\"file saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3d940",
   "metadata": {},
   "source": [
    "### 2.10 Make into multi-index\n",
    "(which is what SKTime expects as input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b492e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lc = df_lc.set_index([\"objectid\", \"label\", \"datetime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae0c8ae",
   "metadata": {},
   "source": [
    "## 3. Prep for ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# could load a previously saved file in order to plot\n",
    "#parquet_loadname = 'output/df_lc_ML.parquet'\n",
    "#df_lc = MultiIndexDFObject()\n",
    "#df_lc.data = pd.read_parquet(parquet_loadname)\n",
    "#print(\"file loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try dropping the uncertainty columns as variables for sktime\n",
    "df_lc.drop(columns = ['err_panstarrs_g',\t'err_panstarrs_i',\t'err_panstarrs_r',\t'err_panstarrs_y',\t\n",
    "                      'err_panstarrs_z',\t'err_w1',\t'err_w2',\t'err_zg',\t'err_zr'], inplace = True)\n",
    "\n",
    "#drop also the time column because that shouldn't be a feature\n",
    "df_lc.drop(columns = ['time'],inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8fa348",
   "metadata": {},
   "source": [
    "### 3.0 Consider data augmentation\n",
    "\n",
    "1. https://arxiv.org/pdf/1811.08295.pdf which has the following github\n",
    "\n",
    "    - https://github.com/gioramponi/GAN_Time_Series/tree/master\n",
    "    - not easily usable\n",
    "2. https://arxiv.org/pdf/2205.06758.pdf\n",
    "\n",
    "3. ChatGPT - give multiindex df function and it will give a starting point for augmenting\n",
    "\n",
    "\n",
    "Worried that augmenting noisy data just makes more noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c8401",
   "metadata": {},
   "source": [
    "### 3.1 Train test split \n",
    "- Because thre are uneven numbers of each type (many more SDSS than CLAGN), we want to make sure to stratify evenly by type\n",
    "- Random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11701800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what does the dataframe look like now?\n",
    "df_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8221a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y is defined to be the labels\n",
    "y = df_lc.droplevel('datetime').index.unique().get_level_values('label').to_series()\n",
    "\n",
    "#want a stratified split based on label\n",
    "test_size = 0.25\n",
    "train_ix, test_ix = train_test_split(df_lc.index.levels[0], stratify = y, shuffle = True, random_state = 43, test_size = test_size)\n",
    "\n",
    "train_df = df_lc.loc[train_ix]  \n",
    "test_df = df_lc.loc[test_ix]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc13b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.groupby([ \"objectid\"]).ngroups, \"n groups in train sample\")\n",
    "print(test_df.groupby([\"objectid\"]).ngroups, \"n groups in test sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67cfdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot to show how many of each type of object in the test dataset\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.title(\"Objects in the Test dataset\")\n",
    "h = plt.hist(test_df.droplevel('datetime').index.unique().get_level_values('label').to_series(),histtype='stepfilled',orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e9cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the dataframe into X and y for ML algorithms \n",
    "\n",
    "#X is the multiindex light curve without the labels\n",
    "X_train  = train_df.droplevel('label')\n",
    "X_test = test_df.droplevel('label')\n",
    "\n",
    "#y are the labels, should be a series \n",
    "y_train = train_df.droplevel('datetime').index.unique().get_level_values('label').to_series()\n",
    "y_test = test_df.droplevel('datetime').index.unique().get_level_values('label').to_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.groupby([ \"objectid\"]).ngroups, \"n groups in train sample\")\n",
    "print(X_test.groupby([\"objectid\"]).ngroups, \"n groups in test sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271da42",
   "metadata": {},
   "source": [
    "## 4. Run sktime algorithms on the light curves\n",
    "\n",
    "We choose to use [sktime](https://www.sktime.net/en/stable/index.html) algorithms beacuse it is a library of many algorithms specifically tailored to time series datasets.  It is based on the sklearn library so syntax is familiar to many users.\n",
    "\n",
    "Types of classifiers are listed [here](https://www.sktime.net/en/stable/api_reference/classification.html).\n",
    "\n",
    "This notebook will invert the actual workflow and show you a single example of the algorithm which best fits the data and has the most accurate classifier. Then it will show how to write a for loop over a bunch of classifiers before narrowing it down to the most accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c95f7",
   "metadata": {},
   "source": [
    "### 4.1 Check that the data types are ok for sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e5f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask sktime if it likes the data type of X_train\n",
    "from sktime.datatypes import check_is_mtype\n",
    "\n",
    "check_is_mtype(X_train, mtype=\"pd-multiindex\", scitype=\"Panel\", return_metadata=True)\n",
    "#check_is_mtype(X_test, mtype=\"pd-multiindex\", scitype=\"Panel\", return_metadata=True)\n",
    "\n",
    "#This test needs to pass in order for sktime to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the list of all possible classifiers that work with multivariate data\n",
    "#all_tags(estimator_types = 'classifier')\n",
    "#classifiers = all_estimators(\"classifier\", filter_tags={'capability:multivariate':True})\n",
    "#classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb14f2",
   "metadata": {},
   "source": [
    "### 4.1 A single Classifier\n",
    "See section 4.3 for how we landed with this algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a97a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#looks like RandomIntervalClassifier is performing the best for the CLAGN (not for the SDSS)\n",
    "\n",
    "#setup the classifier\n",
    "clf = RandomIntervalClassifier(n_intervals = 12, n_jobs = -1, random_state = 43)\n",
    "\n",
    "#fit the classifier on the training dataset\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#make predictions on the test dataset using the trained model \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy of Random Interval Classifier: {accuracy_score(y_test, y_pred)}\\n\", flush=True)\n",
    "\n",
    "#plot a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5b404",
   "metadata": {},
   "source": [
    "### 4.2 Loop over a bunch of classifiers\n",
    "\n",
    "Our method is to do a cursory check of a bunch of classifiers and then later drill down deeper on anything with good initial results.  We choose to run a loop over ~10 classifiers that seem promising and check the accuracy scores for each one.  Any classifier with a promising accuracy score could then be followed up with detailed hyperparameter tuning, or potentially with considering other classifiers in that same type."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5eb1d3aa",
   "metadata": {},
   "source": [
    "%%time\n",
    "#This cell is currently not being run because it takes a while so is not good for testing/debugging\n",
    "\n",
    "#which classifiers are we interestd in\n",
    "#roughly one from each type of classifier\n",
    "\n",
    "names = [\"Arsenal\",                     #kernel based\n",
    "        \"RocektClassifier\",             #kernel based\n",
    "        \"CanonicalIntervalForest\",      #interval based\n",
    "        \"HIVECOTEV2\",                   #hybrid\n",
    "#        \"CNNClassifier\",               #Deep Learning  - **requires tensorflow which is giving import errors\n",
    "#        \"WeightedEnsembleClassifier\",   #Ensemble - **maybe use in the future if we find good options\n",
    "        \"IndividualTDE\",               #Dictionary-based\n",
    "        \"KNeighborsTimeSeriesClassifier\", #Distance Based\n",
    "        \"RandomIntervalClassifier\",     #Feature based\n",
    "        \"Catch22Classifier\",            #Feature based\n",
    "        \"ShapeletTransformClassifier\"   #Shapelet based\n",
    "        \"DummyClassifier\"]             #Dummy - ignores input\n",
    "\n",
    "#for those with an impossible time limit, how long to let them run for before cutting off\n",
    "nmins = 10\n",
    "\n",
    "#these could certainly be more tailored\n",
    "classifier_call = [Arsenal(time_limit_in_minutes=nmins, n_jobs = -1), \n",
    "                  RocketClassifier(num_kernels=2000),\n",
    "                  CanonicalIntervalForest(n_jobs = -1),\n",
    "                  HIVECOTEV2(time_limit_in_minutes=nmins, n_jobs = -1),\n",
    "#                  CNNClassifier(),\n",
    "#                  WeightedEnsembleClassifier(),\n",
    "                  IndividualTDE(n_jobs=-1),\n",
    "                  KNeighborsTimeSeriesClassifier(n_jobs = -1),\n",
    "                  RandomIntervalClassifier(n_intervals = 20, n_jobs = -1, random_state = 43),\n",
    "                  Catch22Classifier(outlier_norm = True, n_jobs = -1, random_state = 43),\n",
    "                  ShapeletTransformClassifier(time_limit_in_minutes=nmins,n_jobs = -1),\n",
    "                  DummyClassifier()]\n",
    "\n",
    "#setup to store the accuracy scores\n",
    "accscore_dict = {}\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, clf in tqdm(zip(names, classifier_call)):\n",
    "    #fit the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    #make predictions on the test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    #calculate and track accuracy score\n",
    "    accscore = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy of {name} classifier: {accscore}\\n\", flush=True)\n",
    "    accscore_dict[name] = accscore\n",
    "    \n",
    "    #plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "#just for keeping track, I also tried \n",
    "#clf = SignatureClassifier(depth = 2, window_depth = 3, random_state = 43)\n",
    "#this fails to complete, and is a known limitation of this algorithm.  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "063ccf57",
   "metadata": {},
   "source": [
    "#show the summary of the algorithms used and their accuracy score\n",
    "accscore_dict"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d7637dd",
   "metadata": {},
   "source": [
    "#save statistics from these runs\n",
    "\n",
    "# Serialize data into file:\n",
    "json.dump( accscore_dict, open( \"output/accscore.json\", 'w' ) )\n",
    "json.dump( completeness_dict, open( \"output/completeness.json\", 'w' ) )\n",
    "json.dump( homogeneity_dict, open( \"output/homogeneity.json\", 'w' ) )\n",
    "\n",
    "# Read data from file:\n",
    "#accscore_dict = json.load( open( \"output/accscore.json\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c082b0",
   "metadata": {},
   "source": [
    "## 5.0 Run pyts algorithms on a single band of the light curves\n",
    "[pyts](https://pyts.readthedocs.io/en/stable/) is a python package for time series classification.\n",
    "\n",
    "We run univariate classification here with just the W1 WISE band.  This is a change from the multivariate sktime classification above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5641f50",
   "metadata": {},
   "source": [
    "### 5.1 Get the data into the correct shape for pyTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many objects are we working with?\n",
    "#this is repeated here in case sktime is not run above.\n",
    "print(X_train.groupby([ \"objectid\"]).ngroups, \"n groups in train sample\")\n",
    "print(X_test.groupby([\"objectid\"]).ngroups, \"n groups in test sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59165530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input to pyTS must be a  numpy.ndarray \n",
    "# a 2d array with shape (n_samples, n_timestamps), where the first axis represents the \n",
    "# samples and the second axis represents time\n",
    "\n",
    "#start working on X\n",
    "X_train_pyts = X_train.reset_index()\n",
    "X_test_pyts = X_test.reset_index()\n",
    "\n",
    "# Extract univariate flux values into NumPy arrays and reshape them\n",
    "X_train_np = X_train_pyts.pivot(index='objectid',  columns='time',values='flux_w1').to_numpy() \n",
    "X_test_np = X_test_pyts.pivot(index='objectid', columns='time', values='flux_w1').to_numpy()\n",
    "\n",
    "#now work on the y\n",
    "train_df_pyts = train_df.reset_index()\n",
    "test_df_pyts = test_df.reset_index()\n",
    "\n",
    "# Extract unique labels for each objectid and convert to a NumPy array\n",
    "y_train_np = train_df_pyts.groupby('objectid')['label'].first().to_numpy()\n",
    "y_test_np = test_df_pyts.groupby('objectid')['label'].first().to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7cd340",
   "metadata": {},
   "source": [
    "### 5.2 A single classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640aeed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup to store the accuracy et al. scores\n",
    "accscore_dict = {}\n",
    "MCC_dict ={}\n",
    "homogeneity_dict = {}\n",
    "completeness_dict = {}\n",
    "f1_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c159c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LearningShapelets(random_state=42, tol=0.01)\n",
    "clf.fit(X_train_np, y_train_np)\n",
    "clf.score(X_test_np, y_test_np)\n",
    "y_pred = clf.predict(X_test_np)\n",
    "\n",
    "#plot confusion matrix\n",
    "cm = confusion_matrix(y_test_np, y_pred, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "#calculate and track accuracy score\n",
    "name = \"learningshapelets\"\n",
    "accscore = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of {name} classifier: {accscore}\\n\", flush=True)\n",
    "accscore_dict[name] = accscore\n",
    "    \n",
    "MCC = matthews_corrcoef(y_test, y_pred)\n",
    "print(f\"MCC of {name} classifier: {MCC}\\n\", flush=True)\n",
    "MCC_dict[name] = MCC\n",
    "    \n",
    "completeness = completeness_score(y_test, y_pred)\n",
    "print(f\"Completeness of {name} classifier: {completeness}\\n\", flush=True)\n",
    "completeness_dict[name] = completeness\n",
    "    \n",
    "homogeneity = homogeneity_score(y_test, y_pred)\n",
    "print(f\"Homogeneity of {name} classifier: {homogeneity}\\n\", flush=True)\n",
    "homogeneity_dict[name] = homogeneity\n",
    "    \n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"F1 score of {name} classifier: {f1}\\n\", flush=True)\n",
    "f1_dict[name] = f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45a837",
   "metadata": {},
   "source": [
    "### 5.3 Loop over a bunch of classifiers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "195fe4cf",
   "metadata": {},
   "source": [
    "%%time\n",
    "#This cell is currently not being run because it takes a while so is not good for testing/debugging\n",
    "\n",
    "names = [\"KNNDTW\",\"saxvsm\",\"bossvs\", \"learningshapelets\",\"timeseriesforest\"]\n",
    "         \n",
    "\n",
    "#these could certainly be more tailored\n",
    "classifier_call = [KNeighborsClassifier(metric='dtw'), \n",
    "                   SAXVSM(window_size=34, sublinear_tf=False, use_idf=False),\n",
    "                   BOSSVS(window_size=28),\n",
    "                   LearningShapelets(random_state=43, tol=0.01),\n",
    "                   TimeSeriesForest(random_state=43)]\n",
    "                   \n",
    "#setup to store the accuracy scores\n",
    "accscore_dict = {}\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, clf in tqdm(zip(names, classifier_call)):\n",
    "    #fit the classifier\n",
    "    clf.fit(X_train_np, y_train_np)\n",
    "    \n",
    "    #make predictions on the test dataset\n",
    "    y_pred = clf.predict(X_test_np)\n",
    "\n",
    "    #calculate and track accuracy score\n",
    "    accscore = accuracy_score(y_test_np, y_pred)\n",
    "    print(f\"Accuracy of {name} classifier: {accscore}\\n\", flush=True)\n",
    "    accscore_dict[name] = accscore\n",
    "    \n",
    "    #plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d685d09b",
   "metadata": {},
   "source": [
    "#save statistics from these runs\n",
    "\n",
    "# Serialize data into file:\n",
    "json.dump( accscore_dict, open( \"output/pyts_accscore.json\", 'w' ) )\n",
    "json.dump( completeness_dict, open( \"output/pyts_completeness.json\", 'w' ) )\n",
    "json.dump( homogeneity_dict, open( \"output/pyts_homogeneity.json\", 'w' ) )\n",
    "\n",
    "# Read data from file:\n",
    "#accscore_dict = json.load( open( \"output/pyts_accscore.json\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad76f6a",
   "metadata": {},
   "source": [
    "## 6.0 Conclusions:  \n",
    "This classifier can be used to predict CLAGN.  The feature based algorithms do the best jobs of having little to no predicted CLAGN that are truly normal SDSS quasars.  We infer then that if the trained model predicts CLAGN, it is a very good target for follow-up spectroscopy to confirm CLAGN.  However this algorthim will not catch all CLAGN, and will incorrectly labels some CLAGN as being normal SDSS quasars.  THis algorithm can therefore not be used to find a complete sample of CLAGN, but can be used to increase the known sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef367dc4",
   "metadata": {},
   "source": [
    "### 6.1 Potential Areas of improvement\n",
    "- Data is messy\n",
    "    - ZTF calibration??\n",
    "- Label inaccuracy is a concern\n",
    "    - mostly SDSS, \n",
    "    - but CLAGN papers all have different selection criteria\n",
    "- Not enough data on CLAGN\n",
    "    - limited number of lightcurves\n",
    "    - consider data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3808bd",
   "metadata": {},
   "source": [
    "## References:\n",
    "Markus Lning, Anthony Bagnall, Sajaysurya Ganesh, Viktor Kazakov, Jason Lines, Franz Kirly (2019): sktime: A Unified Interface for Machine Learning with Time Series\n",
    "Markus Lning, Tony Bagnall, Sajaysurya Ganesh, George Oastler, Jason Lines, ViktorKaz, , Aadesh Deshmukh (2020). sktime/sktime. Zenodo. http://doi.org/10.5281/zenodo.3749000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487539e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.15.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   63,
   68,
   109,
   113,
   126,
   131,
   134,
   140,
   145,
   150,
   155,
   165,
   176,
   238,
   280,
   311,
   341,
   352,
   383,
   436,
   479,
   483,
   494,
   499,
   582,
   591,
   598,
   617,
   622,
   636,
   648,
   655,
   668,
   672,
   677,
   682,
   684,
   688,
   696,
   703,
   718,
   724,
   729,
   741,
   745,
   750,
   758,
   770,
   773,
   783,
   787,
   797,
   802,
   807,
   828,
   834,
   898,
   903,
   913,
   920,
   924,
   931,
   951,
   955,
   964,
   995,
   999,
   1036,
   1046,
   1051,
   1063,
   1069
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}